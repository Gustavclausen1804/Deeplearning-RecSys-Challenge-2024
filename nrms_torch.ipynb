{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Global settings and imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/gustavsiphone/Documents/GitHub/Deeplearning-RecSys-Challenge-2024/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python version: 3.12.5\n",
      "PyTorch version: 2.5.1\n",
      "CUDA available: False\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import sys\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import polars as pl\n",
    "import matplotlib.pyplot as plt\n",
    "#import optuna\n",
    "from tqdm import tqdm\n",
    "import torch.optim as optim\n",
    "\n",
    "from utils._constants import (\n",
    "    DEFAULT_HISTORY_ARTICLE_ID_COL,\n",
    "    DEFAULT_CLICKED_ARTICLES_COL,\n",
    "    DEFAULT_INVIEW_ARTICLES_COL,\n",
    "    DEFAULT_IMPRESSION_ID_COL,\n",
    "    DEFAULT_SUBTITLE_COL,\n",
    "    DEFAULT_LABELS_COL,\n",
    "    DEFAULT_TITLE_COL,\n",
    "    DEFAULT_USER_COL,\n",
    "    DEFAULT_ARTICLE_PUBLISHED_TIMESTAMP_COL\n",
    ")\n",
    "\n",
    "from utils._behaviors import (\n",
    "    create_binary_labels_column,\n",
    "    sampling_strategy_wu2019,\n",
    "    add_known_user_column,\n",
    "    add_prediction_scores,\n",
    "    truncate_history,\n",
    ")\n",
    "from evaluation import MetricEvaluator, AucScore, NdcgScore, MrrScore\n",
    "from utils._articles import convert_text2encoding_with_transformers\n",
    "from utils._polars import concat_str_columns, slice_join_dataframes\n",
    "from utils._articles import create_article_id_to_value_mapping\n",
    "from utils._nlp import get_transformers_word_embeddings, generate_embeddings_with_transformers\n",
    "from utils._python import write_submission_file, rank_predictions_by_score\n",
    "from models_pytorch.model_config import hparams_nrms\n",
    "\n",
    "from models_pytorch.nrms import NRMSModel\n",
    "from models_pytorch.NRMSDocVecModel import NRMSDocVecModel\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from models_pytorch.dataloader import NRMSDataSet\n",
    "\n",
    "# Check Python version\n",
    "print(f\"Python version: {sys.version.split()[0]}\")\n",
    "\n",
    "# Check PyTorch version\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "\n",
    "# Check GPU availability\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"Current GPU device: {torch.cuda.get_device_name()}\")\n",
    "    print(f\"Number of GPUs available: {torch.cuda.device_count()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Looking at behaviours and history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download and load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ebnerd_small\n"
     ]
    }
   ],
   "source": [
    "PATH = Path(\"./ebnerd_small\")  # Base path for your data directory\n",
    "print(PATH)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (2, 6)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>user_id</th><th>article_id_fixed</th><th>article_ids_inview</th><th>article_ids_clicked</th><th>impression_id</th><th>labels</th></tr><tr><td>u32</td><td>list[i32]</td><td>list[i64]</td><td>list[i64]</td><td>u32</td><td>list[i8]</td></tr></thead><tbody><tr><td>1760809</td><td>[0, 0, … 9770829]</td><td>[9776014, 9777296, … 9771254]</td><td>[9777296]</td><td>263453510</td><td>[0, 1, … 0]</td></tr><tr><td>1164782</td><td>[9757533, 9760747, … 9769580]</td><td>[9776099, 9775964, … 9775964]</td><td>[9775939]</td><td>313295046</td><td>[0, 0, … 0]</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (2, 6)\n",
       "┌─────────┬───────────────────┬───────────────────┬──────────────────┬───────────────┬─────────────┐\n",
       "│ user_id ┆ article_id_fixed  ┆ article_ids_invie ┆ article_ids_clic ┆ impression_id ┆ labels      │\n",
       "│ ---     ┆ ---               ┆ w                 ┆ ked              ┆ ---           ┆ ---         │\n",
       "│ u32     ┆ list[i32]         ┆ ---               ┆ ---              ┆ u32           ┆ list[i8]    │\n",
       "│         ┆                   ┆ list[i64]         ┆ list[i64]        ┆               ┆             │\n",
       "╞═════════╪═══════════════════╪═══════════════════╪══════════════════╪═══════════════╪═════════════╡\n",
       "│ 1760809 ┆ [0, 0, … 9770829] ┆ [9776014,         ┆ [9777296]        ┆ 263453510     ┆ [0, 1, … 0] │\n",
       "│         ┆                   ┆ 9777296, …        ┆                  ┆               ┆             │\n",
       "│         ┆                   ┆ 9771254]          ┆                  ┆               ┆             │\n",
       "│ 1164782 ┆ [9757533,         ┆ [9776099,         ┆ [9775939]        ┆ 313295046     ┆ [0, 0, … 0] │\n",
       "│         ┆ 9760747, …        ┆ 9775964, …        ┆                  ┆               ┆             │\n",
       "│         ┆ 9769580]          ┆ 9775964]          ┆                  ┆               ┆             │\n",
       "└─────────┴───────────────────┴───────────────────┴──────────────────┴───────────────┴─────────────┘"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def ebnerd_from_path(path: Path, history_size: int = 30) -> pl.DataFrame:\n",
    "    \"\"\"\n",
    "    Load ebnerd - function\n",
    "    \"\"\"\n",
    "    df_history = (\n",
    "        pl.scan_parquet(path.joinpath(\"history.parquet\"))\n",
    "        .select(DEFAULT_USER_COL, DEFAULT_HISTORY_ARTICLE_ID_COL)\n",
    "        .pipe(\n",
    "            truncate_history,\n",
    "            column=DEFAULT_HISTORY_ARTICLE_ID_COL,\n",
    "            history_size=history_size,\n",
    "            padding_value=0,\n",
    "            enable_warning=False,\n",
    "        )\n",
    "    )\n",
    "    df_behaviors = (\n",
    "        pl.scan_parquet(path.joinpath(\"behaviors.parquet\"))\n",
    "        .collect()\n",
    "        .pipe(\n",
    "            slice_join_dataframes,\n",
    "            df2=df_history.collect(),\n",
    "            on=DEFAULT_USER_COL,\n",
    "            how=\"left\",\n",
    "        )\n",
    "    )\n",
    "    return df_behaviors\n",
    "\n",
    "COLUMNS = [\n",
    "    DEFAULT_USER_COL,\n",
    "    DEFAULT_HISTORY_ARTICLE_ID_COL,\n",
    "    DEFAULT_INVIEW_ARTICLES_COL,\n",
    "    DEFAULT_CLICKED_ARTICLES_COL,\n",
    "    DEFAULT_IMPRESSION_ID_COL,\n",
    "]\n",
    "HISTORY_SIZE = 40 # TODO: History size. \n",
    "FRACTION = 0.01\n",
    "\n",
    "df_train = (\n",
    "    ebnerd_from_path(PATH.joinpath(\"train\"), history_size=HISTORY_SIZE)\n",
    "    .select(COLUMNS)\n",
    "    .pipe(\n",
    "        sampling_strategy_wu2019,\n",
    "        npratio=4,\n",
    "        shuffle=True,\n",
    "        with_replacement=True,\n",
    "        seed=123,\n",
    "    )\n",
    "    .pipe(create_binary_labels_column)\n",
    "    .sample(fraction=FRACTION)\n",
    ")\n",
    "# =>\n",
    "df_validation = (\n",
    "    ebnerd_from_path(PATH.joinpath(\"validation\"), history_size=HISTORY_SIZE)\n",
    "    .select(COLUMNS)\n",
    "    .pipe(create_binary_labels_column)\n",
    "    .sample(fraction=FRACTION)\n",
    ")\n",
    "df_train.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "vscode": {
     "languageId": "ruby"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average length of article_ids_inview in df_train: 5.0\n",
      "Average length of article_ids_inview in df_validation: 12.235895339329518\n"
     ]
    }
   ],
   "source": [
    "def calculate_average_length(df, column):\n",
    "    total_length = sum(len(row) for row in df[column])\n",
    "    average_length = total_length / len(df)\n",
    "    return average_length\n",
    "\n",
    "# Calculate average length for df_train\n",
    "average_length_inview_train = calculate_average_length(df_train, DEFAULT_INVIEW_ARTICLES_COL)\n",
    "print(f\"Average length of article_ids_inview in df_train: {average_length_inview_train}\")\n",
    "\n",
    "# Calculate average length for df_validation\n",
    "average_length_inview_validation = calculate_average_length(df_validation, DEFAULT_INVIEW_ARTICLES_COL)\n",
    "print(f\"Average length of article_ids_inview in df_validation: {average_length_inview_validation}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Longest inview article length in df_train: 5\n",
      "Longest inview article length in df_validation: 88\n",
      "Longest history length in df_train: 40\n",
      "Longest history length in df_validation: 40\n"
     ]
    }
   ],
   "source": [
    "# Function to find the maximum length of arrays in a column\n",
    "def find_max_length(df, column):\n",
    "    max_length = 0\n",
    "    for row in df[column]:\n",
    "        max_length = max(max_length, len(row))\n",
    "    return max_length\n",
    "\n",
    "# Find the longest inview article length in df_train\n",
    "max_inview_length_train = find_max_length(df_train, DEFAULT_INVIEW_ARTICLES_COL)\n",
    "\n",
    "# Find the longest inview article length in df_validation\n",
    "max_inview_length_validation = find_max_length(df_validation, DEFAULT_INVIEW_ARTICLES_COL)\n",
    "\n",
    "print(f\"Longest inview article length in df_train: {max_inview_length_train}\")\n",
    "print(f\"Longest inview article length in df_validation: {max_inview_length_validation}\")\n",
    "\n",
    "max_history_length_train = find_max_length(df_train, DEFAULT_HISTORY_ARTICLE_ID_COL)\n",
    "max_history_length_validation = find_max_length(df_validation, DEFAULT_HISTORY_ARTICLE_ID_COL)\n",
    "\n",
    "print(f\"Longest history length in df_train: {max_history_length_train}\")\n",
    "print(f\"Longest history length in df_validation: {max_history_length_validation}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "vscode": {
     "languageId": "ruby"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows with exactly one clicked article in df_train: 2342\n",
      "Number of rows with exactly one clicked article in df_validation: 2430\n"
     ]
    }
   ],
   "source": [
    "# Function to filter rows with exactly one clicked article\n",
    "def filter_rows_with_one_clicked_article(df, clicked_articles_col):\n",
    "    # Manually filter rows where the array has exactly one element\n",
    "    filtered_rows = []\n",
    "    for row in df.iter_rows(named=True):\n",
    "        if len(row[clicked_articles_col]) == 1:\n",
    "            filtered_rows.append(row)\n",
    "    return pl.DataFrame(filtered_rows)\n",
    "\n",
    "\n",
    "# Filter rows in df_train and df_validation\n",
    "df_train = filter_rows_with_one_clicked_article(df_train, DEFAULT_CLICKED_ARTICLES_COL)\n",
    "df_validation = filter_rows_with_one_clicked_article(df_validation, DEFAULT_CLICKED_ARTICLES_COL)\n",
    "\n",
    "# Print the results\n",
    "print(f\"Number of rows with exactly one clicked article in df_train: {df_train.shape[0]}\")\n",
    "print(f\"Number of rows with exactly one clicked article in df_validation: {df_validation.shape[0]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "vscode": {
     "languageId": "ruby"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (5, 6)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>user_id</th><th>article_id_fixed</th><th>article_ids_inview</th><th>article_ids_clicked</th><th>impression_id</th><th>labels</th></tr><tr><td>i64</td><td>list[i64]</td><td>list[i64]</td><td>list[i64]</td><td>i64</td><td>list[i64]</td></tr></thead><tbody><tr><td>1460316</td><td>[9778021, 9779520, … 9780181]</td><td>[9787501, 9787465, … 9787525]</td><td>[9780986]</td><td>492351673</td><td>[0, 0, … 0]</td></tr><tr><td>2073394</td><td>[9774542, 9775079, … 9779748]</td><td>[9268227, 9270363, … 9717946]</td><td>[9717946]</td><td>541994952</td><td>[0, 0, … 1]</td></tr><tr><td>2427908</td><td>[9773292, 9774532, … 9779511]</td><td>[9770551, 9782391, … 9790406]</td><td>[9789493]</td><td>41413363</td><td>[0, 0, … 0]</td></tr><tr><td>2530015</td><td>[9776710, 9777026, … 9780181]</td><td>[9785828, 9778869, … 9775978]</td><td>[9785790]</td><td>6547110</td><td>[0, 0, … 0]</td></tr><tr><td>2474074</td><td>[9772750, 9772805, … 9226885]</td><td>[9535187, 7213923, … 9503501]</td><td>[9783858]</td><td>483144493</td><td>[0, 0, … 0]</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (5, 6)\n",
       "┌─────────┬───────────────────┬───────────────────┬──────────────────┬───────────────┬─────────────┐\n",
       "│ user_id ┆ article_id_fixed  ┆ article_ids_invie ┆ article_ids_clic ┆ impression_id ┆ labels      │\n",
       "│ ---     ┆ ---               ┆ w                 ┆ ked              ┆ ---           ┆ ---         │\n",
       "│ i64     ┆ list[i64]         ┆ ---               ┆ ---              ┆ i64           ┆ list[i64]   │\n",
       "│         ┆                   ┆ list[i64]         ┆ list[i64]        ┆               ┆             │\n",
       "╞═════════╪═══════════════════╪═══════════════════╪══════════════════╪═══════════════╪═════════════╡\n",
       "│ 1460316 ┆ [9778021,         ┆ [9787501,         ┆ [9780986]        ┆ 492351673     ┆ [0, 0, … 0] │\n",
       "│         ┆ 9779520, …        ┆ 9787465, …        ┆                  ┆               ┆             │\n",
       "│         ┆ 9780181]          ┆ 9787525]          ┆                  ┆               ┆             │\n",
       "│ 2073394 ┆ [9774542,         ┆ [9268227,         ┆ [9717946]        ┆ 541994952     ┆ [0, 0, … 1] │\n",
       "│         ┆ 9775079, …        ┆ 9270363, …        ┆                  ┆               ┆             │\n",
       "│         ┆ 9779748]          ┆ 9717946]          ┆                  ┆               ┆             │\n",
       "│ 2427908 ┆ [9773292,         ┆ [9770551,         ┆ [9789493]        ┆ 41413363      ┆ [0, 0, … 0] │\n",
       "│         ┆ 9774532, …        ┆ 9782391, …        ┆                  ┆               ┆             │\n",
       "│         ┆ 9779511]          ┆ 9790406]          ┆                  ┆               ┆             │\n",
       "│ 2530015 ┆ [9776710,         ┆ [9785828,         ┆ [9785790]        ┆ 6547110       ┆ [0, 0, … 0] │\n",
       "│         ┆ 9777026, …        ┆ 9778869, …        ┆                  ┆               ┆             │\n",
       "│         ┆ 9780181]          ┆ 9775978]          ┆                  ┆               ┆             │\n",
       "│ 2474074 ┆ [9772750,         ┆ [9535187,         ┆ [9783858]        ┆ 483144493     ┆ [0, 0, … 0] │\n",
       "│         ┆ 9772805, …        ┆ 7213923, …        ┆                  ┆               ┆             │\n",
       "│         ┆ 9226885]          ┆ 9503501]          ┆                  ┆               ┆             │\n",
       "└─────────┴───────────────────┴───────────────────┴──────────────────┴───────────────┴─────────────┘"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_validation.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "vscode": {
     "languageId": "ruby"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of users in df_train: 1976\n"
     ]
    }
   ],
   "source": [
    "print(f\"Number of users in df_train: {df_train['user_id'].n_unique()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (10, 21)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>article_id</th><th>title</th><th>subtitle</th><th>last_modified_time</th><th>premium</th><th>body</th><th>published_time</th><th>image_ids</th><th>article_type</th><th>url</th><th>ner_clusters</th><th>entity_groups</th><th>topics</th><th>category</th><th>subcategory</th><th>category_str</th><th>total_inviews</th><th>total_pageviews</th><th>total_read_time</th><th>sentiment_score</th><th>sentiment_label</th></tr><tr><td>i32</td><td>str</td><td>str</td><td>datetime[μs]</td><td>bool</td><td>str</td><td>datetime[μs]</td><td>list[i64]</td><td>str</td><td>str</td><td>list[str]</td><td>list[str]</td><td>list[str]</td><td>i16</td><td>list[i16]</td><td>str</td><td>i32</td><td>i32</td><td>f32</td><td>f32</td><td>str</td></tr></thead><tbody><tr><td>3001353</td><td>&quot;Natascha var ikke den første&quot;</td><td>&quot;Politiet frygter nu, at Natasc…</td><td>2023-06-29 06:20:33</td><td>false</td><td>&quot;Sagen om den østriske Natascha…</td><td>2006-08-31 08:06:45</td><td>[3150850]</td><td>&quot;article_default&quot;</td><td>&quot;https://ekstrabladet.dk/krimi/…</td><td>[]</td><td>[]</td><td>[&quot;Kriminalitet&quot;, &quot;Personfarlig kriminalitet&quot;]</td><td>140</td><td>[]</td><td>&quot;krimi&quot;</td><td>null</td><td>null</td><td>null</td><td>0.9955</td><td>&quot;Negative&quot;</td></tr><tr><td>3003065</td><td>&quot;Kun Star Wars tjente mere&quot;</td><td>&quot;Biografgængerne strømmer ind f…</td><td>2023-06-29 06:20:35</td><td>false</td><td>&quot;Vatikanet har opfordret til at…</td><td>2006-05-21 16:57:00</td><td>[3006712]</td><td>&quot;article_default&quot;</td><td>&quot;https://ekstrabladet.dk/underh…</td><td>[]</td><td>[]</td><td>[&quot;Underholdning&quot;, &quot;Film og tv&quot;, &quot;Økonomi&quot;]</td><td>414</td><td>[433, 434]</td><td>&quot;underholdning&quot;</td><td>null</td><td>null</td><td>null</td><td>0.846</td><td>&quot;Positive&quot;</td></tr><tr><td>3012771</td><td>&quot;Morten Bruun fyret i Sønderjys…</td><td>&quot;FODBOLD: Morten Bruun fyret me…</td><td>2023-06-29 06:20:39</td><td>false</td><td>&quot;Kemien mellem spillerne i Supe…</td><td>2006-05-01 14:28:40</td><td>[3177953]</td><td>&quot;article_default&quot;</td><td>&quot;https://ekstrabladet.dk/sport/…</td><td>[]</td><td>[]</td><td>[&quot;Erhverv&quot;, &quot;Kendt&quot;, … &quot;Ansættelsesforhold&quot;]</td><td>142</td><td>[196, 199]</td><td>&quot;sport&quot;</td><td>null</td><td>null</td><td>null</td><td>0.8241</td><td>&quot;Negative&quot;</td></tr><tr><td>3023463</td><td>&quot;Luderne flytter på landet&quot;</td><td>&quot;I landets tyndest befolkede om…</td><td>2023-06-29 06:20:43</td><td>false</td><td>&quot;Det frække erhverv rykker på l…</td><td>2007-03-24 08:27:59</td><td>[3184029]</td><td>&quot;article_default&quot;</td><td>&quot;https://ekstrabladet.dk/nyhede…</td><td>[]</td><td>[]</td><td>[&quot;Livsstil&quot;, &quot;Erotik&quot;]</td><td>118</td><td>[133]</td><td>&quot;nyheder&quot;</td><td>null</td><td>null</td><td>null</td><td>0.7053</td><td>&quot;Neutral&quot;</td></tr><tr><td>3032577</td><td>&quot;Cybersex: Hvornår er man utro?&quot;</td><td>&quot;En flirtende sms til den flott…</td><td>2023-06-29 06:20:46</td><td>false</td><td>&quot;De fleste af os mener, at et t…</td><td>2007-01-18 10:30:37</td><td>[3030463]</td><td>&quot;article_default&quot;</td><td>&quot;https://ekstrabladet.dk/sex_og…</td><td>[]</td><td>[]</td><td>[&quot;Livsstil&quot;, &quot;Partnerskab&quot;]</td><td>565</td><td>[]</td><td>&quot;sex_og_samliv&quot;</td><td>null</td><td>null</td><td>null</td><td>0.9307</td><td>&quot;Neutral&quot;</td></tr><tr><td>3033563</td><td>&quot;Kniven for struben-vært får se…</td><td>&quot;I aftenens udgave af &#x27;Med kniv…</td><td>2023-06-29 06:20:47</td><td>false</td><td>&quot;Når man ser fjerde program i T…</td><td>2007-03-27 10:22:08</td><td>[3005524, 3005525]</td><td>&quot;article_default&quot;</td><td>&quot;https://ekstrabladet.dk/underh…</td><td>[]</td><td>[]</td><td>[&quot;Livsstil&quot;, &quot;Underholdning&quot;, … &quot;Mad og drikke&quot;]</td><td>414</td><td>[433, 436]</td><td>&quot;underholdning&quot;</td><td>null</td><td>null</td><td>null</td><td>0.9371</td><td>&quot;Neutral&quot;</td></tr><tr><td>3034608</td><td>&quot;Willy Strube har begået selvmo…</td><td>&quot;Den tidligere SiD-chef tog sit…</td><td>2023-06-29 06:20:49</td><td>false</td><td>&quot;Den tidligere formand for Indu…</td><td>2001-10-19 12:30:00</td><td>[3204848]</td><td>&quot;article_default&quot;</td><td>&quot;https://ekstrabladet.dk/nyhede…</td><td>[&quot;Willy Strube&quot;, &quot;Willy Strube&quot;, &quot;Willy Strube&quot;]</td><td>[&quot;PER&quot;, &quot;PER&quot;, &quot;PER&quot;]</td><td>[&quot;Kriminalitet&quot;, &quot;Erhverv&quot;, … &quot;Offentlig instans&quot;]</td><td>118</td><td>[130]</td><td>&quot;nyheder&quot;</td><td>null</td><td>null</td><td>null</td><td>0.9971</td><td>&quot;Negative&quot;</td></tr><tr><td>3034849</td><td>&quot;Venner for livet&quot;</td><td>&quot;VK-REGERINGEN&quot;</td><td>2023-06-29 06:20:50</td><td>false</td><td>&quot;VK-REGERINGEN\n",
       "håndplukkede Bjø…</td><td>2003-01-09 06:00:00</td><td>null</td><td>&quot;article_default&quot;</td><td>&quot;https://ekstrabladet.dk/incomi…</td><td>[]</td><td>[]</td><td>[&quot;Kendt&quot;, &quot;Politik&quot;, &quot;National politik&quot;]</td><td>2</td><td>[]</td><td>&quot;incoming&quot;</td><td>null</td><td>null</td><td>null</td><td>0.8454</td><td>&quot;Neutral&quot;</td></tr><tr><td>3035648</td><td>&quot;Dronning af escort-branchen&quot;</td><td>&quot;Trine Michelsen hjælper københ…</td><td>2023-06-29 06:20:52</td><td>false</td><td>&quot;En af escortbranchens største …</td><td>2003-06-17 07:10:00</td><td>[3082573]</td><td>&quot;article_default&quot;</td><td>&quot;https://ekstrabladet.dk/krimi/…</td><td>[]</td><td>[]</td><td>[&quot;Erhverv&quot;, &quot;Livsstil&quot;, … &quot;Erotik&quot;]</td><td>140</td><td>[]</td><td>&quot;krimi&quot;</td><td>null</td><td>null</td><td>null</td><td>0.8814</td><td>&quot;Neutral&quot;</td></tr><tr><td>3036444</td><td>&quot;Mia kendte sandsynligvis sin m…</td><td>&quot;Hun var ikke den type, der søg…</td><td>2023-06-29 06:20:54</td><td>false</td><td>&quot;Den 12-årige Mia Teglgaard Spr…</td><td>2003-07-13 19:50:00</td><td>null</td><td>&quot;article_default&quot;</td><td>&quot;https://ekstrabladet.dk/krimi/…</td><td>[&quot;Mia Teglgaard Sprotte&quot;, &quot;Erik Andersen&quot;, … &quot;Mia Teglgaard Sprotte&quot;]</td><td>[&quot;PER&quot;, &quot;PER&quot;, … &quot;PER&quot;]</td><td>[&quot;Kriminalitet&quot;, &quot;Personfarlig kriminalitet&quot;]</td><td>140</td><td>[]</td><td>&quot;krimi&quot;</td><td>null</td><td>null</td><td>null</td><td>0.9752</td><td>&quot;Negative&quot;</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (10, 21)\n",
       "┌───────────┬───────────┬───────────┬───────────┬───┬───────────┬───────────┬───────────┬──────────┐\n",
       "│ article_i ┆ title     ┆ subtitle  ┆ last_modi ┆ … ┆ total_pag ┆ total_rea ┆ sentiment ┆ sentimen │\n",
       "│ d         ┆ ---       ┆ ---       ┆ fied_time ┆   ┆ eviews    ┆ d_time    ┆ _score    ┆ t_label  │\n",
       "│ ---       ┆ str       ┆ str       ┆ ---       ┆   ┆ ---       ┆ ---       ┆ ---       ┆ ---      │\n",
       "│ i32       ┆           ┆           ┆ datetime[ ┆   ┆ i32       ┆ f32       ┆ f32       ┆ str      │\n",
       "│           ┆           ┆           ┆ μs]       ┆   ┆           ┆           ┆           ┆          │\n",
       "╞═══════════╪═══════════╪═══════════╪═══════════╪═══╪═══════════╪═══════════╪═══════════╪══════════╡\n",
       "│ 3001353   ┆ Natascha  ┆ Politiet  ┆ 2023-06-2 ┆ … ┆ null      ┆ null      ┆ 0.9955    ┆ Negative │\n",
       "│           ┆ var ikke  ┆ frygter   ┆ 9         ┆   ┆           ┆           ┆           ┆          │\n",
       "│           ┆ den       ┆ nu, at    ┆ 06:20:33  ┆   ┆           ┆           ┆           ┆          │\n",
       "│           ┆ første    ┆ Natasc…   ┆           ┆   ┆           ┆           ┆           ┆          │\n",
       "│ 3003065   ┆ Kun Star  ┆ Biografgæ ┆ 2023-06-2 ┆ … ┆ null      ┆ null      ┆ 0.846     ┆ Positive │\n",
       "│           ┆ Wars      ┆ ngerne    ┆ 9         ┆   ┆           ┆           ┆           ┆          │\n",
       "│           ┆ tjente    ┆ strømmer  ┆ 06:20:35  ┆   ┆           ┆           ┆           ┆          │\n",
       "│           ┆ mere      ┆ ind f…    ┆           ┆   ┆           ┆           ┆           ┆          │\n",
       "│ 3012771   ┆ Morten    ┆ FODBOLD:  ┆ 2023-06-2 ┆ … ┆ null      ┆ null      ┆ 0.8241    ┆ Negative │\n",
       "│           ┆ Bruun     ┆ Morten    ┆ 9         ┆   ┆           ┆           ┆           ┆          │\n",
       "│           ┆ fyret i   ┆ Bruun     ┆ 06:20:39  ┆   ┆           ┆           ┆           ┆          │\n",
       "│           ┆ Sønderjys ┆ fyret me… ┆           ┆   ┆           ┆           ┆           ┆          │\n",
       "│           ┆ …         ┆           ┆           ┆   ┆           ┆           ┆           ┆          │\n",
       "│ 3023463   ┆ Luderne   ┆ I landets ┆ 2023-06-2 ┆ … ┆ null      ┆ null      ┆ 0.7053    ┆ Neutral  │\n",
       "│           ┆ flytter   ┆ tyndest   ┆ 9         ┆   ┆           ┆           ┆           ┆          │\n",
       "│           ┆ på landet ┆ befolkede ┆ 06:20:43  ┆   ┆           ┆           ┆           ┆          │\n",
       "│           ┆           ┆ om…       ┆           ┆   ┆           ┆           ┆           ┆          │\n",
       "│ 3032577   ┆ Cybersex: ┆ En        ┆ 2023-06-2 ┆ … ┆ null      ┆ null      ┆ 0.9307    ┆ Neutral  │\n",
       "│           ┆ Hvornår   ┆ flirtende ┆ 9         ┆   ┆           ┆           ┆           ┆          │\n",
       "│           ┆ er man    ┆ sms til   ┆ 06:20:46  ┆   ┆           ┆           ┆           ┆          │\n",
       "│           ┆ utro?     ┆ den       ┆           ┆   ┆           ┆           ┆           ┆          │\n",
       "│           ┆           ┆ flott…    ┆           ┆   ┆           ┆           ┆           ┆          │\n",
       "│ 3033563   ┆ Kniven    ┆ I         ┆ 2023-06-2 ┆ … ┆ null      ┆ null      ┆ 0.9371    ┆ Neutral  │\n",
       "│           ┆ for strub ┆ aftenens  ┆ 9         ┆   ┆           ┆           ┆           ┆          │\n",
       "│           ┆ en-vært   ┆ udgave af ┆ 06:20:47  ┆   ┆           ┆           ┆           ┆          │\n",
       "│           ┆ får se…   ┆ 'Med      ┆           ┆   ┆           ┆           ┆           ┆          │\n",
       "│           ┆           ┆ kniv…     ┆           ┆   ┆           ┆           ┆           ┆          │\n",
       "│ 3034608   ┆ Willy     ┆ Den       ┆ 2023-06-2 ┆ … ┆ null      ┆ null      ┆ 0.9971    ┆ Negative │\n",
       "│           ┆ Strube    ┆ tidligere ┆ 9         ┆   ┆           ┆           ┆           ┆          │\n",
       "│           ┆ har       ┆ SiD-chef  ┆ 06:20:49  ┆   ┆           ┆           ┆           ┆          │\n",
       "│           ┆ begået    ┆ tog sit…  ┆           ┆   ┆           ┆           ┆           ┆          │\n",
       "│           ┆ selvmo…   ┆           ┆           ┆   ┆           ┆           ┆           ┆          │\n",
       "│ 3034849   ┆ Venner    ┆ VK-REGERI ┆ 2023-06-2 ┆ … ┆ null      ┆ null      ┆ 0.8454    ┆ Neutral  │\n",
       "│           ┆ for livet ┆ NGEN      ┆ 9         ┆   ┆           ┆           ┆           ┆          │\n",
       "│           ┆           ┆           ┆ 06:20:50  ┆   ┆           ┆           ┆           ┆          │\n",
       "│ 3035648   ┆ Dronning  ┆ Trine     ┆ 2023-06-2 ┆ … ┆ null      ┆ null      ┆ 0.8814    ┆ Neutral  │\n",
       "│           ┆ af escort ┆ Michelsen ┆ 9         ┆   ┆           ┆           ┆           ┆          │\n",
       "│           ┆ -branchen ┆ hjælper   ┆ 06:20:52  ┆   ┆           ┆           ┆           ┆          │\n",
       "│           ┆           ┆ københ…   ┆           ┆   ┆           ┆           ┆           ┆          │\n",
       "│ 3036444   ┆ Mia       ┆ Hun var   ┆ 2023-06-2 ┆ … ┆ null      ┆ null      ┆ 0.9752    ┆ Negative │\n",
       "│           ┆ kendte    ┆ ikke den  ┆ 9         ┆   ┆           ┆           ┆           ┆          │\n",
       "│           ┆ sandsynli ┆ type, der ┆ 06:20:54  ┆   ┆           ┆           ┆           ┆          │\n",
       "│           ┆ gvis sin  ┆ søg…      ┆           ┆   ┆           ┆           ┆           ┆          │\n",
       "│           ┆ m…        ┆           ┆           ┆   ┆           ┆           ┆           ┆          │\n",
       "└───────────┴───────────┴───────────┴───────────┴───┴───────────┴───────────┴───────────┴──────────┘"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_articles = pl.read_parquet(PATH.joinpath(\"articles.parquet\"))\n",
    "df_articles.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n",
      "DataFrame after tokenization:\n"
     ]
    }
   ],
   "source": [
    "TRANSFORMER_MODEL_NAME = \"FacebookAI/xlm-roberta-base\"\n",
    "TEXT_COLUMNS_TO_USE = [DEFAULT_SUBTITLE_COL, DEFAULT_TITLE_COL]\n",
    "MAX_TITLE_LENGTH = 30\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(device)\n",
    "\n",
    "# LOAD HUGGINGFACE and move to device immediately:\n",
    "transformer_model = AutoModel.from_pretrained(TRANSFORMER_MODEL_NAME).to(device)\n",
    "transformer_tokenizer = AutoTokenizer.from_pretrained(TRANSFORMER_MODEL_NAME)\n",
    "\n",
    "# We'll init the word embeddings using the\n",
    "word2vec_embedding = get_transformers_word_embeddings(transformer_model)\n",
    "\n",
    "# Concatenate text columns\n",
    "df_articles, cat_cal = concat_str_columns(df_articles, columns=TEXT_COLUMNS_TO_USE)\n",
    "\n",
    "# Get tokenized version\n",
    "df_articles, token_col_title = convert_text2encoding_with_transformers(\n",
    "    df_articles, transformer_tokenizer, cat_cal, max_length=MAX_TITLE_LENGTH\n",
    ")\n",
    "\n",
    "print(\"DataFrame after tokenization:\")\n",
    "# print(df_articles[token_col_title][0].shape)\n",
    "article_mapping = create_article_id_to_value_mapping(df=df_articles, value_col=token_col_title)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30\n",
      "30\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(len(df_articles[\"subtitle-title_encode_FacebookAI/xlm-roberta-base\"][0]))\n",
    "print(len(df_articles[\"subtitle-title_encode_FacebookAI/xlm-roberta-base\"][1]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embedding tokenized article title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from tqdm import tqdm\n",
    "# from utils._python import batch_items_generator\n",
    "\n",
    "\n",
    "# BATCH_SIZE = 32\n",
    "# n_batches = int(np.ceil(df_articles.height / BATCH_SIZE))\n",
    "\n",
    "# chunked_text_list = batch_items_generator(df_articles[DEFAULT_TITLE_COL].to_list(), BATCH_SIZE)\n",
    "# embeddings = (\n",
    "#     generate_embeddings_with_transformers(\n",
    "#         model=transformer_model,\n",
    "#         tokenizer=transformer_tokenizer,\n",
    "#         text_list=text_list,\n",
    "#         batch_size=BATCH_SIZE,\n",
    "#         disable_tqdm=True,\n",
    "#     )\n",
    "#     for text_list in tqdm(\n",
    "#         chunked_text_list, desc=\"Encoding\", total=n_batches, unit=\"text\"\n",
    "#     )\n",
    "# )\n",
    "# embeddings = torch.vstack(list(embeddings))\n",
    "# # print(embeddings.shape)\n",
    "# # embedded_title = f\"{DEFAULT_TITLE_COL}_embedded\"\n",
    "\n",
    "# # df_articles = df_articles.with_columns(pl.Series(embedded_title, embeddings.to(\"cpu\").numpy()))\n",
    "\n",
    "# # article_mapping = create_article_id_to_value_mapping(\n",
    "# #     df=df_articles, value_col=embedded_title\n",
    "# # )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(embeddings[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reduce dimensionality of embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.decomposition import PCA\n",
    "# from sklearn.manifold import TSNE\n",
    "# from sklearn.metrics import explained_variance_score\n",
    "# import numpy as np\n",
    "\n",
    "# def reduce_and_analyze_dimensionality(embeddings_array, target_dims=[24, 32, 64, 128, 256]):\n",
    "#     \"\"\"\n",
    "#     Reduce dimensionality using different methods and analyze information retention\n",
    "#     \"\"\"\n",
    "#     results = {}\n",
    "    \n",
    "#     # PCA Analysis for different dimensions\n",
    "#     for dim in target_dims:\n",
    "#         # PCA\n",
    "#         pca = PCA(n_components=dim)\n",
    "#         reduced_data_pca = pca.fit_transform(embeddings_array)\n",
    "        \n",
    "#         # Calculate explained variance ratio\n",
    "#         explained_var = np.sum(pca.explained_variance_ratio_) * 100\n",
    "        \n",
    "#         results[dim] = {\n",
    "#             'method': 'PCA',\n",
    "#             'explained_variance_ratio': explained_var,\n",
    "#             'reduced_data': reduced_data_pca\n",
    "#         }\n",
    "        \n",
    "#         print(f\"\\nDimensionality Reduction to {dim} dimensions:\")\n",
    "#         print(f\"Explained variance ratio (PCA): {explained_var:.2f}%\")\n",
    "#         print(f\"Shape after reduction: {reduced_data_pca.shape}\")\n",
    "\n",
    "#     return results\n",
    "\n",
    "# # Convert embeddings to numpy array if it's not already\n",
    "# embeddings_numpy = embeddings.cpu().numpy()\n",
    "\n",
    "# # Analyze different dimensionality reductions\n",
    "# reduction_results = reduce_and_analyze_dimensionality(embeddings_numpy)\n",
    "\n",
    "# # Choose the dimension that provides good balance \n",
    "# # between compression and information retention\n",
    "# chosen_dim = hparams_nrms.__dict__['title_size']  # Adjust based on analysis results\n",
    "# pca = PCA(n_components=chosen_dim)\n",
    "# reduced_embeddings = pca.fit_transform(embeddings_numpy)\n",
    "\n",
    "# # Update the dataframe with reduced embeddings\n",
    "# embedded_title = f\"{DEFAULT_TITLE_COL}_embedded_reduced\"\n",
    "# df_articles = df_articles.with_columns(pl.Series(embedded_title, reduced_embeddings))\n",
    "\n",
    "# # Create new article mapping with reduced embeddings\n",
    "# article_mapping = create_article_id_to_value_mapping(\n",
    "#     df=df_articles, value_col=embedded_title\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Init dataloaders\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting preprocessing...\n",
      "Preprocessing data...\n",
      "(2342, 6)\n",
      "Data preprocessing completed in 0.26 seconds.\n",
      "Starting preprocessing...\n",
      "Preprocessing data...\n",
      "(2430, 6)\n",
      "Data preprocessing completed in 0.19 seconds.\n"
     ]
    }
   ],
   "source": [
    "train_dataset = NRMSDataSet(\n",
    "    behaviors=df_train,\n",
    "    article_dict=article_mapping,\n",
    "    unknown_representation=\"zeros\",\n",
    "    history_column=DEFAULT_HISTORY_ARTICLE_ID_COL,\n",
    "    eval_mode=False,\n",
    ")\n",
    "val_dataset = NRMSDataSet(\n",
    "    behaviors=df_validation,\n",
    "    article_dict=article_mapping,\n",
    "    unknown_representation=\"zeros\",\n",
    "    history_column=DEFAULT_HISTORY_ARTICLE_ID_COL,\n",
    "    eval_mode=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample 0:\n",
      "his_input_title shape: torch.Size([40, 30])\n",
      "pred_input_title shape: torch.Size([5, 30]) 3990450.0\n",
      "Targets shape: torch.Size([5]) , torch.float32 1.0\n",
      "impression id: 263453510\n",
      "Sample 1:\n",
      "his_input_title shape: torch.Size([40, 30])\n",
      "pred_input_title shape: torch.Size([5, 30]) 4632353.0\n",
      "Targets shape: torch.Size([5]) , torch.float32 1.0\n",
      "impression id: 313295046\n",
      "Sample 2:\n",
      "his_input_title shape: torch.Size([40, 30])\n",
      "pred_input_title shape: torch.Size([5, 30]) 4784238.0\n",
      "Targets shape: torch.Size([5]) , torch.float32 1.0\n",
      "impression id: 505753169\n",
      "Sample 3:\n",
      "his_input_title shape: torch.Size([40, 30])\n",
      "pred_input_title shape: torch.Size([5, 30]) 3689217.0\n",
      "Targets shape: torch.Size([5]) , torch.float32 1.0\n",
      "impression id: 218529809\n",
      "Sample 4:\n",
      "his_input_title shape: torch.Size([40, 30])\n",
      "pred_input_title shape: torch.Size([5, 30]) 3671328.0\n",
      "Targets shape: torch.Size([5]) , torch.float32 1.0\n",
      "impression id: 428958661\n"
     ]
    }
   ],
   "source": [
    "for idx in range(5):\n",
    "    sample = train_dataset[idx]\n",
    "    print(f\"Sample {idx}:\")\n",
    "    print(f\"his_input_title shape: {sample[0][0].shape}\")\n",
    "    print(f\"pred_input_title shape: {sample[0][1].shape} {sample[0][1].sum()}\")\n",
    "    print(f\"Targets shape: {sample[1].shape} , {sample[1].dtype} {sample[1].sum()}\")\n",
    "    print(f\"impression id: {sample[2]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "\n",
    "def collate_fn_with_global_padding(batch, max_len_pred, apply_padding_to_targets : bool = True):\n",
    "    try:\n",
    "        his_input_titles = [item[0][0] for item in batch]  # History inputs\n",
    "        pred_input_titles = [item[0][1] for item in batch]  # Prediction inputs\n",
    "        batch_ys = [item[1] for item in batch]  # Targets\n",
    "        impression_id = torch.tensor([item[2] for item in batch], dtype=torch.int64)  # Impression ID\n",
    "        \n",
    "\n",
    "        # Pad sequences to the global maximum length\n",
    "        his_input_titles_padded = pad_sequence(his_input_titles, batch_first=True, padding_value=0)\n",
    "\n",
    "        # Pad prediction inputs and adjust to the global maximum length\n",
    "        pred_input_titles_padded = pad_sequence(pred_input_titles, batch_first=True, padding_value=0)\n",
    "        if pred_input_titles_padded.size(1) < max_len_pred:\n",
    "            # Add padding if sequence length is shorter than max_len_pred\n",
    "            pad_size = max_len_pred - pred_input_titles_padded.size(1)\n",
    "            pred_input_titles_padded = torch.nn.functional.pad(\n",
    "                pred_input_titles_padded, (0, 0, 0, pad_size), value=0\n",
    "            )\n",
    "        elif pred_input_titles_padded.size(1) > max_len_pred:\n",
    "            # Trim if sequence length exceeds max_len_pred\n",
    "            pred_input_titles_padded = pred_input_titles_padded[:, :max_len_pred, :]\n",
    "\n",
    "        # Pad targets to the global maximum length\n",
    "        if apply_padding_to_targets:\n",
    "            batch_ys_padded = pad_sequence(batch_ys, batch_first=True, padding_value=-1)\n",
    "            if batch_ys_padded.size(1) < max_len_pred:\n",
    "                pad_size = max_len_pred - batch_ys_padded.size(1)\n",
    "                batch_ys_padded = torch.nn.functional.pad(batch_ys_padded, (0, pad_size), value=-1)\n",
    "            elif batch_ys_padded.size(1) > max_len_pred:\n",
    "                batch_ys_padded = batch_ys_padded[:, :max_len_pred]\n",
    "\n",
    "            return (his_input_titles_padded, pred_input_titles_padded), batch_ys_padded, impression_id\n",
    "        else:\n",
    "            return (his_input_titles_padded, pred_input_titles_padded), batch_ys, impression_id\n",
    "    except Exception as e:\n",
    "        print(f\"Error in collate_fn: {e}\")\n",
    "        raise\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wrap the dataset with DataLoader\n",
    "train_dataloader_temp = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=64,    # Set your desired batch size\n",
    "    shuffle=True,\n",
    "    num_workers=0,\n",
    "    collate_fn=lambda batch: collate_fn_with_global_padding(batch, max_inview_length_validation)\n",
    ")\n",
    "\n",
    "val_dataloader_temp = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=128,    # Set your desired batch size\n",
    "    shuffle=False,\n",
    "    num_workers=0,\n",
    "    collate_fn=lambda batch: collate_fn_with_global_padding(batch, max_inview_length_validation)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([128, 88, 30])\n",
      "torch.Size([128, 88])\n",
      "tensor([492351673, 541994952,  41413363,   6547110, 483144493, 306863492,\n",
      "        188499342, 376338783, 496699482, 376305136, 242499972, 112624555,\n",
      "        470931961, 355758311, 431982750,   8892362, 116083152,  41179635,\n",
      "        183822814,  94070682, 152221802, 496236449, 515510823, 297770511,\n",
      "        433302691,  99001208,  93352262, 285619396, 443426532, 246135945,\n",
      "        509369587, 490290108, 551296965, 174230693, 367985571, 479665107,\n",
      "         30998547, 279611080, 453251043, 157856293, 122634706, 456339157,\n",
      "        399941614, 421158450, 288988428,  70338502,  68624355, 523664442,\n",
      "        171689010, 186160817, 269774227, 206704480, 290977107, 547985639,\n",
      "        392267846, 299649567, 317538614,   6577052,  39172265, 497086263,\n",
      "        469106503, 349636006, 281365694,   6311783, 506634973, 470172613,\n",
      "        543043535, 230077452, 206820270, 159393741, 352136718, 151784627,\n",
      "        334613972,  27153919, 349589514,  85007978, 290011533, 530914970,\n",
      "        473189721, 504814310, 184869821, 102586528, 124028896, 242857491,\n",
      "           197847,  48130002, 216908838, 409459266, 203381766, 528301959,\n",
      "        432140128, 440490248, 102833729, 138755404,  35592341, 511410167,\n",
      "        237337642, 317027797,   6010051, 227475118, 159732420, 184287546,\n",
      "        463138460, 273001993,  48157379, 417162050, 466530201, 431821959,\n",
      "         73778367, 136704539, 417158311, 166422686, 280424556, 267721557,\n",
      "         26193006, 274845440, 436095090, 155088125, 373208792,  74252753,\n",
      "        438108288, 508060350, 489012318, 130281824, 505328617, 176842956,\n",
      "        536165051,  73213466])\n",
      "Batch loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "for batch in val_dataloader_temp:\n",
    "    (his_input_titles_padded, pred_input_titles_padded), batch_ys_padded, impression_id = batch\n",
    "    print(pred_input_titles_padded.shape)  # Look at one padded sequence\n",
    "    print(batch_ys_padded.shape)  # Look at one padded sequence\n",
    "    print(impression_id)\n",
    "\n",
    "    print(\"Batch loaded successfully!\")\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # THIS CODE SHOULD ONLY RUN WHEN GENERATING THE DATA FOR THE FIRST TIME\n",
    "# import torch\n",
    "# from tqdm import tqdm\n",
    "\n",
    "# # Function to preprocess and save data\n",
    "# def preprocess_and_save(dataloader, filepath, device=\"cuda\"):\n",
    "#     all_inputs_his = []\n",
    "#     all_inputs_pred = []\n",
    "#     all_targets = []\n",
    "#     all_impression_ids = []\n",
    "\n",
    "#     # Iterate over DataLoader and collect data\n",
    "#     for (his_inputs, pred_inputs), targets, impressionID in tqdm(dataloader, desc=\"Processing Data\"):\n",
    "#         all_inputs_his.append(his_inputs)\n",
    "#         all_inputs_pred.append(pred_inputs)\n",
    "#         all_targets.append(targets)\n",
    "#         all_impression_ids.append(impressionID)\n",
    "\n",
    "#     # Concatenate all batches into a single tensor\n",
    "#     all_inputs_his = torch.cat(all_inputs_his).to(device)\n",
    "#     all_inputs_pred = torch.cat(all_inputs_pred).to(device)\n",
    "#     all_targets = torch.cat(all_targets).to(device)\n",
    "#     all_impression_ids = torch.cat(all_impression_ids).to(device)\n",
    "\n",
    "#     # Save the preprocessed data as a tuple\n",
    "#     torch.save((all_inputs_his, all_inputs_pred, all_targets, all_impression_ids), filepath)\n",
    "#     print(f\"Data saved to {filepath}\")\n",
    "\n",
    "# # Save train and validation data\n",
    "# preprocess_and_save(val_dataloader_temp, \"val_data_small_dataset_with_impression_ids.pt\", device=\"cuda\")\n",
    "\n",
    "# preprocess_and_save(train_dataloader_temp, \"train_data_small_dataset_with_impression_ids.pt\", device=\"cuda\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def load_preprocessed_data(filepath, device=\"cuda\"):\n",
    "#     # Load the data from the .pt file\n",
    "#     data = torch.load(filepath)\n",
    "\n",
    "#     # Unpack the data\n",
    "#     his_inputs, pred_inputs, targets, impression_ids = data\n",
    "\n",
    "#     # Move the data to the specified device\n",
    "#     his_inputs = his_inputs.to(device, non_blocking=True)\n",
    "#     pred_inputs = pred_inputs.to(device, non_blocking=True)\n",
    "#     targets = targets.to(device, non_blocking=True)\n",
    "#     impression_ids = impression_ids.to(device, non_blocking=True)\n",
    "\n",
    "#     return his_inputs, pred_inputs, targets, impression_ids\n",
    "\n",
    "# # Example: Load train and validation data\n",
    "# device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "# train_his_inputs, train_pred_inputs, train_targets, impression_ids = load_preprocessed_data(\"train_data_small_dataset_with_impression_ids.pt\", device)\n",
    "# val_his_inputs, val_pred_inputs, val_targets, impression_ids = load_preprocessed_data(\"val_data_small_dataset_with_impression_ids.pt\", device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def create_batches(inputs, targets, impression_ids, batch_size):\n",
    "#     his_inputs, pred_inputs = inputs\n",
    "#     for i in range(0, his_inputs.size(0), batch_size):\n",
    "#         his_batch = his_inputs[i:i+batch_size]\n",
    "#         pred_batch = pred_inputs[i:i+batch_size]\n",
    "#         target_batch = targets[i:i+batch_size]\n",
    "#         impression_id_batch = impression_ids[i:i+batch_size]\n",
    "#         yield (his_batch, pred_batch), target_batch, impression_id_batch\n",
    "\n",
    "# # Set the batch size\n",
    "# batch_size = 64\n",
    "\n",
    "# # Example: Create batches for train and validation data\n",
    "# #train_batches = create_batches((train_his_inputs, train_pred_inputs), train_targets, batch_size)\n",
    "# #val_batches = create_batches((val_his_inputs, val_pred_inputs), val_targets, batch_size)\n",
    "\n",
    "# train_batches = list(create_batches((train_his_inputs, train_pred_inputs), train_targets, impression_ids, batch_size))\n",
    "# val_batches = list(create_batches((val_his_inputs, val_pred_inputs), val_targets, impression_ids, batch_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for (his_batch, pred_batch), target_batch, impression_ids_batch in train_batches:\n",
    "#     print(f\"his_batch device: {his_batch.device}\")\n",
    "#     print(f\"pred_batch device: {pred_batch.device}\")\n",
    "#     print(f\"target_batch device: {target_batch.device}\")\n",
    "#     print(f\"impression_ids_batch device: {impression_ids_batch.device}\")\n",
    "#     break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create hyper-parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'__module__': 'models_pytorch.model_config', '__annotations__': {'title_size': <class 'int'>, 'embedding_dim': <class 'int'>, 'word_emb_dim': <class 'int'>, 'vocab_size': <class 'int'>, 'head_num': <class 'int'>, 'head_dim': <class 'int'>, 'attention_hidden_dim': <class 'int'>, 'optimizer': <class 'str'>, 'loss': <class 'str'>, 'dropout': <class 'float'>, 'learning_rate': <class 'float'>, 'weight_decay': <class 'float'>, 'units_per_layer': list[int]}, 'title_size': 30, 'embedding_dim': 32, 'word_emb_dim': 8, 'vocab_size': 10000, 'head_num': 16, 'head_dim': 768, 'attention_hidden_dim': 768, 'hidden_dim': 4, 'optimizer': 'adam', 'loss': 'cross_entropy_loss', 'dropout': 0.2, 'learning_rate': 0.0001, 'weight_decay': 0.001, 'news_output_dim': 768, 'units_per_layer': [128, 128, 128], '__dict__': <attribute '__dict__' of 'hparams_nrms' objects>, '__weakref__': <attribute '__weakref__' of 'hparams_nrms' objects>, '__doc__': None}\n"
     ]
    }
   ],
   "source": [
    "# see the model parameters: \n",
    "hparams_nrms.attention_hidden_dim = 768\n",
    "hparams_nrms.head_num = 16\n",
    "hparams_nrms.head_dim = 768\n",
    "print(hparams_nrms.__dict__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the NRMS model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "Model device: cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/gustavsiphone/Documents/GitHub/Deeplearning-RecSys-Challenge-2024/models_pytorch/nrms.py:145: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  self.scaler = amp.GradScaler()\n",
      "/Users/gustavsiphone/Documents/GitHub/Deeplearning-RecSys-Challenge-2024/.venv/lib/python3.12/site-packages/torch/amp/grad_scaler.py:132: UserWarning: torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "# Define paths\n",
    "MODEL_NAME = \"NRMS\"\n",
    "LOG_DIR = os.path.join(\"downloads\", \"runs\", MODEL_NAME)\n",
    "MODEL_WEIGHTS = os.path.join(\"downloads\", \"data\", \"state_dict\", MODEL_NAME, \"weights.pth\")\n",
    "\n",
    "# Create directories if they don't exist\n",
    "os.makedirs(LOG_DIR, exist_ok=True)\n",
    "os.makedirs(os.path.dirname(MODEL_WEIGHTS), exist_ok=True)\n",
    "\n",
    "# Define ModelCheckpoint class\n",
    "class ModelCheckpoint:\n",
    "    \"\"\"Saves the model after every epoch if it has the best performance so far.\"\"\"\n",
    "    def __init__(self, filepath, verbose=False, save_best_only=True):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            filepath (str): Path to save the model checkpoint.\n",
    "            verbose (bool): If True, prints a message when the model is saved.\n",
    "            save_best_only (bool): If True, saves only when the model is better than before.\n",
    "        \"\"\"\n",
    "        self.filepath = filepath\n",
    "        self.verbose = verbose\n",
    "        self.save_best_only = save_best_only\n",
    "        self.best_loss = None\n",
    "\n",
    "    def __call__(self, model, val_loss):\n",
    "        if self.best_loss is None:\n",
    "            self.best_loss = val_loss\n",
    "            self.save_checkpoint(model)\n",
    "        elif val_loss < self.best_loss:\n",
    "            self.best_loss = val_loss\n",
    "            self.save_checkpoint(model)\n",
    "\n",
    "    def save_checkpoint(self, model):\n",
    "        torch.save(model.state_dict(), self.filepath)\n",
    "        if self.verbose:\n",
    "            print(f\"Model saved to {self.filepath}\")\n",
    "\n",
    "# Define EarlyStopping class\n",
    "class EarlyStopping:\n",
    "    \"\"\"Early stops the training if validation loss doesn't improve by a given percentage over a patience period.\"\"\"\n",
    "    def __init__(self, patience=3, min_delta=0.05, verbose=False):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            patience (int): Number of epochs to wait after last time validation loss improved by min_delta.\n",
    "            min_delta (float): Minimum percentage improvement required to reset patience.\n",
    "            verbose (bool): If True, prints a message when early stopping is triggered.\n",
    "        \"\"\"\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta  # Minimum percentage improvement\n",
    "        self.verbose = verbose\n",
    "        self.counter = 0\n",
    "        self.best_loss = None\n",
    "        self.early_stop = False\n",
    "\n",
    "    def __call__(self, val_loss):\n",
    "        if self.best_loss is None:\n",
    "            # Initialize best_loss with the first validation loss\n",
    "            self.best_loss = val_loss\n",
    "            self.counter = 0\n",
    "        elif val_loss < self.best_loss * (1 - self.min_delta):\n",
    "            # Significant improvement found\n",
    "            self.best_loss = val_loss\n",
    "            self.counter = 0\n",
    "            if self.verbose:\n",
    "                print(f\"Validation loss improved by at least {self.min_delta*100:.1f}%\")\n",
    "        else:\n",
    "            # No significant improvement\n",
    "            self.counter += 1\n",
    "            if self.verbose:\n",
    "                print(f\"No significant improvement in validation loss. Counter: {self.counter}/{self.patience}\")\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "\n",
    "# Initialize TensorBoard SummaryWriter\n",
    "writer = SummaryWriter(log_dir=LOG_DIR)\n",
    "\n",
    "# Initialize callbacks\n",
    "model_checkpoint = ModelCheckpoint(filepath=MODEL_WEIGHTS, verbose=True, save_best_only=True)\n",
    "# Initialize EarlyStopping\n",
    "early_stopping = EarlyStopping(patience=3, min_delta=0.05, verbose=True)\n",
    "\n",
    "# Initialize your model\n",
    "# Ensure that NRMSModel is a PyTorch nn.Module\n",
    "\n",
    "# CUDA checks\n",
    "#print(f\"CUDA Available: {torch.cuda.is_available()}\")\n",
    "#print(f\"Current Device: {torch.cuda.current_device()}\")\n",
    "#print(f\"Device Name: {torch.cuda.get_device_name()}\")\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "# elif torch.backends.mps.is_available():\n",
    "#     device = torch.device(\"mps\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    \n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Initialize model\n",
    "model = NRMSModel(\n",
    "    hparams=hparams_nrms.__dict__,\n",
    "    word2vec_embedding=word2vec_embedding,\n",
    "    vocab_size=1000,\n",
    "    word_emb_dim=8,\n",
    "    device=device\n",
    ")\n",
    "\n",
    "# model = NRMSDocVecModel(hparams=hparams_nrms.__dict__,\n",
    "#                         device=device)\n",
    "\n",
    "# model = NRMSModel(hparams=hparams_nrms.__dict__,\n",
    "#                   word2vec_embedding=word2vec_embedding,\n",
    "#                         device=device)\n",
    "print(f\"Model device: {next(model.parameters()).device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def print_prediction_details(outputs, targets, k=5):\n",
    "#     \"\"\"Print detailed prediction information for the first k samples\"\"\"\n",
    "#     # Get predicted class (highest score)\n",
    "#     _, predicted = torch.max(outputs.data, 1)\n",
    "    \n",
    "#     # Calculate accuracy for this batch\n",
    "#     correct = (predicted == targets).sum().item()\n",
    "#     total = targets.size(0)\n",
    "#     accuracy = 100 * correct / total\n",
    "    \n",
    "#     # Print details for k samples\n",
    "#     for i in range(min(k, len(targets))):\n",
    "#         print(f\"\\nSample {i}:\")\n",
    "#         print(f\"Predicted probabilities: {torch.softmax(outputs[i], dim=0)}\")\n",
    "#         print(f\"Predicted class: {predicted[i]}, True class: {targets[i]}\")\n",
    "#         print(f\"Correct: {predicted[i] == targets[i]}\")\n",
    "    \n",
    "#     return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NRMSModel(\n",
      "  (newsencoder): NewsEncoder(\n",
      "    (embedding): Embedding(250002, 768)\n",
      "    (self_attention): SelfAttention(\n",
      "      (multihead_attention): MultiheadAttention(\n",
      "        (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
      "      )\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (attention_layer): AttLayer2(\n",
      "      (proj): Linear(in_features=768, out_features=1, bias=False)\n",
      "    )\n",
      "  )\n",
      "  (userencoder): UserEncoder(\n",
      "    (titleencoder): NewsEncoder(\n",
      "      (embedding): Embedding(250002, 768)\n",
      "      (self_attention): SelfAttention(\n",
      "        (multihead_attention): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
      "        )\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (attention_layer): AttLayer2(\n",
      "        (proj): Linear(in_features=768, out_features=1, bias=False)\n",
      "      )\n",
      "    )\n",
      "    (self_attention): SelfAttention(\n",
      "      (multihead_attention): MultiheadAttention(\n",
      "        (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
      "      )\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (attention_layer): AttLayer2(\n",
      "      (proj): Linear(in_features=768, out_features=1, bias=False)\n",
      "    )\n",
      "    (user_projection): Linear(in_features=768, out_features=768, bias=True)\n",
      "  )\n",
      ")\n",
      "Layer: newsencoder.embedding.weight | Size: torch.Size([250002, 768])\n",
      "Layer: newsencoder.self_attention.multihead_attention.in_proj_weight | Size: torch.Size([2304, 768])\n",
      "Layer: newsencoder.self_attention.multihead_attention.in_proj_bias | Size: torch.Size([2304])\n",
      "Layer: newsencoder.self_attention.multihead_attention.out_proj.weight | Size: torch.Size([768, 768])\n",
      "Layer: newsencoder.self_attention.multihead_attention.out_proj.bias | Size: torch.Size([768])\n",
      "Layer: newsencoder.attention_layer.proj.weight | Size: torch.Size([1, 768])\n",
      "Layer: userencoder.self_attention.multihead_attention.in_proj_weight | Size: torch.Size([2304, 768])\n",
      "Layer: userencoder.self_attention.multihead_attention.in_proj_bias | Size: torch.Size([2304])\n",
      "Layer: userencoder.self_attention.multihead_attention.out_proj.weight | Size: torch.Size([768, 768])\n",
      "Layer: userencoder.self_attention.multihead_attention.out_proj.bias | Size: torch.Size([768])\n",
      "Layer: userencoder.attention_layer.proj.weight | Size: torch.Size([1, 768])\n",
      "Layer: userencoder.user_projection.weight | Size: torch.Size([768, 768])\n",
      "Layer: userencoder.user_projection.bias | Size: torch.Size([768])\n",
      "\n",
      "Total parameters: 197,318,400\n"
     ]
    }
   ],
   "source": [
    "# 1. Print model architecture\n",
    "print(model)\n",
    "\n",
    "# 2. Print specific layer sizes\n",
    "for name, param in model.named_parameters():\n",
    "    print(f\"Layer: {name} | Size: {param.size()}\")\n",
    "\n",
    "# 3. Get total number of parameters\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"\\nTotal parameters: {total_params:,}\")\n",
    "\n",
    "# 4. Print layer by layer with shapes\n",
    "def print_model_structure(model):\n",
    "    print(\"\\nDetailed Model Structure:\")\n",
    "    for name, module in model.named_children():\n",
    "        print(f\"\\nLayer: {name}\")\n",
    "        print(f\"Type: {type(module).__name__}\")\n",
    "        if hasattr(module, 'weight'):\n",
    "            print(f\"Shape: {module.weight.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for name, param in model.named_parameters():\n",
    "#     print(f\"Layer: {name} | Size: {param.size()} | Parameters: {param.numel()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MAX_GRAD_NORM = np.sqrt(sum(p.numel() for p in model.parameters()))\n",
    "# print(f\"Max grad norm: {MAX_GRAD_NORM}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###\n",
    "Added gradient clipping to avoid exploiding gradients. The paramter MAX_GRAD_NORM is set to 5.0 as this is a common value used in transformers. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# # Variables to track counts\n",
    "# total_inputs = 0\n",
    "# total_targets = 0\n",
    "\n",
    "# # Iterate over the DataLoader\n",
    "# for batch_idx, (inputs, targets, impression_ids) in enumerate(train_dataloader_temp):\n",
    "#     # Move data to GPU\n",
    "#     inputs = [inp.to(device) for inp in inputs]\n",
    "#     targets = targets.to(device)\n",
    "#     impression_ids = impression_ids.to(device)\n",
    "    \n",
    "#     # Print information for the first few batches to avoid delays\n",
    "#     if batch_idx < 5:  # Adjust the number of batches to print as needed\n",
    "#         print(f\"Batch {batch_idx + 1} (on {device}):\")\n",
    "#         print(f\"  - Number of inputs: {len(inputs[0])}\")  # History input\n",
    "#         print(f\"  - Number of targets: {len(targets)}\")   # Target labels\n",
    "#         print(f\"  - Impression IDs: {len(impression_ids)}\")\n",
    "    \n",
    "#     # Update total counts\n",
    "#     total_inputs += len(inputs[0])\n",
    "#     total_targets += len(targets)\n",
    "\n",
    "# # Final counts after iteration\n",
    "# print(f\"\\nTotal number of inputs in train_dataloader_temp: {total_inputs}\")\n",
    "# print(f\"Total number of targets in train_dataloader_temp: {total_targets}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperoptimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NUM_EPOCHS = 30\n",
    "\n",
    "# def objective(trial):\n",
    "#     # Define hyperparameter search space\n",
    "#     hparams = {\n",
    "#         'title_size': 768,\n",
    "#         'history_size': trial.suggest_int('history_size', 5, 20),\n",
    "#         'head_num': trial.suggest_int('head_num', 2, 8),\n",
    "#         'head_dim': trial.suggest_int('head_dim', 4, 16),\n",
    "#         'attention_hidden_dim': trial.suggest_int('attention_hidden_dim', 32, 128),\n",
    "#         'dropout': trial.suggest_float('dropout', 0.1, 0.5),\n",
    "#         'learning_rate': trial.suggest_float('learning_rate', 1e-5, 1e-3, log=True),\n",
    "#         'news_output_dim': trial.suggest_int('news_output_dim', 32, 128),\n",
    "#         'units_per_layer': [trial.suggest_int(f'unit_layer_{i}', 32, 128) for i in range(3)]\n",
    "#     }\n",
    "\n",
    "#     # Initialize model and training components\n",
    "#     model = NRMSDocVecModel(hparams=hparams, device=device)\n",
    "#     criterion = model.get_loss().to(device)\n",
    "#     optimizer = optim.Adam(model.parameters(), \n",
    "#                           lr=hparams['learning_rate'], \n",
    "#                           weight_decay=WEIGHT_DECAY)\n",
    "    \n",
    "#     # Initialize EarlyStopping\n",
    "#     early_stopping = EarlyStopping(patience=3, min_delta=0.05, verbose=True)\n",
    "#     best_val_loss = float('inf')\n",
    "\n",
    "#     for epoch in range(NUM_EPOCHS):\n",
    "#         # Training phase\n",
    "#         model.train()\n",
    "#         train_loss = train_one_epoch(model, train_dataloader_temp, optimizer, criterion)\n",
    "        \n",
    "#         # Validation phase\n",
    "#         val_loss = validate(model, val_dataloader_temp, criterion)\n",
    "        \n",
    "#         # Update best validation loss\n",
    "#         if val_loss < best_val_loss:\n",
    "#             best_val_loss = val_loss\n",
    "\n",
    "#         # Early stopping check\n",
    "#         early_stopping(val_loss)\n",
    "#         if early_stopping.early_stop:\n",
    "#             print(f\"Early stopping triggered at epoch {epoch}\")\n",
    "#             break\n",
    "\n",
    "#         # Report to Optuna\n",
    "#         trial.report(val_loss, epoch)\n",
    "#         if trial.should_prune():\n",
    "#             raise optuna.TrialPruned()\n",
    "\n",
    "#     return best_val_loss\n",
    "\n",
    "# def train_one_epoch(model, dataloader, optimizer, criterion):\n",
    "#     running_loss = 0.0\n",
    "#     batch_count = 0\n",
    "    \n",
    "#     for inputs, targets, impression_ids in dataloader:\n",
    "#         inputs = [inp.to(device) for inp in inputs]\n",
    "#         targets = targets.to(device)\n",
    "#         positive_indices = (targets == 1).nonzero(as_tuple=False)\n",
    "#         targets = positive_indices[:, 1].long()\n",
    "        \n",
    "#         optimizer.zero_grad()\n",
    "#         outputs = model(*inputs)\n",
    "#         loss = criterion(outputs, targets)\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "        \n",
    "#         running_loss += loss.item()\n",
    "#         batch_count += 1\n",
    "    \n",
    "#     return running_loss / batch_count\n",
    "\n",
    "# def validate(model, dataloader, criterion):\n",
    "#     model.eval()\n",
    "#     val_loss = 0.0\n",
    "#     batch_count = 0\n",
    "    \n",
    "#     with torch.no_grad():\n",
    "#         for inputs, targets, impression_ids in dataloader:\n",
    "#             inputs = [inp.to(device) for inp in inputs]\n",
    "#             targets = targets.to(device)\n",
    "#             positive_indices = (targets == 1).nonzero(as_tuple=False)\n",
    "#             targets = positive_indices[:, 1].long()\n",
    "#             outputs = model(*inputs)\n",
    "#             loss = criterion(outputs, targets)\n",
    "#             val_loss += loss.item()\n",
    "#             batch_count += 1\n",
    "    \n",
    "#     return val_loss / batch_count\n",
    "\n",
    "# # Create study and optimize\n",
    "# study = optuna.create_study(direction='minimize')\n",
    "# study.optimize(objective, n_trials=3)  # Run 50 trials\n",
    "\n",
    "# # Print results\n",
    "# print(\"Best trial:\")\n",
    "# trial = study.best_trial\n",
    "\n",
    "# print(\"  Value: \", trial.value)\n",
    "# print(\"  Params: \")\n",
    "# for key, value in trial.params.items():\n",
    "#     print(f\"    {key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the NRMS model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:   0%|          | 0/30 [00:00<?, ?it/s]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/Users/gustavsiphone/Documents/GitHub/Deeplearning-RecSys-Challenge-2024/models_pytorch/nrms.py:162: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast():\n",
      "/Users/gustavsiphone/Documents/GitHub/Deeplearning-RecSys-Challenge-2024/.venv/lib/python3.12/site-packages/torch/amp/autocast_mode.py:266: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reshaped_input shape: torch.Size([2560, 30])\n",
      "x shape: torch.Size([2560, 30])\n",
      "encoded_titles shape: torch.Size([2560, 768])\n",
      "feature_dim: 768\n",
      "click_title_presents shape: torch.Size([64, 40, 768])\n",
      "x shape: torch.Size([5632, 30])\n",
      "reshaped_input shape: torch.Size([2560, 30])\n",
      "x shape: torch.Size([2560, 30])\n",
      "encoded_titles shape: torch.Size([2560, 768])\n",
      "feature_dim: 768\n",
      "click_title_presents shape: torch.Size([64, 40, 768])\n",
      "x shape: torch.Size([5632, 30])\n",
      "reshaped_input shape: torch.Size([2560, 30])\n",
      "x shape: torch.Size([2560, 30])\n",
      "encoded_titles shape: torch.Size([2560, 768])\n",
      "feature_dim: 768\n",
      "click_title_presents shape: torch.Size([64, 40, 768])\n",
      "x shape: torch.Size([5632, 30])\n",
      "reshaped_input shape: torch.Size([2560, 30])\n",
      "x shape: torch.Size([2560, 30])\n",
      "encoded_titles shape: torch.Size([2560, 768])\n",
      "feature_dim: 768\n",
      "click_title_presents shape: torch.Size([64, 40, 768])\n",
      "x shape: torch.Size([5632, 30])\n",
      "reshaped_input shape: torch.Size([2560, 30])\n",
      "x shape: torch.Size([2560, 30])\n",
      "encoded_titles shape: torch.Size([2560, 768])\n",
      "feature_dim: 768\n",
      "click_title_presents shape: torch.Size([64, 40, 768])\n",
      "x shape: torch.Size([5632, 30])\n",
      "reshaped_input shape: torch.Size([2560, 30])\n",
      "x shape: torch.Size([2560, 30])\n",
      "encoded_titles shape: torch.Size([2560, 768])\n",
      "feature_dim: 768\n",
      "click_title_presents shape: torch.Size([64, 40, 768])\n",
      "x shape: torch.Size([5632, 30])\n",
      "reshaped_input shape: torch.Size([2560, 30])\n",
      "x shape: torch.Size([2560, 30])\n",
      "encoded_titles shape: torch.Size([2560, 768])\n",
      "feature_dim: 768\n",
      "click_title_presents shape: torch.Size([64, 40, 768])\n",
      "x shape: torch.Size([5632, 30])\n",
      "reshaped_input shape: torch.Size([2560, 30])\n",
      "x shape: torch.Size([2560, 30])\n",
      "encoded_titles shape: torch.Size([2560, 768])\n",
      "feature_dim: 768\n",
      "click_title_presents shape: torch.Size([64, 40, 768])\n",
      "x shape: torch.Size([5632, 30])\n",
      "reshaped_input shape: torch.Size([2560, 30])\n",
      "x shape: torch.Size([2560, 30])\n",
      "encoded_titles shape: torch.Size([2560, 768])\n",
      "feature_dim: 768\n",
      "click_title_presents shape: torch.Size([64, 40, 768])\n",
      "x shape: torch.Size([5632, 30])\n",
      "reshaped_input shape: torch.Size([2560, 30])\n",
      "x shape: torch.Size([2560, 30])\n",
      "encoded_titles shape: torch.Size([2560, 768])\n",
      "feature_dim: 768\n",
      "click_title_presents shape: torch.Size([64, 40, 768])\n",
      "x shape: torch.Size([5632, 30])\n",
      "reshaped_input shape: torch.Size([2560, 30])\n",
      "x shape: torch.Size([2560, 30])\n",
      "encoded_titles shape: torch.Size([2560, 768])\n",
      "feature_dim: 768\n",
      "click_title_presents shape: torch.Size([64, 40, 768])\n",
      "x shape: torch.Size([5632, 30])\n",
      "reshaped_input shape: torch.Size([2560, 30])\n",
      "x shape: torch.Size([2560, 30])\n",
      "encoded_titles shape: torch.Size([2560, 768])\n",
      "feature_dim: 768\n",
      "click_title_presents shape: torch.Size([64, 40, 768])\n",
      "x shape: torch.Size([5632, 30])\n",
      "reshaped_input shape: torch.Size([2560, 30])\n",
      "x shape: torch.Size([2560, 30])\n",
      "encoded_titles shape: torch.Size([2560, 768])\n",
      "feature_dim: 768\n",
      "click_title_presents shape: torch.Size([64, 40, 768])\n",
      "x shape: torch.Size([5632, 30])\n",
      "reshaped_input shape: torch.Size([2560, 30])\n",
      "x shape: torch.Size([2560, 30])\n",
      "encoded_titles shape: torch.Size([2560, 768])\n",
      "feature_dim: 768\n",
      "click_title_presents shape: torch.Size([64, 40, 768])\n",
      "x shape: torch.Size([5632, 30])\n",
      "reshaped_input shape: torch.Size([2560, 30])\n",
      "x shape: torch.Size([2560, 30])\n",
      "encoded_titles shape: torch.Size([2560, 768])\n",
      "feature_dim: 768\n",
      "click_title_presents shape: torch.Size([64, 40, 768])\n",
      "x shape: torch.Size([5632, 30])\n",
      "reshaped_input shape: torch.Size([2560, 30])\n",
      "x shape: torch.Size([2560, 30])\n",
      "encoded_titles shape: torch.Size([2560, 768])\n",
      "feature_dim: 768\n",
      "click_title_presents shape: torch.Size([64, 40, 768])\n",
      "x shape: torch.Size([5632, 30])\n",
      "reshaped_input shape: torch.Size([2560, 30])\n",
      "x shape: torch.Size([2560, 30])\n",
      "encoded_titles shape: torch.Size([2560, 768])\n",
      "feature_dim: 768\n",
      "click_title_presents shape: torch.Size([64, 40, 768])\n",
      "x shape: torch.Size([5632, 30])\n",
      "reshaped_input shape: torch.Size([2560, 30])\n",
      "x shape: torch.Size([2560, 30])\n",
      "encoded_titles shape: torch.Size([2560, 768])\n",
      "feature_dim: 768\n",
      "click_title_presents shape: torch.Size([64, 40, 768])\n",
      "x shape: torch.Size([5632, 30])\n",
      "reshaped_input shape: torch.Size([2560, 30])\n",
      "x shape: torch.Size([2560, 30])\n",
      "encoded_titles shape: torch.Size([2560, 768])\n",
      "feature_dim: 768\n",
      "click_title_presents shape: torch.Size([64, 40, 768])\n",
      "x shape: torch.Size([5632, 30])\n",
      "reshaped_input shape: torch.Size([2560, 30])\n",
      "x shape: torch.Size([2560, 30])\n",
      "encoded_titles shape: torch.Size([2560, 768])\n",
      "feature_dim: 768\n",
      "click_title_presents shape: torch.Size([64, 40, 768])\n",
      "x shape: torch.Size([5632, 30])\n",
      "reshaped_input shape: torch.Size([2560, 30])\n",
      "x shape: torch.Size([2560, 30])\n",
      "encoded_titles shape: torch.Size([2560, 768])\n",
      "feature_dim: 768\n",
      "click_title_presents shape: torch.Size([64, 40, 768])\n",
      "x shape: torch.Size([5632, 30])\n",
      "reshaped_input shape: torch.Size([2560, 30])\n",
      "x shape: torch.Size([2560, 30])\n",
      "encoded_titles shape: torch.Size([2560, 768])\n",
      "feature_dim: 768\n",
      "click_title_presents shape: torch.Size([64, 40, 768])\n",
      "x shape: torch.Size([5632, 30])\n",
      "reshaped_input shape: torch.Size([2560, 30])\n",
      "x shape: torch.Size([2560, 30])\n",
      "encoded_titles shape: torch.Size([2560, 768])\n",
      "feature_dim: 768\n",
      "click_title_presents shape: torch.Size([64, 40, 768])\n",
      "x shape: torch.Size([5632, 30])\n",
      "reshaped_input shape: torch.Size([2560, 30])\n",
      "x shape: torch.Size([2560, 30])\n",
      "encoded_titles shape: torch.Size([2560, 768])\n",
      "feature_dim: 768\n",
      "click_title_presents shape: torch.Size([64, 40, 768])\n",
      "x shape: torch.Size([5632, 30])\n",
      "reshaped_input shape: torch.Size([2560, 30])\n",
      "x shape: torch.Size([2560, 30])\n",
      "encoded_titles shape: torch.Size([2560, 768])\n",
      "feature_dim: 768\n",
      "click_title_presents shape: torch.Size([64, 40, 768])\n",
      "x shape: torch.Size([5632, 30])\n",
      "reshaped_input shape: torch.Size([2560, 30])\n",
      "x shape: torch.Size([2560, 30])\n",
      "encoded_titles shape: torch.Size([2560, 768])\n",
      "feature_dim: 768\n",
      "click_title_presents shape: torch.Size([64, 40, 768])\n",
      "x shape: torch.Size([5632, 30])\n",
      "reshaped_input shape: torch.Size([2560, 30])\n",
      "x shape: torch.Size([2560, 30])\n",
      "encoded_titles shape: torch.Size([2560, 768])\n",
      "feature_dim: 768\n",
      "click_title_presents shape: torch.Size([64, 40, 768])\n",
      "x shape: torch.Size([5632, 30])\n",
      "reshaped_input shape: torch.Size([2560, 30])\n",
      "x shape: torch.Size([2560, 30])\n",
      "encoded_titles shape: torch.Size([2560, 768])\n",
      "feature_dim: 768\n",
      "click_title_presents shape: torch.Size([64, 40, 768])\n",
      "x shape: torch.Size([5632, 30])\n",
      "reshaped_input shape: torch.Size([2560, 30])\n",
      "x shape: torch.Size([2560, 30])\n",
      "encoded_titles shape: torch.Size([2560, 768])\n",
      "feature_dim: 768\n",
      "click_title_presents shape: torch.Size([64, 40, 768])\n",
      "x shape: torch.Size([5632, 30])\n",
      "reshaped_input shape: torch.Size([2560, 30])\n",
      "x shape: torch.Size([2560, 30])\n",
      "encoded_titles shape: torch.Size([2560, 768])\n",
      "feature_dim: 768\n",
      "click_title_presents shape: torch.Size([64, 40, 768])\n",
      "x shape: torch.Size([5632, 30])\n",
      "reshaped_input shape: torch.Size([2560, 30])\n",
      "x shape: torch.Size([2560, 30])\n",
      "encoded_titles shape: torch.Size([2560, 768])\n",
      "feature_dim: 768\n",
      "click_title_presents shape: torch.Size([64, 40, 768])\n",
      "x shape: torch.Size([5632, 30])\n",
      "reshaped_input shape: torch.Size([2560, 30])\n",
      "x shape: torch.Size([2560, 30])\n",
      "encoded_titles shape: torch.Size([2560, 768])\n",
      "feature_dim: 768\n",
      "click_title_presents shape: torch.Size([64, 40, 768])\n",
      "x shape: torch.Size([5632, 30])\n",
      "reshaped_input shape: torch.Size([2560, 30])\n",
      "x shape: torch.Size([2560, 30])\n",
      "encoded_titles shape: torch.Size([2560, 768])\n",
      "feature_dim: 768\n",
      "click_title_presents shape: torch.Size([64, 40, 768])\n",
      "x shape: torch.Size([5632, 30])\n",
      "reshaped_input shape: torch.Size([2560, 30])\n",
      "x shape: torch.Size([2560, 30])\n",
      "encoded_titles shape: torch.Size([2560, 768])\n",
      "feature_dim: 768\n",
      "click_title_presents shape: torch.Size([64, 40, 768])\n",
      "x shape: torch.Size([5632, 30])\n",
      "reshaped_input shape: torch.Size([2560, 30])\n",
      "x shape: torch.Size([2560, 30])\n",
      "encoded_titles shape: torch.Size([2560, 768])\n",
      "feature_dim: 768\n",
      "click_title_presents shape: torch.Size([64, 40, 768])\n",
      "x shape: torch.Size([5632, 30])\n",
      "reshaped_input shape: torch.Size([2560, 30])\n",
      "x shape: torch.Size([2560, 30])\n",
      "encoded_titles shape: torch.Size([2560, 768])\n",
      "feature_dim: 768\n",
      "click_title_presents shape: torch.Size([64, 40, 768])\n",
      "x shape: torch.Size([5632, 30])\n",
      "reshaped_input shape: torch.Size([1520, 30])\n",
      "x shape: torch.Size([1520, 30])\n",
      "encoded_titles shape: torch.Size([1520, 768])\n",
      "feature_dim: 768\n",
      "click_title_presents shape: torch.Size([38, 40, 768])\n",
      "x shape: torch.Size([3344, 30])\n",
      "reshaped_input shape: torch.Size([5120, 30])\n",
      "x shape: torch.Size([5120, 30])\n",
      "encoded_titles shape: torch.Size([5120, 768])\n",
      "feature_dim: 768\n",
      "click_title_presents shape: torch.Size([128, 40, 768])\n",
      "x shape: torch.Size([11264, 30])\n",
      "reshaped_input shape: torch.Size([5120, 30])\n",
      "x shape: torch.Size([5120, 30])\n",
      "encoded_titles shape: torch.Size([5120, 768])\n",
      "feature_dim: 768\n",
      "click_title_presents shape: torch.Size([128, 40, 768])\n",
      "x shape: torch.Size([11264, 30])\n",
      "reshaped_input shape: torch.Size([5120, 30])\n",
      "x shape: torch.Size([5120, 30])\n",
      "encoded_titles shape: torch.Size([5120, 768])\n",
      "feature_dim: 768\n",
      "click_title_presents shape: torch.Size([128, 40, 768])\n",
      "x shape: torch.Size([11264, 30])\n",
      "reshaped_input shape: torch.Size([5120, 30])\n",
      "x shape: torch.Size([5120, 30])\n",
      "encoded_titles shape: torch.Size([5120, 768])\n",
      "feature_dim: 768\n",
      "click_title_presents shape: torch.Size([128, 40, 768])\n",
      "x shape: torch.Size([11264, 30])\n",
      "reshaped_input shape: torch.Size([5120, 30])\n",
      "x shape: torch.Size([5120, 30])\n",
      "encoded_titles shape: torch.Size([5120, 768])\n",
      "feature_dim: 768\n",
      "click_title_presents shape: torch.Size([128, 40, 768])\n",
      "x shape: torch.Size([11264, 30])\n",
      "reshaped_input shape: torch.Size([5120, 30])\n",
      "x shape: torch.Size([5120, 30])\n",
      "encoded_titles shape: torch.Size([5120, 768])\n",
      "feature_dim: 768\n",
      "click_title_presents shape: torch.Size([128, 40, 768])\n",
      "x shape: torch.Size([11264, 30])\n",
      "reshaped_input shape: torch.Size([5120, 30])\n",
      "x shape: torch.Size([5120, 30])\n",
      "encoded_titles shape: torch.Size([5120, 768])\n",
      "feature_dim: 768\n",
      "click_title_presents shape: torch.Size([128, 40, 768])\n",
      "x shape: torch.Size([11264, 30])\n",
      "reshaped_input shape: torch.Size([5120, 30])\n",
      "x shape: torch.Size([5120, 30])\n",
      "encoded_titles shape: torch.Size([5120, 768])\n",
      "feature_dim: 768\n",
      "click_title_presents shape: torch.Size([128, 40, 768])\n",
      "x shape: torch.Size([11264, 30])\n",
      "reshaped_input shape: torch.Size([5120, 30])\n",
      "x shape: torch.Size([5120, 30])\n",
      "encoded_titles shape: torch.Size([5120, 768])\n",
      "feature_dim: 768\n",
      "click_title_presents shape: torch.Size([128, 40, 768])\n",
      "x shape: torch.Size([11264, 30])\n",
      "reshaped_input shape: torch.Size([5120, 30])\n",
      "x shape: torch.Size([5120, 30])\n",
      "encoded_titles shape: torch.Size([5120, 768])\n",
      "feature_dim: 768\n",
      "click_title_presents shape: torch.Size([128, 40, 768])\n",
      "x shape: torch.Size([11264, 30])\n",
      "reshaped_input shape: torch.Size([5120, 30])\n",
      "x shape: torch.Size([5120, 30])\n",
      "encoded_titles shape: torch.Size([5120, 768])\n",
      "feature_dim: 768\n",
      "click_title_presents shape: torch.Size([128, 40, 768])\n",
      "x shape: torch.Size([11264, 30])\n",
      "reshaped_input shape: torch.Size([5120, 30])\n",
      "x shape: torch.Size([5120, 30])\n",
      "encoded_titles shape: torch.Size([5120, 768])\n",
      "feature_dim: 768\n",
      "click_title_presents shape: torch.Size([128, 40, 768])\n",
      "x shape: torch.Size([11264, 30])\n",
      "reshaped_input shape: torch.Size([5120, 30])\n",
      "x shape: torch.Size([5120, 30])\n",
      "encoded_titles shape: torch.Size([5120, 768])\n",
      "feature_dim: 768\n",
      "click_title_presents shape: torch.Size([128, 40, 768])\n",
      "x shape: torch.Size([11264, 30])\n",
      "reshaped_input shape: torch.Size([5120, 30])\n",
      "x shape: torch.Size([5120, 30])\n",
      "encoded_titles shape: torch.Size([5120, 768])\n",
      "feature_dim: 768\n",
      "click_title_presents shape: torch.Size([128, 40, 768])\n",
      "x shape: torch.Size([11264, 30])\n",
      "reshaped_input shape: torch.Size([5120, 30])\n",
      "x shape: torch.Size([5120, 30])\n",
      "encoded_titles shape: torch.Size([5120, 768])\n",
      "feature_dim: 768\n",
      "click_title_presents shape: torch.Size([128, 40, 768])\n",
      "x shape: torch.Size([11264, 30])\n",
      "reshaped_input shape: torch.Size([5120, 30])\n",
      "x shape: torch.Size([5120, 30])\n",
      "encoded_titles shape: torch.Size([5120, 768])\n",
      "feature_dim: 768\n",
      "click_title_presents shape: torch.Size([128, 40, 768])\n",
      "x shape: torch.Size([11264, 30])\n",
      "reshaped_input shape: torch.Size([5120, 30])\n",
      "x shape: torch.Size([5120, 30])\n",
      "encoded_titles shape: torch.Size([5120, 768])\n",
      "feature_dim: 768\n",
      "click_title_presents shape: torch.Size([128, 40, 768])\n",
      "x shape: torch.Size([11264, 30])\n",
      "reshaped_input shape: torch.Size([5120, 30])\n",
      "x shape: torch.Size([5120, 30])\n",
      "encoded_titles shape: torch.Size([5120, 768])\n",
      "feature_dim: 768\n",
      "click_title_presents shape: torch.Size([128, 40, 768])\n",
      "x shape: torch.Size([11264, 30])\n",
      "reshaped_input shape: torch.Size([5040, 30])\n",
      "x shape: torch.Size([5040, 30])\n",
      "encoded_titles shape: torch.Size([5040, 768])\n",
      "feature_dim: 768\n",
      "click_title_presents shape: torch.Size([126, 40, 768])\n",
      "x shape: torch.Size([11088, 30])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:   3%|▎         | 1/30 [08:05<3:54:42, 485.60s/it, train_loss=1.9602, val_loss=2.3188]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reshaped_input shape: torch.Size([2560, 30])\n",
      "x shape: torch.Size([2560, 30])\n",
      "encoded_titles shape: torch.Size([2560, 768])\n",
      "feature_dim: 768\n",
      "click_title_presents shape: torch.Size([64, 40, 768])\n",
      "x shape: torch.Size([5632, 30])\n",
      "reshaped_input shape: torch.Size([2560, 30])\n",
      "x shape: torch.Size([2560, 30])\n",
      "encoded_titles shape: torch.Size([2560, 768])\n",
      "feature_dim: 768\n",
      "click_title_presents shape: torch.Size([64, 40, 768])\n",
      "x shape: torch.Size([5632, 30])\n",
      "reshaped_input shape: torch.Size([2560, 30])\n",
      "x shape: torch.Size([2560, 30])\n",
      "encoded_titles shape: torch.Size([2560, 768])\n",
      "feature_dim: 768\n",
      "click_title_presents shape: torch.Size([64, 40, 768])\n",
      "x shape: torch.Size([5632, 30])\n",
      "reshaped_input shape: torch.Size([2560, 30])\n",
      "x shape: torch.Size([2560, 30])\n",
      "encoded_titles shape: torch.Size([2560, 768])\n",
      "feature_dim: 768\n",
      "click_title_presents shape: torch.Size([64, 40, 768])\n",
      "x shape: torch.Size([5632, 30])\n",
      "reshaped_input shape: torch.Size([2560, 30])\n",
      "x shape: torch.Size([2560, 30])\n",
      "encoded_titles shape: torch.Size([2560, 768])\n",
      "feature_dim: 768\n",
      "click_title_presents shape: torch.Size([64, 40, 768])\n",
      "x shape: torch.Size([5632, 30])\n",
      "reshaped_input shape: torch.Size([2560, 30])\n",
      "x shape: torch.Size([2560, 30])\n",
      "encoded_titles shape: torch.Size([2560, 768])\n",
      "feature_dim: 768\n",
      "click_title_presents shape: torch.Size([64, 40, 768])\n",
      "x shape: torch.Size([5632, 30])\n",
      "reshaped_input shape: torch.Size([2560, 30])\n",
      "x shape: torch.Size([2560, 30])\n",
      "encoded_titles shape: torch.Size([2560, 768])\n",
      "feature_dim: 768\n",
      "click_title_presents shape: torch.Size([64, 40, 768])\n",
      "x shape: torch.Size([5632, 30])\n",
      "reshaped_input shape: torch.Size([2560, 30])\n",
      "x shape: torch.Size([2560, 30])\n",
      "encoded_titles shape: torch.Size([2560, 768])\n",
      "feature_dim: 768\n",
      "click_title_presents shape: torch.Size([64, 40, 768])\n",
      "x shape: torch.Size([5632, 30])\n",
      "reshaped_input shape: torch.Size([2560, 30])\n",
      "x shape: torch.Size([2560, 30])\n",
      "encoded_titles shape: torch.Size([2560, 768])\n",
      "feature_dim: 768\n",
      "click_title_presents shape: torch.Size([64, 40, 768])\n",
      "x shape: torch.Size([5632, 30])\n",
      "reshaped_input shape: torch.Size([2560, 30])\n",
      "x shape: torch.Size([2560, 30])\n",
      "encoded_titles shape: torch.Size([2560, 768])\n",
      "feature_dim: 768\n",
      "click_title_presents shape: torch.Size([64, 40, 768])\n",
      "x shape: torch.Size([5632, 30])\n",
      "reshaped_input shape: torch.Size([2560, 30])\n",
      "x shape: torch.Size([2560, 30])\n",
      "encoded_titles shape: torch.Size([2560, 768])\n",
      "feature_dim: 768\n",
      "click_title_presents shape: torch.Size([64, 40, 768])\n",
      "x shape: torch.Size([5632, 30])\n",
      "reshaped_input shape: torch.Size([2560, 30])\n",
      "x shape: torch.Size([2560, 30])\n",
      "encoded_titles shape: torch.Size([2560, 768])\n",
      "feature_dim: 768\n",
      "click_title_presents shape: torch.Size([64, 40, 768])\n",
      "x shape: torch.Size([5632, 30])\n",
      "reshaped_input shape: torch.Size([2560, 30])\n",
      "x shape: torch.Size([2560, 30])\n",
      "encoded_titles shape: torch.Size([2560, 768])\n",
      "feature_dim: 768\n",
      "click_title_presents shape: torch.Size([64, 40, 768])\n",
      "x shape: torch.Size([5632, 30])\n",
      "reshaped_input shape: torch.Size([2560, 30])\n",
      "x shape: torch.Size([2560, 30])\n",
      "encoded_titles shape: torch.Size([2560, 768])\n",
      "feature_dim: 768\n",
      "click_title_presents shape: torch.Size([64, 40, 768])\n",
      "x shape: torch.Size([5632, 30])\n",
      "reshaped_input shape: torch.Size([2560, 30])\n",
      "x shape: torch.Size([2560, 30])\n",
      "encoded_titles shape: torch.Size([2560, 768])\n",
      "feature_dim: 768\n",
      "click_title_presents shape: torch.Size([64, 40, 768])\n",
      "x shape: torch.Size([5632, 30])\n",
      "reshaped_input shape: torch.Size([2560, 30])\n",
      "x shape: torch.Size([2560, 30])\n",
      "encoded_titles shape: torch.Size([2560, 768])\n",
      "feature_dim: 768\n",
      "click_title_presents shape: torch.Size([64, 40, 768])\n",
      "x shape: torch.Size([5632, 30])\n",
      "reshaped_input shape: torch.Size([2560, 30])\n",
      "x shape: torch.Size([2560, 30])\n",
      "encoded_titles shape: torch.Size([2560, 768])\n",
      "feature_dim: 768\n",
      "click_title_presents shape: torch.Size([64, 40, 768])\n",
      "x shape: torch.Size([5632, 30])\n",
      "reshaped_input shape: torch.Size([2560, 30])\n",
      "x shape: torch.Size([2560, 30])\n",
      "encoded_titles shape: torch.Size([2560, 768])\n",
      "feature_dim: 768\n",
      "click_title_presents shape: torch.Size([64, 40, 768])\n",
      "x shape: torch.Size([5632, 30])\n",
      "reshaped_input shape: torch.Size([2560, 30])\n",
      "x shape: torch.Size([2560, 30])\n",
      "encoded_titles shape: torch.Size([2560, 768])\n",
      "feature_dim: 768\n",
      "click_title_presents shape: torch.Size([64, 40, 768])\n",
      "x shape: torch.Size([5632, 30])\n",
      "reshaped_input shape: torch.Size([2560, 30])\n",
      "x shape: torch.Size([2560, 30])\n",
      "encoded_titles shape: torch.Size([2560, 768])\n",
      "feature_dim: 768\n",
      "click_title_presents shape: torch.Size([64, 40, 768])\n",
      "x shape: torch.Size([5632, 30])\n",
      "reshaped_input shape: torch.Size([2560, 30])\n",
      "x shape: torch.Size([2560, 30])\n",
      "encoded_titles shape: torch.Size([2560, 768])\n",
      "feature_dim: 768\n",
      "click_title_presents shape: torch.Size([64, 40, 768])\n",
      "x shape: torch.Size([5632, 30])\n",
      "reshaped_input shape: torch.Size([2560, 30])\n",
      "x shape: torch.Size([2560, 30])\n",
      "encoded_titles shape: torch.Size([2560, 768])\n",
      "feature_dim: 768\n",
      "click_title_presents shape: torch.Size([64, 40, 768])\n",
      "x shape: torch.Size([5632, 30])\n",
      "reshaped_input shape: torch.Size([2560, 30])\n",
      "x shape: torch.Size([2560, 30])\n",
      "encoded_titles shape: torch.Size([2560, 768])\n",
      "feature_dim: 768\n",
      "click_title_presents shape: torch.Size([64, 40, 768])\n",
      "x shape: torch.Size([5632, 30])\n",
      "reshaped_input shape: torch.Size([2560, 30])\n",
      "x shape: torch.Size([2560, 30])\n",
      "encoded_titles shape: torch.Size([2560, 768])\n",
      "feature_dim: 768\n",
      "click_title_presents shape: torch.Size([64, 40, 768])\n",
      "x shape: torch.Size([5632, 30])\n",
      "reshaped_input shape: torch.Size([2560, 30])\n",
      "x shape: torch.Size([2560, 30])\n",
      "encoded_titles shape: torch.Size([2560, 768])\n",
      "feature_dim: 768\n",
      "click_title_presents shape: torch.Size([64, 40, 768])\n",
      "x shape: torch.Size([5632, 30])\n",
      "reshaped_input shape: torch.Size([2560, 30])\n",
      "x shape: torch.Size([2560, 30])\n",
      "encoded_titles shape: torch.Size([2560, 768])\n",
      "feature_dim: 768\n",
      "click_title_presents shape: torch.Size([64, 40, 768])\n",
      "x shape: torch.Size([5632, 30])\n",
      "reshaped_input shape: torch.Size([2560, 30])\n",
      "x shape: torch.Size([2560, 30])\n",
      "encoded_titles shape: torch.Size([2560, 768])\n",
      "feature_dim: 768\n",
      "click_title_presents shape: torch.Size([64, 40, 768])\n",
      "x shape: torch.Size([5632, 30])\n",
      "reshaped_input shape: torch.Size([2560, 30])\n",
      "x shape: torch.Size([2560, 30])\n",
      "encoded_titles shape: torch.Size([2560, 768])\n",
      "feature_dim: 768\n",
      "click_title_presents shape: torch.Size([64, 40, 768])\n",
      "x shape: torch.Size([5632, 30])\n",
      "reshaped_input shape: torch.Size([2560, 30])\n",
      "x shape: torch.Size([2560, 30])\n",
      "encoded_titles shape: torch.Size([2560, 768])\n",
      "feature_dim: 768\n",
      "click_title_presents shape: torch.Size([64, 40, 768])\n",
      "x shape: torch.Size([5632, 30])\n",
      "reshaped_input shape: torch.Size([2560, 30])\n",
      "x shape: torch.Size([2560, 30])\n",
      "encoded_titles shape: torch.Size([2560, 768])\n",
      "feature_dim: 768\n",
      "click_title_presents shape: torch.Size([64, 40, 768])\n",
      "x shape: torch.Size([5632, 30])\n",
      "reshaped_input shape: torch.Size([2560, 30])\n",
      "x shape: torch.Size([2560, 30])\n",
      "encoded_titles shape: torch.Size([2560, 768])\n",
      "feature_dim: 768\n",
      "click_title_presents shape: torch.Size([64, 40, 768])\n",
      "x shape: torch.Size([5632, 30])\n",
      "reshaped_input shape: torch.Size([2560, 30])\n",
      "x shape: torch.Size([2560, 30])\n",
      "encoded_titles shape: torch.Size([2560, 768])\n",
      "feature_dim: 768\n",
      "click_title_presents shape: torch.Size([64, 40, 768])\n",
      "x shape: torch.Size([5632, 30])\n",
      "reshaped_input shape: torch.Size([2560, 30])\n",
      "x shape: torch.Size([2560, 30])\n",
      "encoded_titles shape: torch.Size([2560, 768])\n",
      "feature_dim: 768\n",
      "click_title_presents shape: torch.Size([64, 40, 768])\n",
      "x shape: torch.Size([5632, 30])\n",
      "reshaped_input shape: torch.Size([2560, 30])\n",
      "x shape: torch.Size([2560, 30])\n",
      "encoded_titles shape: torch.Size([2560, 768])\n",
      "feature_dim: 768\n",
      "click_title_presents shape: torch.Size([64, 40, 768])\n",
      "x shape: torch.Size([5632, 30])\n",
      "reshaped_input shape: torch.Size([2560, 30])\n",
      "x shape: torch.Size([2560, 30])\n",
      "encoded_titles shape: torch.Size([2560, 768])\n",
      "feature_dim: 768\n",
      "click_title_presents shape: torch.Size([64, 40, 768])\n",
      "x shape: torch.Size([5632, 30])\n",
      "reshaped_input shape: torch.Size([2560, 30])\n",
      "x shape: torch.Size([2560, 30])\n",
      "encoded_titles shape: torch.Size([2560, 768])\n",
      "feature_dim: 768\n",
      "click_title_presents shape: torch.Size([64, 40, 768])\n",
      "x shape: torch.Size([5632, 30])\n",
      "reshaped_input shape: torch.Size([1520, 30])\n",
      "x shape: torch.Size([1520, 30])\n",
      "encoded_titles shape: torch.Size([1520, 768])\n",
      "feature_dim: 768\n",
      "click_title_presents shape: torch.Size([38, 40, 768])\n",
      "x shape: torch.Size([3344, 30])\n",
      "reshaped_input shape: torch.Size([5120, 30])\n",
      "x shape: torch.Size([5120, 30])\n",
      "encoded_titles shape: torch.Size([5120, 768])\n",
      "feature_dim: 768\n",
      "click_title_presents shape: torch.Size([128, 40, 768])\n",
      "x shape: torch.Size([11264, 30])\n",
      "reshaped_input shape: torch.Size([5120, 30])\n",
      "x shape: torch.Size([5120, 30])\n",
      "encoded_titles shape: torch.Size([5120, 768])\n",
      "feature_dim: 768\n",
      "click_title_presents shape: torch.Size([128, 40, 768])\n",
      "x shape: torch.Size([11264, 30])\n",
      "reshaped_input shape: torch.Size([5120, 30])\n",
      "x shape: torch.Size([5120, 30])\n",
      "encoded_titles shape: torch.Size([5120, 768])\n",
      "feature_dim: 768\n",
      "click_title_presents shape: torch.Size([128, 40, 768])\n",
      "x shape: torch.Size([11264, 30])\n",
      "reshaped_input shape: torch.Size([5120, 30])\n",
      "x shape: torch.Size([5120, 30])\n",
      "encoded_titles shape: torch.Size([5120, 768])\n",
      "feature_dim: 768\n",
      "click_title_presents shape: torch.Size([128, 40, 768])\n",
      "x shape: torch.Size([11264, 30])\n",
      "reshaped_input shape: torch.Size([5120, 30])\n",
      "x shape: torch.Size([5120, 30])\n",
      "encoded_titles shape: torch.Size([5120, 768])\n",
      "feature_dim: 768\n",
      "click_title_presents shape: torch.Size([128, 40, 768])\n",
      "x shape: torch.Size([11264, 30])\n",
      "reshaped_input shape: torch.Size([5120, 30])\n",
      "x shape: torch.Size([5120, 30])\n",
      "encoded_titles shape: torch.Size([5120, 768])\n",
      "feature_dim: 768\n",
      "click_title_presents shape: torch.Size([128, 40, 768])\n",
      "x shape: torch.Size([11264, 30])\n",
      "reshaped_input shape: torch.Size([5120, 30])\n",
      "x shape: torch.Size([5120, 30])\n",
      "encoded_titles shape: torch.Size([5120, 768])\n",
      "feature_dim: 768\n",
      "click_title_presents shape: torch.Size([128, 40, 768])\n",
      "x shape: torch.Size([11264, 30])\n",
      "reshaped_input shape: torch.Size([5120, 30])\n",
      "x shape: torch.Size([5120, 30])\n",
      "encoded_titles shape: torch.Size([5120, 768])\n",
      "feature_dim: 768\n",
      "click_title_presents shape: torch.Size([128, 40, 768])\n",
      "x shape: torch.Size([11264, 30])\n",
      "reshaped_input shape: torch.Size([5120, 30])\n",
      "x shape: torch.Size([5120, 30])\n",
      "encoded_titles shape: torch.Size([5120, 768])\n",
      "feature_dim: 768\n",
      "click_title_presents shape: torch.Size([128, 40, 768])\n",
      "x shape: torch.Size([11264, 30])\n",
      "reshaped_input shape: torch.Size([5120, 30])\n",
      "x shape: torch.Size([5120, 30])\n",
      "encoded_titles shape: torch.Size([5120, 768])\n",
      "feature_dim: 768\n",
      "click_title_presents shape: torch.Size([128, 40, 768])\n",
      "x shape: torch.Size([11264, 30])\n",
      "reshaped_input shape: torch.Size([5120, 30])\n",
      "x shape: torch.Size([5120, 30])\n",
      "encoded_titles shape: torch.Size([5120, 768])\n",
      "feature_dim: 768\n",
      "click_title_presents shape: torch.Size([128, 40, 768])\n",
      "x shape: torch.Size([11264, 30])\n",
      "reshaped_input shape: torch.Size([5120, 30])\n",
      "x shape: torch.Size([5120, 30])\n",
      "encoded_titles shape: torch.Size([5120, 768])\n",
      "feature_dim: 768\n",
      "click_title_presents shape: torch.Size([128, 40, 768])\n",
      "x shape: torch.Size([11264, 30])\n",
      "reshaped_input shape: torch.Size([5120, 30])\n",
      "x shape: torch.Size([5120, 30])\n",
      "encoded_titles shape: torch.Size([5120, 768])\n",
      "feature_dim: 768\n",
      "click_title_presents shape: torch.Size([128, 40, 768])\n",
      "x shape: torch.Size([11264, 30])\n",
      "reshaped_input shape: torch.Size([5120, 30])\n",
      "x shape: torch.Size([5120, 30])\n",
      "encoded_titles shape: torch.Size([5120, 768])\n",
      "feature_dim: 768\n",
      "click_title_presents shape: torch.Size([128, 40, 768])\n",
      "x shape: torch.Size([11264, 30])\n",
      "reshaped_input shape: torch.Size([5120, 30])\n",
      "x shape: torch.Size([5120, 30])\n",
      "encoded_titles shape: torch.Size([5120, 768])\n",
      "feature_dim: 768\n",
      "click_title_presents shape: torch.Size([128, 40, 768])\n",
      "x shape: torch.Size([11264, 30])\n",
      "reshaped_input shape: torch.Size([5120, 30])\n",
      "x shape: torch.Size([5120, 30])\n",
      "encoded_titles shape: torch.Size([5120, 768])\n",
      "feature_dim: 768\n",
      "click_title_presents shape: torch.Size([128, 40, 768])\n",
      "x shape: torch.Size([11264, 30])\n",
      "reshaped_input shape: torch.Size([5120, 30])\n",
      "x shape: torch.Size([5120, 30])\n",
      "encoded_titles shape: torch.Size([5120, 768])\n",
      "feature_dim: 768\n",
      "click_title_presents shape: torch.Size([128, 40, 768])\n",
      "x shape: torch.Size([11264, 30])\n",
      "reshaped_input shape: torch.Size([5120, 30])\n",
      "x shape: torch.Size([5120, 30])\n",
      "encoded_titles shape: torch.Size([5120, 768])\n",
      "feature_dim: 768\n",
      "click_title_presents shape: torch.Size([128, 40, 768])\n",
      "x shape: torch.Size([11264, 30])\n",
      "reshaped_input shape: torch.Size([5040, 30])\n",
      "x shape: torch.Size([5040, 30])\n",
      "encoded_titles shape: torch.Size([5040, 768])\n",
      "feature_dim: 768\n",
      "click_title_presents shape: torch.Size([126, 40, 768])\n",
      "x shape: torch.Size([11088, 30])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:   7%|▋         | 2/30 [15:31<3:35:41, 462.19s/it, train_loss=1.5804, val_loss=2.3193]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reshaped_input shape: torch.Size([2560, 30])\n",
      "x shape: torch.Size([2560, 30])\n",
      "encoded_titles shape: torch.Size([2560, 768])\n",
      "feature_dim: 768\n",
      "click_title_presents shape: torch.Size([64, 40, 768])\n",
      "x shape: torch.Size([5632, 30])\n",
      "reshaped_input shape: torch.Size([2560, 30])\n",
      "x shape: torch.Size([2560, 30])\n",
      "encoded_titles shape: torch.Size([2560, 768])\n",
      "feature_dim: 768\n",
      "click_title_presents shape: torch.Size([64, 40, 768])\n",
      "x shape: torch.Size([5632, 30])\n",
      "reshaped_input shape: torch.Size([2560, 30])\n",
      "x shape: torch.Size([2560, 30])\n",
      "encoded_titles shape: torch.Size([2560, 768])\n",
      "feature_dim: 768\n",
      "click_title_presents shape: torch.Size([64, 40, 768])\n",
      "x shape: torch.Size([5632, 30])\n",
      "reshaped_input shape: torch.Size([2560, 30])\n",
      "x shape: torch.Size([2560, 30])\n",
      "encoded_titles shape: torch.Size([2560, 768])\n",
      "feature_dim: 768\n",
      "click_title_presents shape: torch.Size([64, 40, 768])\n",
      "x shape: torch.Size([5632, 30])\n",
      "reshaped_input shape: torch.Size([2560, 30])\n",
      "x shape: torch.Size([2560, 30])\n",
      "encoded_titles shape: torch.Size([2560, 768])\n",
      "feature_dim: 768\n",
      "click_title_presents shape: torch.Size([64, 40, 768])\n",
      "x shape: torch.Size([5632, 30])\n",
      "reshaped_input shape: torch.Size([2560, 30])\n",
      "x shape: torch.Size([2560, 30])\n",
      "encoded_titles shape: torch.Size([2560, 768])\n",
      "feature_dim: 768\n",
      "click_title_presents shape: torch.Size([64, 40, 768])\n",
      "x shape: torch.Size([5632, 30])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:   7%|▋         | 2/30 [16:32<3:51:36, 496.32s/it, train_loss=1.5804, val_loss=2.3193]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[33], line 31\u001b[0m\n\u001b[1;32m     29\u001b[0m outputs \u001b[38;5;241m=\u001b[39m model(\u001b[38;5;241m*\u001b[39minputs)\n\u001b[1;32m     30\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(outputs, targets)\n\u001b[0;32m---> 31\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     32\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     34\u001b[0m \u001b[38;5;66;03m# Update running statistics\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/GitHub/Deeplearning-RecSys-Challenge-2024/.venv/lib/python3.12/site-packages/torch/_tensor.py:581\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    571\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    572\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    573\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    574\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    579\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    580\u001b[0m     )\n\u001b[0;32m--> 581\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    582\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    583\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/GitHub/Deeplearning-RecSys-Challenge-2024/.venv/lib/python3.12/site-packages/torch/autograd/__init__.py:347\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    342\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    344\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    345\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    346\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 347\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    348\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    349\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    350\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    351\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    352\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    353\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    354\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    355\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/GitHub/Deeplearning-RecSys-Challenge-2024/.venv/lib/python3.12/site-packages/torch/autograd/graph.py:825\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    823\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    824\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 825\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    826\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    827\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    828\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    829\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "NUM_EPOCHS = 30\n",
    "\n",
    "# Define loss function and optimizer\n",
    "criterion = model.get_loss().to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=hparams_nrms.__dict__['learning_rate'], weight_decay=hparams_nrms.__dict__['weight_decay'])\n",
    "\n",
    "# Training parameters\n",
    "train_losses, val_losses = [], []\n",
    "\n",
    "# Epoch progress bar\n",
    "epoch_pbar = tqdm(range(1, NUM_EPOCHS + 1), desc=\"Training Progress\", dynamic_ncols=True)\n",
    "for epoch in epoch_pbar:\n",
    "    # Training phase\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    train_batch_count = 0\n",
    "\n",
    "    for batch_idx, (inputs, targets, impression_ids) in enumerate(train_dataloader_temp):\n",
    "        # Prepare data\n",
    "        inputs = [inp.to(device) for inp in inputs]\n",
    "        targets = targets.to(device)\n",
    "\n",
    "        # Get positive labels\n",
    "        positive_indices = (targets == 1).nonzero(as_tuple=False)\n",
    "        targets = positive_indices[:, 1].long()\n",
    "\n",
    "        # Forward and backward passes\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(*inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Update running statistics\n",
    "        running_loss += loss.item()\n",
    "        train_batch_count += 1\n",
    "\n",
    "    # Compute average training loss\n",
    "    avg_train_loss = running_loss / train_batch_count if train_batch_count > 0 else float('inf')\n",
    "    train_losses.append(avg_train_loss)\n",
    "\n",
    "    # Validation phase\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    val_batch_count = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, targets, impression_ids in val_dataloader_temp:\n",
    "            inputs = [inp.to(device) for inp in inputs]\n",
    "            targets = targets.to(device)\n",
    "            positive_indices = (targets == 1).nonzero(as_tuple=False)\n",
    "            targets = positive_indices[:, 1].long()\n",
    "            outputs = model(*inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "\n",
    "            val_loss += loss.item()\n",
    "            val_batch_count += 1\n",
    "\n",
    "    avg_val_loss = val_loss / val_batch_count if val_batch_count > 0 else float('inf')\n",
    "    val_losses.append(avg_val_loss)\n",
    "\n",
    "    # Update tensorboard\n",
    "    writer.add_scalar('Loss/Train', avg_train_loss, epoch)\n",
    "    writer.add_scalar('Loss/Validation', avg_val_loss, epoch)\n",
    "\n",
    "    # Update epoch progress bar with metrics\n",
    "    epoch_pbar.set_postfix({\n",
    "        'train_loss': f'{avg_train_loss:.4f}',\n",
    "        'val_loss': f'{avg_val_loss:.4f}',\n",
    "    })\n",
    "\n",
    "    # Save checkpoint\n",
    "    if epoch % 10 == 0:\n",
    "        model_checkpoint(model, avg_val_loss)\n",
    "\n",
    "    # Check early stopping condition\n",
    "    # early_stopping(avg_val_loss)\n",
    "    # if early_stopping.early_stop:\n",
    "    #     print(\"Early stopping triggered. Stopping training.\")\n",
    "    #     break  # Exit the training loop\n",
    "\n",
    "writer.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Summary:\n",
      "Initial Training Loss: 1.9602\n",
      "Final Training Loss: 1.5804\n",
      "Best Training Loss: 1.5804\n",
      "\n",
      "Initial Validation Loss: 2.3188\n",
      "Final Validation Loss: 2.3193\n",
      "Best Validation Loss: 2.3188\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Plot the losses\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(range(1, len(train_losses) + 1), train_losses, label='Training Loss')\n",
    "plt.plot(range(1, len(val_losses) + 1), val_losses, label='Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training and Validation Losses')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.savefig(\"nrms_training_loss.png\")\n",
    "plt.show()\n",
    "\n",
    "# Print final statistics\n",
    "print(\"\\nTraining Summary:\")\n",
    "print(f\"Initial Training Loss: {train_losses[0]:.4f}\")\n",
    "print(f\"Final Training Loss: {train_losses[-1]:.4f}\")\n",
    "print(f\"Best Training Loss: {min(train_losses):.4f}\")\n",
    "print(f\"\\nInitial Validation Loss: {val_losses[0]:.4f}\")\n",
    "print(f\"Final Validation Loss: {val_losses[-1]:.4f}\")\n",
    "print(f\"Best Validation Loss: {min(val_losses):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating DataLoader...\n",
      "Total samples in DataLoader: 2430\n",
      "\n",
      "First batch shapes:\n",
      "  - his_input_title: torch.Size([128, 40, 30])\n",
      "  - pred_input_title: torch.Size([128, 88, 30])\n",
      "  - targets: torch.Size([128, 88])\n",
      "reshaped_input shape: torch.Size([5120, 30])\n",
      "x shape: torch.Size([5120, 30])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/gustavsiphone/Documents/GitHub/Deeplearning-RecSys-Challenge-2024/models_pytorch/nrms.py:151: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  def _build_userencoder(self, titleencoder):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encoded_titles shape: torch.Size([5120, 768])\n",
      "feature_dim: 768\n",
      "click_title_presents shape: torch.Size([128, 40, 768])\n",
      "x shape: torch.Size([11264, 30])\n",
      "reshaped_input shape: torch.Size([5120, 30])\n",
      "x shape: torch.Size([5120, 30])\n",
      "encoded_titles shape: torch.Size([5120, 768])\n",
      "feature_dim: 768\n",
      "click_title_presents shape: torch.Size([128, 40, 768])\n",
      "x shape: torch.Size([11264, 30])\n",
      "reshaped_input shape: torch.Size([5120, 30])\n",
      "x shape: torch.Size([5120, 30])\n",
      "encoded_titles shape: torch.Size([5120, 768])\n",
      "feature_dim: 768\n",
      "click_title_presents shape: torch.Size([128, 40, 768])\n",
      "x shape: torch.Size([11264, 30])\n",
      "reshaped_input shape: torch.Size([5120, 30])\n",
      "x shape: torch.Size([5120, 30])\n",
      "encoded_titles shape: torch.Size([5120, 768])\n",
      "feature_dim: 768\n",
      "click_title_presents shape: torch.Size([128, 40, 768])\n",
      "x shape: torch.Size([11264, 30])\n",
      "reshaped_input shape: torch.Size([5120, 30])\n",
      "x shape: torch.Size([5120, 30])\n",
      "encoded_titles shape: torch.Size([5120, 768])\n",
      "feature_dim: 768\n",
      "click_title_presents shape: torch.Size([128, 40, 768])\n",
      "x shape: torch.Size([11264, 30])\n",
      "reshaped_input shape: torch.Size([5120, 30])\n",
      "x shape: torch.Size([5120, 30])\n",
      "encoded_titles shape: torch.Size([5120, 768])\n",
      "feature_dim: 768\n",
      "click_title_presents shape: torch.Size([128, 40, 768])\n",
      "x shape: torch.Size([11264, 30])\n",
      "reshaped_input shape: torch.Size([5120, 30])\n",
      "x shape: torch.Size([5120, 30])\n",
      "encoded_titles shape: torch.Size([5120, 768])\n",
      "feature_dim: 768\n",
      "click_title_presents shape: torch.Size([128, 40, 768])\n",
      "x shape: torch.Size([11264, 30])\n",
      "reshaped_input shape: torch.Size([5120, 30])\n",
      "x shape: torch.Size([5120, 30])\n",
      "encoded_titles shape: torch.Size([5120, 768])\n",
      "feature_dim: 768\n",
      "click_title_presents shape: torch.Size([128, 40, 768])\n",
      "x shape: torch.Size([11264, 30])\n",
      "reshaped_input shape: torch.Size([5120, 30])\n",
      "x shape: torch.Size([5120, 30])\n",
      "encoded_titles shape: torch.Size([5120, 768])\n",
      "feature_dim: 768\n",
      "click_title_presents shape: torch.Size([128, 40, 768])\n",
      "x shape: torch.Size([11264, 30])\n",
      "reshaped_input shape: torch.Size([5120, 30])\n",
      "x shape: torch.Size([5120, 30])\n",
      "encoded_titles shape: torch.Size([5120, 768])\n",
      "feature_dim: 768\n",
      "click_title_presents shape: torch.Size([128, 40, 768])\n",
      "x shape: torch.Size([11264, 30])\n",
      "reshaped_input shape: torch.Size([5120, 30])\n",
      "x shape: torch.Size([5120, 30])\n",
      "encoded_titles shape: torch.Size([5120, 768])\n",
      "feature_dim: 768\n",
      "click_title_presents shape: torch.Size([128, 40, 768])\n",
      "x shape: torch.Size([11264, 30])\n",
      "reshaped_input shape: torch.Size([5120, 30])\n",
      "x shape: torch.Size([5120, 30])\n",
      "encoded_titles shape: torch.Size([5120, 768])\n",
      "feature_dim: 768\n",
      "click_title_presents shape: torch.Size([128, 40, 768])\n",
      "x shape: torch.Size([11264, 30])\n",
      "reshaped_input shape: torch.Size([5120, 30])\n",
      "x shape: torch.Size([5120, 30])\n",
      "encoded_titles shape: torch.Size([5120, 768])\n",
      "feature_dim: 768\n",
      "click_title_presents shape: torch.Size([128, 40, 768])\n",
      "x shape: torch.Size([11264, 30])\n",
      "reshaped_input shape: torch.Size([5120, 30])\n",
      "x shape: torch.Size([5120, 30])\n",
      "encoded_titles shape: torch.Size([5120, 768])\n",
      "feature_dim: 768\n",
      "click_title_presents shape: torch.Size([128, 40, 768])\n",
      "x shape: torch.Size([11264, 30])\n",
      "reshaped_input shape: torch.Size([5120, 30])\n",
      "x shape: torch.Size([5120, 30])\n",
      "encoded_titles shape: torch.Size([5120, 768])\n",
      "feature_dim: 768\n",
      "click_title_presents shape: torch.Size([128, 40, 768])\n",
      "x shape: torch.Size([11264, 30])\n",
      "reshaped_input shape: torch.Size([5120, 30])\n",
      "x shape: torch.Size([5120, 30])\n",
      "encoded_titles shape: torch.Size([5120, 768])\n",
      "feature_dim: 768\n",
      "click_title_presents shape: torch.Size([128, 40, 768])\n",
      "x shape: torch.Size([11264, 30])\n",
      "reshaped_input shape: torch.Size([5120, 30])\n",
      "x shape: torch.Size([5120, 30])\n",
      "encoded_titles shape: torch.Size([5120, 768])\n",
      "feature_dim: 768\n",
      "click_title_presents shape: torch.Size([128, 40, 768])\n",
      "x shape: torch.Size([11264, 30])\n",
      "reshaped_input shape: torch.Size([5120, 30])\n",
      "x shape: torch.Size([5120, 30])\n",
      "encoded_titles shape: torch.Size([5120, 768])\n",
      "feature_dim: 768\n",
      "click_title_presents shape: torch.Size([128, 40, 768])\n",
      "x shape: torch.Size([11264, 30])\n",
      "reshaped_input shape: torch.Size([5040, 30])\n",
      "x shape: torch.Size([5040, 30])\n",
      "encoded_titles shape: torch.Size([5040, 768])\n",
      "feature_dim: 768\n",
      "click_title_presents shape: torch.Size([126, 40, 768])\n",
      "x shape: torch.Size([11088, 30])\n",
      "\n",
      "Evaluation completed.\n",
      "Total predictions generated: 2430\n",
      "First few prediction lengths: [5, 6, 7, 17, 5, 11, 7, 14, 5, 9, 9, 6, 16, 9, 5]\n",
      "\n",
      "Validation against DataFrame:\n",
      "\n",
      "Metrics: {'auc': 0.5292444462694742, 'mrr': 0.31778919078418216, 'ndcg@5': 0.3503207465375865, 'ndcg@10': 0.43819916022141203}\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "def evaluate_model(model, dataloader, device):\n",
    "    \"\"\"Evaluate the model and return predictions and labels for metric calculation.\"\"\"\n",
    "    model.eval()\n",
    "    all_predictions = []\n",
    "    all_labels = []\n",
    "\n",
    "    print(\"\\nEvaluating DataLoader...\")\n",
    "    total_samples = len(dataloader.dataset)\n",
    "    print(f\"Total samples in DataLoader: {total_samples}\")\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (inputs, targets, impression_ids) in enumerate(dataloader):\n",
    "            his_input_title, pred_input_title = inputs\n",
    "\n",
    "            if batch_idx == 0:  # Debug first batch shapes\n",
    "                print(\"\\nFirst batch shapes:\")\n",
    "                print(f\"  - his_input_title: {his_input_title.shape}\")\n",
    "                print(f\"  - pred_input_title: {pred_input_title.shape}\")\n",
    "                print(f\"  - targets: {targets.shape}\")\n",
    "\n",
    "            # Move data to device\n",
    "            his_input_title = his_input_title.to(device)\n",
    "            pred_input_title = pred_input_title.to(device)\n",
    "            targets = targets.to(device)\n",
    "\n",
    "            # Get predictions\n",
    "            predictions = model.predict(his_input_title, pred_input_title)\n",
    "            predictions = predictions.cpu().numpy()\n",
    "            targets = targets.cpu().numpy()\n",
    "\n",
    "            # Process each sample in the batch\n",
    "            batch_size = predictions.shape[0]\n",
    "            for sample_idx in range(batch_size):\n",
    "                pred = predictions[sample_idx]\n",
    "                label = targets[sample_idx]\n",
    "\n",
    "                # Create valid_mask where label is not equal to the padding value (-1)\n",
    "                valid_mask = (label != -1)\n",
    "                sample_preds = pred[valid_mask]\n",
    "                sample_labels = label[valid_mask]\n",
    "\n",
    "                if len(sample_labels) == 0:\n",
    "                    continue  # Skip empty samples\n",
    "\n",
    "                # Ensure that there is at least one positive and one negative label\n",
    "                if len(np.unique(sample_labels)) < 2:\n",
    "                    continue  # Skip samples with only one class\n",
    "\n",
    "                all_predictions.append(sample_preds.tolist())\n",
    "                all_labels.append(sample_labels.tolist())\n",
    "\n",
    "    print(\"\\nEvaluation completed.\")\n",
    "    print(f\"Total predictions generated: {len(all_predictions)}\")\n",
    "    print(f\"First few prediction lengths: {[len(x) for x in all_predictions[:15]]}\")\n",
    "    return all_labels, all_predictions\n",
    "\n",
    "\n",
    "# Evaluate the model\n",
    "labels_list, scores_list = evaluate_model(model, val_dataloader_temp, device)\n",
    "\n",
    "# Validate predictions against the DataFrame\n",
    "print(\"\\nValidation against DataFrame:\")\n",
    "if len(scores_list) != len(df_validation):\n",
    "    print(\"WARNING: Length mismatch!\")\n",
    "    print(f\"  - Number of predictions: {len(scores_list)}\")\n",
    "    print(f\"  - Number of rows in DataFrame: {len(df_validation)}\")\n",
    "\n",
    "# Compute metrics\n",
    "metrics = MetricEvaluator(\n",
    "    labels=labels_list,\n",
    "    predictions=scores_list,\n",
    "    metric_functions=[\n",
    "        AucScore(),\n",
    "        MrrScore(),\n",
    "        NdcgScore(k=5),\n",
    "        NdcgScore(k=10)\n",
    "    ],\n",
    ")\n",
    "results = metrics.evaluate()\n",
    "print(\"\\nMetrics:\", results.evaluations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"FRACTION: {FRACTION}, HISTORY_SIZE: {HISTORY_SIZE}\")\n",
    "\n",
    "# Filter out special Python attributes and print parameters\n",
    "params = {k: v for k, v in hparams_nrms.__dict__.items() if not k.startswith('__')}\n",
    "print(\"Hyperparameters:\")\n",
    "for key, value in params.items():\n",
    "    print(f\"{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_length_of_labels(df: pl.DataFrame, impression_id: int) -> int:\n",
    "    # Filter for matching impression_id\n",
    "    filtered = df.filter(pl.col('impression_id') == impression_id)\n",
    "    \n",
    "    if filtered.height == 0:\n",
    "        raise ValueError(f\"No row found for impression_id {impression_id}\")\n",
    "    \n",
    "    # Get labels from first row\n",
    "    labels = filtered.select('labels').row(0)[0]\n",
    "    \n",
    "    return len(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get_length_of_labels(df_validation, 349992000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get first 5 labels\n",
    "first_5_labels = df_validation.select('labels').head(5)\n",
    "\n",
    "# Print each label with index\n",
    "for i, row in enumerate(first_5_labels.iter_rows()):\n",
    "    print(f\"Label {i}: {row[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "# Initialize lists\n",
    "all_predictions = []\n",
    "all_labels = []\n",
    "skipped_samples = 0\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for (his_input_title, pred_input_title), targets, impression_ids in val_dataloader_temp:\n",
    "        # Move to device\n",
    "        his_input_title = his_input_title.to(device)\n",
    "        pred_input_title = pred_input_title.to(device)\n",
    "        \n",
    "        # Get predictions\n",
    "        predictions = model.predict(his_input_title, pred_input_title)\n",
    "        \n",
    "        \n",
    "        # Convert to probabilities if needed\n",
    "        if not torch.is_floating_point(predictions):\n",
    "            predictions = torch.sigmoid(predictions)\n",
    "        \n",
    "        # Convert to lists while preserving structure\n",
    "        batch_preds = predictions.cpu().numpy().tolist()\n",
    "        batch_labels = targets.cpu().numpy().tolist()\n",
    "        impression_ids = impression_ids.cpu().numpy().tolist()\n",
    "        \n",
    "        batch_preds_without_padding = []\n",
    "        batch_labels_without_padding = []\n",
    "        \n",
    "        for pred_sample, label_sample, impression_id_sample in zip(batch_preds, batch_labels, impression_ids):\n",
    "            # Remove padding\n",
    "            actual_length = get_length_of_labels(df_validation, impression_id_sample)\n",
    "            pred_sample = pred_sample[:actual_length]\n",
    "            \n",
    "            # Check if sample has both classes before adding\n",
    "            if 1 in label_sample[:actual_length] and 0 in label_sample[:actual_length]:\n",
    "                batch_preds_without_padding.append(pred_sample)\n",
    "                batch_labels_without_padding.append(label_sample[:actual_length])\n",
    "            else:\n",
    "                skipped_samples += 1\n",
    "        \n",
    "        # Add batch predictions and labels\n",
    "        all_predictions.extend(batch_preds_without_padding)\n",
    "        all_labels.extend(batch_labels_without_padding)\n",
    "print(f\"Skipped {skipped_samples} samples with only one class\")\n",
    "print(f\"Remaining valid samples: {len(all_predictions)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Debug: Print label distribution for first 5 samples\n",
    "# for i, (preds, labels) in enumerate(zip(all_predictions[:5], all_labels[:5])):\n",
    "#     print(f\"\\nSample {i}:\")\n",
    "#     print(f\"Labels length:      {len(labels)}\")\n",
    "#     print(f\"Predictions length: {len(preds)}\")\n",
    "#     print(f\"Num positives: {sum(labels)}\")\n",
    "#     print(f\"Num negatives: {len(labels) - sum(labels)}\")\n",
    "#     print(f\"Label distribution: {labels}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(type(all_predictions))\n",
    "# print(type(all_labels))\n",
    "\n",
    "# print(f\"Number of predictions: {len(all_predictions)}\")\n",
    "# print(f\"example prediction: {all_predictions[0:2]}\")\n",
    "# print(f\"Number of labels: {len(all_labels)}\")\n",
    "# print(f\"example label: {all_labels[0:2]}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize counters\n",
    "correct_predictions = 0\n",
    "total_samples = len(all_predictions)\n",
    "\n",
    "# Iterate over samples\n",
    "for idx, _ in enumerate(all_predictions):\n",
    "    # print(f\"Sample {idx}: Prediction: {all_predictions[idx]}\")\n",
    "    # print(f\"Sample {idx}: Label:      {all_labels[idx]}\")\n",
    "    \n",
    "    # Extract index of maximum value in predictions and labels\n",
    "    pred_max_index = np.argmax(all_predictions[idx])\n",
    "    label_max_index = np.argmax(all_labels[idx])\n",
    "    \n",
    "    # Compare indices to determine if the prediction was correct\n",
    "    if pred_max_index == label_max_index:\n",
    "        # print(f\"Sample {idx}: Prediction was correct.\")\n",
    "        correct_predictions += 1\n",
    "  #  else:\n",
    "        # print(f\"Sample {idx}: Prediction was wrong.\")\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = correct_predictions / total_samples\n",
    "print(f\"Accuracy: {accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from evaluation import AucScore\n",
    "\n",
    "# auc_score = AucScore()\n",
    "\n",
    "# auc_score.calculate(all_predictions, all_labels)\n",
    "# print(f\"AUC: {auc_score.score}\")\n",
    "# # Calculate AUC per sample\n",
    "aucs = []\n",
    "for preds, labels in zip(all_predictions, all_labels):\n",
    "    try:\n",
    "        # Only calculate if we have both positive and negative samples\n",
    "        if sum(labels) > 0 and sum(labels) < len(labels):\n",
    "            auc = roc_auc_score(labels, preds)\n",
    "            aucs.append(auc)\n",
    "    except ValueError:\n",
    "        raise ValueError(\"Only one class present in labels. Cannot calculate AUC.\")\n",
    "\n",
    "print(f\"\\nMean AUC: {np.mean(aucs):.4f}\")\n",
    "print(f\"Number of valid AUC calculations: {len(aucs)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Output Prediction File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "https://github.com/recommenders-team/recommenders/blob/main/examples/00_quick_start/nrms_MIND.ipynb"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
