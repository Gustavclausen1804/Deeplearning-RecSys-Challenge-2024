{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Global settings and imports"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\gusta\\Documents\\Deeplearning-RecSys-Challenge-2024\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python version: 3.12.1\n",
      "PyTorch version: 2.5.1+cu121\n",
      "CUDA available: True\n",
      "Current GPU device: NVIDIA GeForce RTX 3070\n",
      "Number of GPUs available: 1\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import sys\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import polars as pl\n",
    "import matplotlib.pyplot as plt\n",
    "#import optuna\n",
    "from tqdm import tqdm\n",
    "import torch.optim as optim\n",
    "\n",
    "from utils._constants import (\n",
    "    DEFAULT_HISTORY_ARTICLE_ID_COL,\n",
    "    DEFAULT_CLICKED_ARTICLES_COL,\n",
    "    DEFAULT_INVIEW_ARTICLES_COL,\n",
    "    DEFAULT_IMPRESSION_ID_COL,\n",
    "    DEFAULT_IMPRESSION_TIMESTAMP_COL,\n",
    "    DEFAULT_SUBTITLE_COL,\n",
    "    DEFAULT_LABELS_COL,\n",
    "    DEFAULT_TITLE_COL,\n",
    "    DEFAULT_USER_COL,\n",
    "    DEFAULT_ARTICLE_PUBLISHED_TIMESTAMP_COL,\n",
    "    DEFAULT_CATEGORY_STR_COL,\n",
    "    DEFAULT_TOPICS_COL,\n",
    "    DEFAULT_CATEGORY_STR_COL,\n",
    "    DEFAULT_TOPICS_COL,\n",
    "    DEFAULT_TOTAL_PAGEVIEWS_COL,\n",
    "    DEFAULT_TOTAL_READ_TIME_COL,\n",
    "    DEFAULT_SENTIMENT_SCORE_COL,\n",
    ")\n",
    "\n",
    "from utils._behaviors import (\n",
    "    create_binary_labels_column,\n",
    "    sampling_strategy_wu2019,\n",
    "    add_known_user_column,\n",
    "    add_prediction_scores,\n",
    "    truncate_history,\n",
    ")\n",
    "from evaluation import MetricEvaluator, AucScore, NdcgScore, MrrScore\n",
    "from utils._articles import convert_text2encoding_with_transformers\n",
    "from utils._polars import concat_str_columns, slice_join_dataframes\n",
    "from utils._articles import create_article_id_to_value_mapping\n",
    "from utils._nlp import get_transformers_word_embeddings, generate_embeddings_with_transformers\n",
    "from utils._python import write_submission_file, rank_predictions_by_score\n",
    "from models_pytorch.model_config import hparams_nrms\n",
    "\n",
    "from models_pytorch.nrms import NRMSModel\n",
    "from models_pytorch.NRMSDocVecModel import NRMSDocVecModel\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from models_pytorch.dataloader import NRMSDataSet\n",
    "\n",
    "# Check Python version\n",
    "print(f\"Python version: {sys.version.split()[0]}\")\n",
    "\n",
    "# Check PyTorch version\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "\n",
    "# Check GPU availability\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"Current GPU device: {torch.cuda.get_device_name()}\")\n",
    "    print(f\"Number of GPUs available: {torch.cuda.device_count()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Looking at behaviours and history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download and load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ebnerd_small\n"
     ]
    }
   ],
   "source": [
    "PATH = Path(\"./ebnerd_small\")  # Base path for your data directory\n",
    "print(PATH)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (2, 7)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>user_id</th><th>article_id_fixed</th><th>article_ids_inview</th><th>article_ids_clicked</th><th>impression_id</th><th>impression_time</th><th>labels</th></tr><tr><td>u32</td><td>list[i32]</td><td>list[i64]</td><td>list[i64]</td><td>u32</td><td>datetime[μs]</td><td>list[i8]</td></tr></thead><tbody><tr><td>2498520</td><td>[9759241, 9759142, … 9770741]</td><td>[9772099, 9772443, … 9772750]</td><td>[9772443]</td><td>337281027</td><td>2023-05-19 20:34:48</td><td>[0, 1, … 0]</td></tr><tr><td>603561</td><td>[9765172, 9765551, … 9769553]</td><td>[9778827, 9778668, … 9778769]</td><td>[9778666]</td><td>279245246</td><td>2023-05-24 08:54:32</td><td>[0, 0, … 0]</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (2, 7)\n",
       "┌─────────┬──────────────┬──────────────┬──────────────┬──────────────┬──────────────┬─────────────┐\n",
       "│ user_id ┆ article_id_f ┆ article_ids_ ┆ article_ids_ ┆ impression_i ┆ impression_t ┆ labels      │\n",
       "│ ---     ┆ ixed         ┆ inview       ┆ clicked      ┆ d            ┆ ime          ┆ ---         │\n",
       "│ u32     ┆ ---          ┆ ---          ┆ ---          ┆ ---          ┆ ---          ┆ list[i8]    │\n",
       "│         ┆ list[i32]    ┆ list[i64]    ┆ list[i64]    ┆ u32          ┆ datetime[μs] ┆             │\n",
       "╞═════════╪══════════════╪══════════════╪══════════════╪══════════════╪══════════════╪═════════════╡\n",
       "│ 2498520 ┆ [9759241,    ┆ [9772099,    ┆ [9772443]    ┆ 337281027    ┆ 2023-05-19   ┆ [0, 1, … 0] │\n",
       "│         ┆ 9759142, …   ┆ 9772443, …   ┆              ┆              ┆ 20:34:48     ┆             │\n",
       "│         ┆ 9770741]     ┆ 9772750]     ┆              ┆              ┆              ┆             │\n",
       "│ 603561  ┆ [9765172,    ┆ [9778827,    ┆ [9778666]    ┆ 279245246    ┆ 2023-05-24   ┆ [0, 0, … 0] │\n",
       "│         ┆ 9765551, …   ┆ 9778668, …   ┆              ┆              ┆ 08:54:32     ┆             │\n",
       "│         ┆ 9769553]     ┆ 9778769]     ┆              ┆              ┆              ┆             │\n",
       "└─────────┴──────────────┴──────────────┴──────────────┴──────────────┴──────────────┴─────────────┘"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def ebnerd_from_path(path: Path, history_size: int = 30) -> pl.DataFrame:\n",
    "    \"\"\"\n",
    "    Load ebnerd - function\n",
    "    \"\"\"\n",
    "    df_history = (\n",
    "        pl.scan_parquet(path.joinpath(\"history.parquet\"))\n",
    "        .select(DEFAULT_USER_COL, DEFAULT_HISTORY_ARTICLE_ID_COL)\n",
    "        .pipe(\n",
    "            truncate_history,\n",
    "            column=DEFAULT_HISTORY_ARTICLE_ID_COL,\n",
    "            history_size=history_size,\n",
    "            padding_value=0,\n",
    "            enable_warning=False,\n",
    "        )\n",
    "    )\n",
    "    df_behaviors = (\n",
    "        pl.scan_parquet(path.joinpath(\"behaviors.parquet\"))\n",
    "        .collect()\n",
    "        .pipe(\n",
    "            slice_join_dataframes,\n",
    "            df2=df_history.collect(),\n",
    "            on=DEFAULT_USER_COL,\n",
    "            how=\"left\",\n",
    "        )\n",
    "    )\n",
    "    return df_behaviors\n",
    "\n",
    "COLUMNS = [\n",
    "    DEFAULT_USER_COL,\n",
    "    DEFAULT_HISTORY_ARTICLE_ID_COL,\n",
    "    DEFAULT_INVIEW_ARTICLES_COL,\n",
    "    DEFAULT_CLICKED_ARTICLES_COL,\n",
    "    DEFAULT_IMPRESSION_ID_COL,\n",
    "    DEFAULT_IMPRESSION_TIMESTAMP_COL,\n",
    "]\n",
    "HISTORY_SIZE = 50 # TODO: History size. \n",
    "FRACTION = 0.2\n",
    "\n",
    "df_train = (\n",
    "    ebnerd_from_path(PATH.joinpath(\"train\"), history_size=HISTORY_SIZE)\n",
    "    .select(COLUMNS)\n",
    "    .pipe(\n",
    "        sampling_strategy_wu2019,\n",
    "        npratio=4,\n",
    "        shuffle=True,\n",
    "        with_replacement=True,\n",
    "        seed=123,\n",
    "    )\n",
    "    .pipe(create_binary_labels_column)\n",
    "    .sample(fraction=FRACTION)\n",
    ")\n",
    "# =>\n",
    "df_validation = (\n",
    "    ebnerd_from_path(PATH.joinpath(\"validation\"), history_size=HISTORY_SIZE)\n",
    "    .select(COLUMNS)\n",
    "    .pipe(create_binary_labels_column)\n",
    "    .sample(fraction=FRACTION)\n",
    ")\n",
    "df_train.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = df_train.with_columns([\n",
    "    (\n",
    "        (pl.col(DEFAULT_IMPRESSION_TIMESTAMP_COL)\n",
    "        .cast(pl.Datetime)\n",
    "        .dt.epoch(time_unit='s') / 3600)\n",
    "        .cast(pl.Int64)\n",
    "    ).alias(DEFAULT_IMPRESSION_TIMESTAMP_COL)\n",
    "])\n",
    "\n",
    "df_validation = df_validation.with_columns([\n",
    "    (\n",
    "        (pl.col(DEFAULT_IMPRESSION_TIMESTAMP_COL)\n",
    "        .cast(pl.Datetime)\n",
    "        .dt.epoch(time_unit='s') / 3600)\n",
    "        .cast(pl.Int64)\n",
    "    ).alias(DEFAULT_IMPRESSION_TIMESTAMP_COL)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (5, 7)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>user_id</th><th>article_id_fixed</th><th>article_ids_inview</th><th>article_ids_clicked</th><th>impression_id</th><th>impression_time</th><th>labels</th></tr><tr><td>u32</td><td>list[i32]</td><td>list[i64]</td><td>list[i64]</td><td>u32</td><td>i64</td><td>list[i8]</td></tr></thead><tbody><tr><td>2498520</td><td>[9759241, 9759142, … 9770741]</td><td>[9772099, 9772443, … 9772750]</td><td>[9772443]</td><td>337281027</td><td>467924</td><td>[0, 1, … 0]</td></tr><tr><td>603561</td><td>[9765172, 9765551, … 9769553]</td><td>[9778827, 9778668, … 9778769]</td><td>[9778666]</td><td>279245246</td><td>468032</td><td>[0, 0, … 0]</td></tr><tr><td>329699</td><td>[9766592, 9759345, … 9742173]</td><td>[9771859, 9771859, … 9773015]</td><td>[9761859]</td><td>142964085</td><td>467934</td><td>[0, 0, … 0]</td></tr><tr><td>1168921</td><td>[9767772, 9767746, … 9663715]</td><td>[9761926, 8169191, … 9771896]</td><td>[9771896]</td><td>263407383</td><td>467908</td><td>[0, 0, … 1]</td></tr><tr><td>1012106</td><td>[9752962, 9749760, … 9754730]</td><td>[9774618, 9120051, … 9774527]</td><td>[9774527]</td><td>316436953</td><td>467966</td><td>[0, 0, … 1]</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (5, 7)\n",
       "┌─────────┬──────────────┬──────────────┬──────────────┬──────────────┬──────────────┬─────────────┐\n",
       "│ user_id ┆ article_id_f ┆ article_ids_ ┆ article_ids_ ┆ impression_i ┆ impression_t ┆ labels      │\n",
       "│ ---     ┆ ixed         ┆ inview       ┆ clicked      ┆ d            ┆ ime          ┆ ---         │\n",
       "│ u32     ┆ ---          ┆ ---          ┆ ---          ┆ ---          ┆ ---          ┆ list[i8]    │\n",
       "│         ┆ list[i32]    ┆ list[i64]    ┆ list[i64]    ┆ u32          ┆ i64          ┆             │\n",
       "╞═════════╪══════════════╪══════════════╪══════════════╪══════════════╪══════════════╪═════════════╡\n",
       "│ 2498520 ┆ [9759241,    ┆ [9772099,    ┆ [9772443]    ┆ 337281027    ┆ 467924       ┆ [0, 1, … 0] │\n",
       "│         ┆ 9759142, …   ┆ 9772443, …   ┆              ┆              ┆              ┆             │\n",
       "│         ┆ 9770741]     ┆ 9772750]     ┆              ┆              ┆              ┆             │\n",
       "│ 603561  ┆ [9765172,    ┆ [9778827,    ┆ [9778666]    ┆ 279245246    ┆ 468032       ┆ [0, 0, … 0] │\n",
       "│         ┆ 9765551, …   ┆ 9778668, …   ┆              ┆              ┆              ┆             │\n",
       "│         ┆ 9769553]     ┆ 9778769]     ┆              ┆              ┆              ┆             │\n",
       "│ 329699  ┆ [9766592,    ┆ [9771859,    ┆ [9761859]    ┆ 142964085    ┆ 467934       ┆ [0, 0, … 0] │\n",
       "│         ┆ 9759345, …   ┆ 9771859, …   ┆              ┆              ┆              ┆             │\n",
       "│         ┆ 9742173]     ┆ 9773015]     ┆              ┆              ┆              ┆             │\n",
       "│ 1168921 ┆ [9767772,    ┆ [9761926,    ┆ [9771896]    ┆ 263407383    ┆ 467908       ┆ [0, 0, … 1] │\n",
       "│         ┆ 9767746, …   ┆ 8169191, …   ┆              ┆              ┆              ┆             │\n",
       "│         ┆ 9663715]     ┆ 9771896]     ┆              ┆              ┆              ┆             │\n",
       "│ 1012106 ┆ [9752962,    ┆ [9774618,    ┆ [9774527]    ┆ 316436953    ┆ 467966       ┆ [0, 0, … 1] │\n",
       "│         ┆ 9749760, …   ┆ 9120051, …   ┆              ┆              ┆              ┆             │\n",
       "│         ┆ 9754730]     ┆ 9774527]     ┆              ┆              ┆              ┆             │\n",
       "└─────────┴──────────────┴──────────────┴──────────────┴──────────────┴──────────────┴─────────────┘"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (5, 7)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>user_id</th><th>article_id_fixed</th><th>article_ids_inview</th><th>article_ids_clicked</th><th>impression_id</th><th>impression_time</th><th>labels</th></tr><tr><td>u32</td><td>list[i32]</td><td>list[i32]</td><td>list[i32]</td><td>u32</td><td>i64</td><td>list[i8]</td></tr></thead><tbody><tr><td>1582341</td><td>[9753518, 9752685, … 9756397]</td><td>[9704868, 9789704, … 9717601]</td><td>[9789710]</td><td>150364157</td><td>468203</td><td>[0, 0, … 0]</td></tr><tr><td>1748668</td><td>[9778842, 9779019, … 9779968]</td><td>[9780498, 9780860, … 9780514]</td><td>[9779779]</td><td>412610146</td><td>468060</td><td>[0, 0, … 0]</td></tr><tr><td>2199962</td><td>[9778902, 9777397, … 9779263]</td><td>[9400164, 9780482, … 9780498]</td><td>[9780428]</td><td>525133049</td><td>468060</td><td>[0, 0, … 0]</td></tr><tr><td>1959613</td><td>[9775419, 9775402, … 9779738]</td><td>[9785718, 9785742, … 9785434]</td><td>[9785732]</td><td>73604021</td><td>468182</td><td>[0, 0, … 0]</td></tr><tr><td>612674</td><td>[9776023, 9665220, … 9778902]</td><td>[9781158, 9781362, … 9781057]</td><td>[9779225]</td><td>121362715</td><td>468063</td><td>[0, 0, … 0]</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (5, 7)\n",
       "┌─────────┬──────────────┬──────────────┬──────────────┬──────────────┬──────────────┬─────────────┐\n",
       "│ user_id ┆ article_id_f ┆ article_ids_ ┆ article_ids_ ┆ impression_i ┆ impression_t ┆ labels      │\n",
       "│ ---     ┆ ixed         ┆ inview       ┆ clicked      ┆ d            ┆ ime          ┆ ---         │\n",
       "│ u32     ┆ ---          ┆ ---          ┆ ---          ┆ ---          ┆ ---          ┆ list[i8]    │\n",
       "│         ┆ list[i32]    ┆ list[i32]    ┆ list[i32]    ┆ u32          ┆ i64          ┆             │\n",
       "╞═════════╪══════════════╪══════════════╪══════════════╪══════════════╪══════════════╪═════════════╡\n",
       "│ 1582341 ┆ [9753518,    ┆ [9704868,    ┆ [9789710]    ┆ 150364157    ┆ 468203       ┆ [0, 0, … 0] │\n",
       "│         ┆ 9752685, …   ┆ 9789704, …   ┆              ┆              ┆              ┆             │\n",
       "│         ┆ 9756397]     ┆ 9717601]     ┆              ┆              ┆              ┆             │\n",
       "│ 1748668 ┆ [9778842,    ┆ [9780498,    ┆ [9779779]    ┆ 412610146    ┆ 468060       ┆ [0, 0, … 0] │\n",
       "│         ┆ 9779019, …   ┆ 9780860, …   ┆              ┆              ┆              ┆             │\n",
       "│         ┆ 9779968]     ┆ 9780514]     ┆              ┆              ┆              ┆             │\n",
       "│ 2199962 ┆ [9778902,    ┆ [9400164,    ┆ [9780428]    ┆ 525133049    ┆ 468060       ┆ [0, 0, … 0] │\n",
       "│         ┆ 9777397, …   ┆ 9780482, …   ┆              ┆              ┆              ┆             │\n",
       "│         ┆ 9779263]     ┆ 9780498]     ┆              ┆              ┆              ┆             │\n",
       "│ 1959613 ┆ [9775419,    ┆ [9785718,    ┆ [9785732]    ┆ 73604021     ┆ 468182       ┆ [0, 0, … 0] │\n",
       "│         ┆ 9775402, …   ┆ 9785742, …   ┆              ┆              ┆              ┆             │\n",
       "│         ┆ 9779738]     ┆ 9785434]     ┆              ┆              ┆              ┆             │\n",
       "│ 612674  ┆ [9776023,    ┆ [9781158,    ┆ [9779225]    ┆ 121362715    ┆ 468063       ┆ [0, 0, … 0] │\n",
       "│         ┆ 9665220, …   ┆ 9781362, …   ┆              ┆              ┆              ┆             │\n",
       "│         ┆ 9778902]     ┆ 9781057]     ┆              ┆              ┆              ┆             │\n",
       "└─────────┴──────────────┴──────────────┴──────────────┴──────────────┴──────────────┴─────────────┘"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_validation.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "vscode": {
     "languageId": "ruby"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average length of article_ids_inview in df_train: 5.0\n",
      "Average length of article_ids_inview in df_validation: 11.97173455414989\n"
     ]
    }
   ],
   "source": [
    "def calculate_average_length(df, column):\n",
    "    total_length = sum(len(row) for row in df[column])\n",
    "    average_length = total_length / len(df)\n",
    "    return average_length\n",
    "\n",
    "# Calculate average length for df_train\n",
    "average_length_inview_train = calculate_average_length(df_train, DEFAULT_INVIEW_ARTICLES_COL)\n",
    "print(f\"Average length of article_ids_inview in df_train: {average_length_inview_train}\")\n",
    "\n",
    "# Calculate average length for df_validation\n",
    "average_length_inview_validation = calculate_average_length(df_validation, DEFAULT_INVIEW_ARTICLES_COL)\n",
    "print(f\"Average length of article_ids_inview in df_validation: {average_length_inview_validation}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "ruby"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Longest inview article length in df_train: 5\n",
      "Longest inview article length in df_validation: 89\n",
      "Longest history length in df_train: 50\n",
      "Longest history length in df_validation: 50\n"
     ]
    }
   ],
   "source": [
    "# Function to find the maximum length of arrays in a column\n",
    "def find_max_length(df, column):\n",
    "    max_length = 0\n",
    "    for row in df[column]:\n",
    "        max_length = max(max_length, len(row))\n",
    "    return max_length\n",
    "\n",
    "# Find the longest inview article length in df_train\n",
    "max_inview_length_train = find_max_length(df_train, DEFAULT_INVIEW_ARTICLES_COL)\n",
    "\n",
    "# Find the longest inview article length in df_validation\n",
    "max_inview_length_validation = find_max_length(df_validation, DEFAULT_INVIEW_ARTICLES_COL)\n",
    "\n",
    "print(f\"Longest inview article length in df_train: {max_inview_length_train}\")\n",
    "print(f\"Longest inview article length in df_validation: {max_inview_length_validation}\")\n",
    "\n",
    "max_history_length_train = find_max_length(df_train, DEFAULT_HISTORY_ARTICLE_ID_COL)\n",
    "max_history_length_validation = find_max_length(df_validation, DEFAULT_HISTORY_ARTICLE_ID_COL)\n",
    "\n",
    "print(f\"Longest history length in df_train: {max_history_length_train}\")\n",
    "print(f\"Longest history length in df_validation: {max_history_length_validation}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "vscode": {
     "languageId": "ruby"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows with exactly one clicked article in df_train: 46855\n",
      "Number of rows with exactly one clicked article in df_validation: 48639\n"
     ]
    }
   ],
   "source": [
    "# Function to filter rows with exactly one clicked article\n",
    "def filter_rows_with_one_clicked_article(df, clicked_articles_col):\n",
    "    # Manually filter rows where the array has exactly one element\n",
    "    filtered_rows = []\n",
    "    for row in df.iter_rows(named=True):\n",
    "        if len(row[clicked_articles_col]) == 1:\n",
    "            filtered_rows.append(row)\n",
    "    return pl.DataFrame(filtered_rows)\n",
    "\n",
    "\n",
    "# Filter rows in df_train and df_validation\n",
    "df_train = filter_rows_with_one_clicked_article(df_train, DEFAULT_CLICKED_ARTICLES_COL)\n",
    "df_validation = filter_rows_with_one_clicked_article(df_validation, DEFAULT_CLICKED_ARTICLES_COL)\n",
    "\n",
    "# Print the results\n",
    "print(f\"Number of rows with exactly one clicked article in df_train: {df_train.shape[0]}\")\n",
    "print(f\"Number of rows with exactly one clicked article in df_validation: {df_validation.shape[0]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "vscode": {
     "languageId": "ruby"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (5, 7)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>user_id</th><th>article_id_fixed</th><th>article_ids_inview</th><th>article_ids_clicked</th><th>impression_id</th><th>impression_time</th><th>labels</th></tr><tr><td>i64</td><td>list[i64]</td><td>list[i64]</td><td>list[i64]</td><td>i64</td><td>i64</td><td>list[i64]</td></tr></thead><tbody><tr><td>1582341</td><td>[9753518, 9752685, … 9756397]</td><td>[9704868, 9789704, … 9717601]</td><td>[9789710]</td><td>150364157</td><td>468203</td><td>[0, 0, … 0]</td></tr><tr><td>1748668</td><td>[9778842, 9779019, … 9779968]</td><td>[9780498, 9780860, … 9780514]</td><td>[9779779]</td><td>412610146</td><td>468060</td><td>[0, 0, … 0]</td></tr><tr><td>2199962</td><td>[9778902, 9777397, … 9779263]</td><td>[9400164, 9780482, … 9780498]</td><td>[9780428]</td><td>525133049</td><td>468060</td><td>[0, 0, … 0]</td></tr><tr><td>1959613</td><td>[9775419, 9775402, … 9779738]</td><td>[9785718, 9785742, … 9785434]</td><td>[9785732]</td><td>73604021</td><td>468182</td><td>[0, 0, … 0]</td></tr><tr><td>612674</td><td>[9776023, 9665220, … 9778902]</td><td>[9781158, 9781362, … 9781057]</td><td>[9779225]</td><td>121362715</td><td>468063</td><td>[0, 0, … 0]</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (5, 7)\n",
       "┌─────────┬──────────────┬──────────────┬──────────────┬──────────────┬──────────────┬─────────────┐\n",
       "│ user_id ┆ article_id_f ┆ article_ids_ ┆ article_ids_ ┆ impression_i ┆ impression_t ┆ labels      │\n",
       "│ ---     ┆ ixed         ┆ inview       ┆ clicked      ┆ d            ┆ ime          ┆ ---         │\n",
       "│ i64     ┆ ---          ┆ ---          ┆ ---          ┆ ---          ┆ ---          ┆ list[i64]   │\n",
       "│         ┆ list[i64]    ┆ list[i64]    ┆ list[i64]    ┆ i64          ┆ i64          ┆             │\n",
       "╞═════════╪══════════════╪══════════════╪══════════════╪══════════════╪══════════════╪═════════════╡\n",
       "│ 1582341 ┆ [9753518,    ┆ [9704868,    ┆ [9789710]    ┆ 150364157    ┆ 468203       ┆ [0, 0, … 0] │\n",
       "│         ┆ 9752685, …   ┆ 9789704, …   ┆              ┆              ┆              ┆             │\n",
       "│         ┆ 9756397]     ┆ 9717601]     ┆              ┆              ┆              ┆             │\n",
       "│ 1748668 ┆ [9778842,    ┆ [9780498,    ┆ [9779779]    ┆ 412610146    ┆ 468060       ┆ [0, 0, … 0] │\n",
       "│         ┆ 9779019, …   ┆ 9780860, …   ┆              ┆              ┆              ┆             │\n",
       "│         ┆ 9779968]     ┆ 9780514]     ┆              ┆              ┆              ┆             │\n",
       "│ 2199962 ┆ [9778902,    ┆ [9400164,    ┆ [9780428]    ┆ 525133049    ┆ 468060       ┆ [0, 0, … 0] │\n",
       "│         ┆ 9777397, …   ┆ 9780482, …   ┆              ┆              ┆              ┆             │\n",
       "│         ┆ 9779263]     ┆ 9780498]     ┆              ┆              ┆              ┆             │\n",
       "│ 1959613 ┆ [9775419,    ┆ [9785718,    ┆ [9785732]    ┆ 73604021     ┆ 468182       ┆ [0, 0, … 0] │\n",
       "│         ┆ 9775402, …   ┆ 9785742, …   ┆              ┆              ┆              ┆             │\n",
       "│         ┆ 9779738]     ┆ 9785434]     ┆              ┆              ┆              ┆             │\n",
       "│ 612674  ┆ [9776023,    ┆ [9781158,    ┆ [9779225]    ┆ 121362715    ┆ 468063       ┆ [0, 0, … 0] │\n",
       "│         ┆ 9665220, …   ┆ 9781362, …   ┆              ┆              ┆              ┆             │\n",
       "│         ┆ 9778902]     ┆ 9781057]     ┆              ┆              ┆              ┆             │\n",
       "└─────────┴──────────────┴──────────────┴──────────────┴──────────────┴──────────────┴─────────────┘"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_validation.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "vscode": {
     "languageId": "ruby"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of users in df_train: 11274\n"
     ]
    }
   ],
   "source": [
    "print(f\"Number of users in df_train: {df_train['user_id'].n_unique()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (10, 21)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>article_id</th><th>title</th><th>subtitle</th><th>last_modified_time</th><th>premium</th><th>body</th><th>published_time</th><th>image_ids</th><th>article_type</th><th>url</th><th>ner_clusters</th><th>entity_groups</th><th>topics</th><th>category</th><th>subcategory</th><th>category_str</th><th>total_inviews</th><th>total_pageviews</th><th>total_read_time</th><th>sentiment_score</th><th>sentiment_label</th></tr><tr><td>i32</td><td>str</td><td>str</td><td>datetime[μs]</td><td>bool</td><td>str</td><td>datetime[μs]</td><td>list[i64]</td><td>str</td><td>str</td><td>list[str]</td><td>list[str]</td><td>list[str]</td><td>i16</td><td>list[i16]</td><td>str</td><td>i32</td><td>i32</td><td>f32</td><td>f32</td><td>str</td></tr></thead><tbody><tr><td>3001353</td><td>&quot;Natascha var ikke den første&quot;</td><td>&quot;Politiet frygter nu, at Natasc…</td><td>2023-06-29 06:20:33</td><td>false</td><td>&quot;Sagen om den østriske Natascha…</td><td>2006-08-31 08:06:45</td><td>[3150850]</td><td>&quot;article_default&quot;</td><td>&quot;https://ekstrabladet.dk/krimi/…</td><td>[]</td><td>[]</td><td>[&quot;Kriminalitet&quot;, &quot;Personfarlig kriminalitet&quot;]</td><td>140</td><td>[]</td><td>&quot;krimi&quot;</td><td>null</td><td>null</td><td>null</td><td>0.9955</td><td>&quot;Negative&quot;</td></tr><tr><td>3003065</td><td>&quot;Kun Star Wars tjente mere&quot;</td><td>&quot;Biografgængerne strømmer ind f…</td><td>2023-06-29 06:20:35</td><td>false</td><td>&quot;Vatikanet har opfordret til at…</td><td>2006-05-21 16:57:00</td><td>[3006712]</td><td>&quot;article_default&quot;</td><td>&quot;https://ekstrabladet.dk/underh…</td><td>[]</td><td>[]</td><td>[&quot;Underholdning&quot;, &quot;Film og tv&quot;, &quot;Økonomi&quot;]</td><td>414</td><td>[433, 434]</td><td>&quot;underholdning&quot;</td><td>null</td><td>null</td><td>null</td><td>0.846</td><td>&quot;Positive&quot;</td></tr><tr><td>3012771</td><td>&quot;Morten Bruun fyret i Sønderjys…</td><td>&quot;FODBOLD: Morten Bruun fyret me…</td><td>2023-06-29 06:20:39</td><td>false</td><td>&quot;Kemien mellem spillerne i Supe…</td><td>2006-05-01 14:28:40</td><td>[3177953]</td><td>&quot;article_default&quot;</td><td>&quot;https://ekstrabladet.dk/sport/…</td><td>[]</td><td>[]</td><td>[&quot;Erhverv&quot;, &quot;Kendt&quot;, … &quot;Ansættelsesforhold&quot;]</td><td>142</td><td>[196, 199]</td><td>&quot;sport&quot;</td><td>null</td><td>null</td><td>null</td><td>0.8241</td><td>&quot;Negative&quot;</td></tr><tr><td>3023463</td><td>&quot;Luderne flytter på landet&quot;</td><td>&quot;I landets tyndest befolkede om…</td><td>2023-06-29 06:20:43</td><td>false</td><td>&quot;Det frække erhverv rykker på l…</td><td>2007-03-24 08:27:59</td><td>[3184029]</td><td>&quot;article_default&quot;</td><td>&quot;https://ekstrabladet.dk/nyhede…</td><td>[]</td><td>[]</td><td>[&quot;Livsstil&quot;, &quot;Erotik&quot;]</td><td>118</td><td>[133]</td><td>&quot;nyheder&quot;</td><td>null</td><td>null</td><td>null</td><td>0.7053</td><td>&quot;Neutral&quot;</td></tr><tr><td>3032577</td><td>&quot;Cybersex: Hvornår er man utro?&quot;</td><td>&quot;En flirtende sms til den flott…</td><td>2023-06-29 06:20:46</td><td>false</td><td>&quot;De fleste af os mener, at et t…</td><td>2007-01-18 10:30:37</td><td>[3030463]</td><td>&quot;article_default&quot;</td><td>&quot;https://ekstrabladet.dk/sex_og…</td><td>[]</td><td>[]</td><td>[&quot;Livsstil&quot;, &quot;Partnerskab&quot;]</td><td>565</td><td>[]</td><td>&quot;sex_og_samliv&quot;</td><td>null</td><td>null</td><td>null</td><td>0.9307</td><td>&quot;Neutral&quot;</td></tr><tr><td>3033563</td><td>&quot;Kniven for struben-vært får se…</td><td>&quot;I aftenens udgave af &#x27;Med kniv…</td><td>2023-06-29 06:20:47</td><td>false</td><td>&quot;Når man ser fjerde program i T…</td><td>2007-03-27 10:22:08</td><td>[3005524, 3005525]</td><td>&quot;article_default&quot;</td><td>&quot;https://ekstrabladet.dk/underh…</td><td>[]</td><td>[]</td><td>[&quot;Livsstil&quot;, &quot;Underholdning&quot;, … &quot;Mad og drikke&quot;]</td><td>414</td><td>[433, 436]</td><td>&quot;underholdning&quot;</td><td>null</td><td>null</td><td>null</td><td>0.9371</td><td>&quot;Neutral&quot;</td></tr><tr><td>3034608</td><td>&quot;Willy Strube har begået selvmo…</td><td>&quot;Den tidligere SiD-chef tog sit…</td><td>2023-06-29 06:20:49</td><td>false</td><td>&quot;Den tidligere formand for Indu…</td><td>2001-10-19 12:30:00</td><td>[3204848]</td><td>&quot;article_default&quot;</td><td>&quot;https://ekstrabladet.dk/nyhede…</td><td>[&quot;Willy Strube&quot;, &quot;Willy Strube&quot;, &quot;Willy Strube&quot;]</td><td>[&quot;PER&quot;, &quot;PER&quot;, &quot;PER&quot;]</td><td>[&quot;Kriminalitet&quot;, &quot;Erhverv&quot;, … &quot;Offentlig instans&quot;]</td><td>118</td><td>[130]</td><td>&quot;nyheder&quot;</td><td>null</td><td>null</td><td>null</td><td>0.9971</td><td>&quot;Negative&quot;</td></tr><tr><td>3034849</td><td>&quot;Venner for livet&quot;</td><td>&quot;VK-REGERINGEN&quot;</td><td>2023-06-29 06:20:50</td><td>false</td><td>&quot;VK-REGERINGEN\n",
       "håndplukkede Bjø…</td><td>2003-01-09 06:00:00</td><td>null</td><td>&quot;article_default&quot;</td><td>&quot;https://ekstrabladet.dk/incomi…</td><td>[]</td><td>[]</td><td>[&quot;Kendt&quot;, &quot;Politik&quot;, &quot;National politik&quot;]</td><td>2</td><td>[]</td><td>&quot;incoming&quot;</td><td>null</td><td>null</td><td>null</td><td>0.8454</td><td>&quot;Neutral&quot;</td></tr><tr><td>3035648</td><td>&quot;Dronning af escort-branchen&quot;</td><td>&quot;Trine Michelsen hjælper københ…</td><td>2023-06-29 06:20:52</td><td>false</td><td>&quot;En af escortbranchens største …</td><td>2003-06-17 07:10:00</td><td>[3082573]</td><td>&quot;article_default&quot;</td><td>&quot;https://ekstrabladet.dk/krimi/…</td><td>[]</td><td>[]</td><td>[&quot;Erhverv&quot;, &quot;Livsstil&quot;, … &quot;Erotik&quot;]</td><td>140</td><td>[]</td><td>&quot;krimi&quot;</td><td>null</td><td>null</td><td>null</td><td>0.8814</td><td>&quot;Neutral&quot;</td></tr><tr><td>3036444</td><td>&quot;Mia kendte sandsynligvis sin m…</td><td>&quot;Hun var ikke den type, der søg…</td><td>2023-06-29 06:20:54</td><td>false</td><td>&quot;Den 12-årige Mia Teglgaard Spr…</td><td>2003-07-13 19:50:00</td><td>null</td><td>&quot;article_default&quot;</td><td>&quot;https://ekstrabladet.dk/krimi/…</td><td>[&quot;Mia Teglgaard Sprotte&quot;, &quot;Erik Andersen&quot;, … &quot;Mia Teglgaard Sprotte&quot;]</td><td>[&quot;PER&quot;, &quot;PER&quot;, … &quot;PER&quot;]</td><td>[&quot;Kriminalitet&quot;, &quot;Personfarlig kriminalitet&quot;]</td><td>140</td><td>[]</td><td>&quot;krimi&quot;</td><td>null</td><td>null</td><td>null</td><td>0.9752</td><td>&quot;Negative&quot;</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (10, 21)\n",
       "┌───────────┬───────────┬───────────┬───────────┬───┬───────────┬───────────┬───────────┬──────────┐\n",
       "│ article_i ┆ title     ┆ subtitle  ┆ last_modi ┆ … ┆ total_pag ┆ total_rea ┆ sentiment ┆ sentimen │\n",
       "│ d         ┆ ---       ┆ ---       ┆ fied_time ┆   ┆ eviews    ┆ d_time    ┆ _score    ┆ t_label  │\n",
       "│ ---       ┆ str       ┆ str       ┆ ---       ┆   ┆ ---       ┆ ---       ┆ ---       ┆ ---      │\n",
       "│ i32       ┆           ┆           ┆ datetime[ ┆   ┆ i32       ┆ f32       ┆ f32       ┆ str      │\n",
       "│           ┆           ┆           ┆ μs]       ┆   ┆           ┆           ┆           ┆          │\n",
       "╞═══════════╪═══════════╪═══════════╪═══════════╪═══╪═══════════╪═══════════╪═══════════╪══════════╡\n",
       "│ 3001353   ┆ Natascha  ┆ Politiet  ┆ 2023-06-2 ┆ … ┆ null      ┆ null      ┆ 0.9955    ┆ Negative │\n",
       "│           ┆ var ikke  ┆ frygter   ┆ 9         ┆   ┆           ┆           ┆           ┆          │\n",
       "│           ┆ den       ┆ nu, at    ┆ 06:20:33  ┆   ┆           ┆           ┆           ┆          │\n",
       "│           ┆ første    ┆ Natasc…   ┆           ┆   ┆           ┆           ┆           ┆          │\n",
       "│ 3003065   ┆ Kun Star  ┆ Biografgæ ┆ 2023-06-2 ┆ … ┆ null      ┆ null      ┆ 0.846     ┆ Positive │\n",
       "│           ┆ Wars      ┆ ngerne    ┆ 9         ┆   ┆           ┆           ┆           ┆          │\n",
       "│           ┆ tjente    ┆ strømmer  ┆ 06:20:35  ┆   ┆           ┆           ┆           ┆          │\n",
       "│           ┆ mere      ┆ ind f…    ┆           ┆   ┆           ┆           ┆           ┆          │\n",
       "│ 3012771   ┆ Morten    ┆ FODBOLD:  ┆ 2023-06-2 ┆ … ┆ null      ┆ null      ┆ 0.8241    ┆ Negative │\n",
       "│           ┆ Bruun     ┆ Morten    ┆ 9         ┆   ┆           ┆           ┆           ┆          │\n",
       "│           ┆ fyret i   ┆ Bruun     ┆ 06:20:39  ┆   ┆           ┆           ┆           ┆          │\n",
       "│           ┆ Sønderjys ┆ fyret me… ┆           ┆   ┆           ┆           ┆           ┆          │\n",
       "│           ┆ …         ┆           ┆           ┆   ┆           ┆           ┆           ┆          │\n",
       "│ 3023463   ┆ Luderne   ┆ I landets ┆ 2023-06-2 ┆ … ┆ null      ┆ null      ┆ 0.7053    ┆ Neutral  │\n",
       "│           ┆ flytter   ┆ tyndest   ┆ 9         ┆   ┆           ┆           ┆           ┆          │\n",
       "│           ┆ på landet ┆ befolkede ┆ 06:20:43  ┆   ┆           ┆           ┆           ┆          │\n",
       "│           ┆           ┆ om…       ┆           ┆   ┆           ┆           ┆           ┆          │\n",
       "│ 3032577   ┆ Cybersex: ┆ En        ┆ 2023-06-2 ┆ … ┆ null      ┆ null      ┆ 0.9307    ┆ Neutral  │\n",
       "│           ┆ Hvornår   ┆ flirtende ┆ 9         ┆   ┆           ┆           ┆           ┆          │\n",
       "│           ┆ er man    ┆ sms til   ┆ 06:20:46  ┆   ┆           ┆           ┆           ┆          │\n",
       "│           ┆ utro?     ┆ den       ┆           ┆   ┆           ┆           ┆           ┆          │\n",
       "│           ┆           ┆ flott…    ┆           ┆   ┆           ┆           ┆           ┆          │\n",
       "│ 3033563   ┆ Kniven    ┆ I         ┆ 2023-06-2 ┆ … ┆ null      ┆ null      ┆ 0.9371    ┆ Neutral  │\n",
       "│           ┆ for strub ┆ aftenens  ┆ 9         ┆   ┆           ┆           ┆           ┆          │\n",
       "│           ┆ en-vært   ┆ udgave af ┆ 06:20:47  ┆   ┆           ┆           ┆           ┆          │\n",
       "│           ┆ får se…   ┆ 'Med      ┆           ┆   ┆           ┆           ┆           ┆          │\n",
       "│           ┆           ┆ kniv…     ┆           ┆   ┆           ┆           ┆           ┆          │\n",
       "│ 3034608   ┆ Willy     ┆ Den       ┆ 2023-06-2 ┆ … ┆ null      ┆ null      ┆ 0.9971    ┆ Negative │\n",
       "│           ┆ Strube    ┆ tidligere ┆ 9         ┆   ┆           ┆           ┆           ┆          │\n",
       "│           ┆ har       ┆ SiD-chef  ┆ 06:20:49  ┆   ┆           ┆           ┆           ┆          │\n",
       "│           ┆ begået    ┆ tog sit…  ┆           ┆   ┆           ┆           ┆           ┆          │\n",
       "│           ┆ selvmo…   ┆           ┆           ┆   ┆           ┆           ┆           ┆          │\n",
       "│ 3034849   ┆ Venner    ┆ VK-REGERI ┆ 2023-06-2 ┆ … ┆ null      ┆ null      ┆ 0.8454    ┆ Neutral  │\n",
       "│           ┆ for livet ┆ NGEN      ┆ 9         ┆   ┆           ┆           ┆           ┆          │\n",
       "│           ┆           ┆           ┆ 06:20:50  ┆   ┆           ┆           ┆           ┆          │\n",
       "│ 3035648   ┆ Dronning  ┆ Trine     ┆ 2023-06-2 ┆ … ┆ null      ┆ null      ┆ 0.8814    ┆ Neutral  │\n",
       "│           ┆ af escort ┆ Michelsen ┆ 9         ┆   ┆           ┆           ┆           ┆          │\n",
       "│           ┆ -branchen ┆ hjælper   ┆ 06:20:52  ┆   ┆           ┆           ┆           ┆          │\n",
       "│           ┆           ┆ københ…   ┆           ┆   ┆           ┆           ┆           ┆          │\n",
       "│ 3036444   ┆ Mia       ┆ Hun var   ┆ 2023-06-2 ┆ … ┆ null      ┆ null      ┆ 0.9752    ┆ Negative │\n",
       "│           ┆ kendte    ┆ ikke den  ┆ 9         ┆   ┆           ┆           ┆           ┆          │\n",
       "│           ┆ sandsynli ┆ type, der ┆ 06:20:54  ┆   ┆           ┆           ┆           ┆          │\n",
       "│           ┆ gvis sin  ┆ søg…      ┆           ┆   ┆           ┆           ┆           ┆          │\n",
       "│           ┆ m…        ┆           ┆           ┆   ┆           ┆           ┆           ┆          │\n",
       "└───────────┴───────────┴───────────┴───────────┴───┴───────────┴───────────┴───────────┴──────────┘"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_articles = pl.read_parquet(PATH.joinpath(\"articles.parquet\"))\n",
    "df_articles.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_articles = df_articles.with_columns([\n",
    "    (\n",
    "        pl.col(DEFAULT_ARTICLE_PUBLISHED_TIMESTAMP_COL)\n",
    "        .cast(pl.Datetime)                   # Step 3: Cast to Datetime\n",
    "        .dt.epoch(time_unit='s')            # Step 4: Convert to Epoch Seconds\n",
    "        / 3600                               # Step 5: Convert Seconds to Hours\n",
    "    ).cast(pl.Int64)                         # Step 6: Cast to Integer\n",
    "    .alias(DEFAULT_ARTICLE_PUBLISHED_TIMESTAMP_COL)  # Step 7: Alias the Column\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (10, 21)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>article_id</th><th>title</th><th>subtitle</th><th>last_modified_time</th><th>premium</th><th>body</th><th>published_time</th><th>image_ids</th><th>article_type</th><th>url</th><th>ner_clusters</th><th>entity_groups</th><th>topics</th><th>category</th><th>subcategory</th><th>category_str</th><th>total_inviews</th><th>total_pageviews</th><th>total_read_time</th><th>sentiment_score</th><th>sentiment_label</th></tr><tr><td>i32</td><td>str</td><td>str</td><td>datetime[μs]</td><td>bool</td><td>str</td><td>i64</td><td>list[i64]</td><td>str</td><td>str</td><td>list[str]</td><td>list[str]</td><td>list[str]</td><td>i16</td><td>list[i16]</td><td>str</td><td>i32</td><td>i32</td><td>f32</td><td>f32</td><td>str</td></tr></thead><tbody><tr><td>3001353</td><td>&quot;Natascha var ikke den første&quot;</td><td>&quot;Politiet frygter nu, at Natasc…</td><td>2023-06-29 06:20:33</td><td>false</td><td>&quot;Sagen om den østriske Natascha…</td><td>321392</td><td>[3150850]</td><td>&quot;article_default&quot;</td><td>&quot;https://ekstrabladet.dk/krimi/…</td><td>[]</td><td>[]</td><td>[&quot;Kriminalitet&quot;, &quot;Personfarlig kriminalitet&quot;]</td><td>140</td><td>[]</td><td>&quot;krimi&quot;</td><td>null</td><td>null</td><td>null</td><td>0.9955</td><td>&quot;Negative&quot;</td></tr><tr><td>3003065</td><td>&quot;Kun Star Wars tjente mere&quot;</td><td>&quot;Biografgængerne strømmer ind f…</td><td>2023-06-29 06:20:35</td><td>false</td><td>&quot;Vatikanet har opfordret til at…</td><td>318952</td><td>[3006712]</td><td>&quot;article_default&quot;</td><td>&quot;https://ekstrabladet.dk/underh…</td><td>[]</td><td>[]</td><td>[&quot;Underholdning&quot;, &quot;Film og tv&quot;, &quot;Økonomi&quot;]</td><td>414</td><td>[433, 434]</td><td>&quot;underholdning&quot;</td><td>null</td><td>null</td><td>null</td><td>0.846</td><td>&quot;Positive&quot;</td></tr><tr><td>3012771</td><td>&quot;Morten Bruun fyret i Sønderjys…</td><td>&quot;FODBOLD: Morten Bruun fyret me…</td><td>2023-06-29 06:20:39</td><td>false</td><td>&quot;Kemien mellem spillerne i Supe…</td><td>318470</td><td>[3177953]</td><td>&quot;article_default&quot;</td><td>&quot;https://ekstrabladet.dk/sport/…</td><td>[]</td><td>[]</td><td>[&quot;Erhverv&quot;, &quot;Kendt&quot;, … &quot;Ansættelsesforhold&quot;]</td><td>142</td><td>[196, 199]</td><td>&quot;sport&quot;</td><td>null</td><td>null</td><td>null</td><td>0.8241</td><td>&quot;Negative&quot;</td></tr><tr><td>3023463</td><td>&quot;Luderne flytter på landet&quot;</td><td>&quot;I landets tyndest befolkede om…</td><td>2023-06-29 06:20:43</td><td>false</td><td>&quot;Det frække erhverv rykker på l…</td><td>326312</td><td>[3184029]</td><td>&quot;article_default&quot;</td><td>&quot;https://ekstrabladet.dk/nyhede…</td><td>[]</td><td>[]</td><td>[&quot;Livsstil&quot;, &quot;Erotik&quot;]</td><td>118</td><td>[133]</td><td>&quot;nyheder&quot;</td><td>null</td><td>null</td><td>null</td><td>0.7053</td><td>&quot;Neutral&quot;</td></tr><tr><td>3032577</td><td>&quot;Cybersex: Hvornår er man utro?&quot;</td><td>&quot;En flirtende sms til den flott…</td><td>2023-06-29 06:20:46</td><td>false</td><td>&quot;De fleste af os mener, at et t…</td><td>324754</td><td>[3030463]</td><td>&quot;article_default&quot;</td><td>&quot;https://ekstrabladet.dk/sex_og…</td><td>[]</td><td>[]</td><td>[&quot;Livsstil&quot;, &quot;Partnerskab&quot;]</td><td>565</td><td>[]</td><td>&quot;sex_og_samliv&quot;</td><td>null</td><td>null</td><td>null</td><td>0.9307</td><td>&quot;Neutral&quot;</td></tr><tr><td>3033563</td><td>&quot;Kniven for struben-vært får se…</td><td>&quot;I aftenens udgave af &#x27;Med kniv…</td><td>2023-06-29 06:20:47</td><td>false</td><td>&quot;Når man ser fjerde program i T…</td><td>326386</td><td>[3005524, 3005525]</td><td>&quot;article_default&quot;</td><td>&quot;https://ekstrabladet.dk/underh…</td><td>[]</td><td>[]</td><td>[&quot;Livsstil&quot;, &quot;Underholdning&quot;, … &quot;Mad og drikke&quot;]</td><td>414</td><td>[433, 436]</td><td>&quot;underholdning&quot;</td><td>null</td><td>null</td><td>null</td><td>0.9371</td><td>&quot;Neutral&quot;</td></tr><tr><td>3034608</td><td>&quot;Willy Strube har begået selvmo…</td><td>&quot;Den tidligere SiD-chef tog sit…</td><td>2023-06-29 06:20:49</td><td>false</td><td>&quot;Den tidligere formand for Indu…</td><td>278748</td><td>[3204848]</td><td>&quot;article_default&quot;</td><td>&quot;https://ekstrabladet.dk/nyhede…</td><td>[&quot;Willy Strube&quot;, &quot;Willy Strube&quot;, &quot;Willy Strube&quot;]</td><td>[&quot;PER&quot;, &quot;PER&quot;, &quot;PER&quot;]</td><td>[&quot;Kriminalitet&quot;, &quot;Erhverv&quot;, … &quot;Offentlig instans&quot;]</td><td>118</td><td>[130]</td><td>&quot;nyheder&quot;</td><td>null</td><td>null</td><td>null</td><td>0.9971</td><td>&quot;Negative&quot;</td></tr><tr><td>3034849</td><td>&quot;Venner for livet&quot;</td><td>&quot;VK-REGERINGEN&quot;</td><td>2023-06-29 06:20:50</td><td>false</td><td>&quot;VK-REGERINGEN\n",
       "håndplukkede Bjø…</td><td>289470</td><td>null</td><td>&quot;article_default&quot;</td><td>&quot;https://ekstrabladet.dk/incomi…</td><td>[]</td><td>[]</td><td>[&quot;Kendt&quot;, &quot;Politik&quot;, &quot;National politik&quot;]</td><td>2</td><td>[]</td><td>&quot;incoming&quot;</td><td>null</td><td>null</td><td>null</td><td>0.8454</td><td>&quot;Neutral&quot;</td></tr><tr><td>3035648</td><td>&quot;Dronning af escort-branchen&quot;</td><td>&quot;Trine Michelsen hjælper københ…</td><td>2023-06-29 06:20:52</td><td>false</td><td>&quot;En af escortbranchens største …</td><td>293287</td><td>[3082573]</td><td>&quot;article_default&quot;</td><td>&quot;https://ekstrabladet.dk/krimi/…</td><td>[]</td><td>[]</td><td>[&quot;Erhverv&quot;, &quot;Livsstil&quot;, … &quot;Erotik&quot;]</td><td>140</td><td>[]</td><td>&quot;krimi&quot;</td><td>null</td><td>null</td><td>null</td><td>0.8814</td><td>&quot;Neutral&quot;</td></tr><tr><td>3036444</td><td>&quot;Mia kendte sandsynligvis sin m…</td><td>&quot;Hun var ikke den type, der søg…</td><td>2023-06-29 06:20:54</td><td>false</td><td>&quot;Den 12-årige Mia Teglgaard Spr…</td><td>293923</td><td>null</td><td>&quot;article_default&quot;</td><td>&quot;https://ekstrabladet.dk/krimi/…</td><td>[&quot;Mia Teglgaard Sprotte&quot;, &quot;Erik Andersen&quot;, … &quot;Mia Teglgaard Sprotte&quot;]</td><td>[&quot;PER&quot;, &quot;PER&quot;, … &quot;PER&quot;]</td><td>[&quot;Kriminalitet&quot;, &quot;Personfarlig kriminalitet&quot;]</td><td>140</td><td>[]</td><td>&quot;krimi&quot;</td><td>null</td><td>null</td><td>null</td><td>0.9752</td><td>&quot;Negative&quot;</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (10, 21)\n",
       "┌───────────┬───────────┬───────────┬───────────┬───┬───────────┬───────────┬───────────┬──────────┐\n",
       "│ article_i ┆ title     ┆ subtitle  ┆ last_modi ┆ … ┆ total_pag ┆ total_rea ┆ sentiment ┆ sentimen │\n",
       "│ d         ┆ ---       ┆ ---       ┆ fied_time ┆   ┆ eviews    ┆ d_time    ┆ _score    ┆ t_label  │\n",
       "│ ---       ┆ str       ┆ str       ┆ ---       ┆   ┆ ---       ┆ ---       ┆ ---       ┆ ---      │\n",
       "│ i32       ┆           ┆           ┆ datetime[ ┆   ┆ i32       ┆ f32       ┆ f32       ┆ str      │\n",
       "│           ┆           ┆           ┆ μs]       ┆   ┆           ┆           ┆           ┆          │\n",
       "╞═══════════╪═══════════╪═══════════╪═══════════╪═══╪═══════════╪═══════════╪═══════════╪══════════╡\n",
       "│ 3001353   ┆ Natascha  ┆ Politiet  ┆ 2023-06-2 ┆ … ┆ null      ┆ null      ┆ 0.9955    ┆ Negative │\n",
       "│           ┆ var ikke  ┆ frygter   ┆ 9         ┆   ┆           ┆           ┆           ┆          │\n",
       "│           ┆ den       ┆ nu, at    ┆ 06:20:33  ┆   ┆           ┆           ┆           ┆          │\n",
       "│           ┆ første    ┆ Natasc…   ┆           ┆   ┆           ┆           ┆           ┆          │\n",
       "│ 3003065   ┆ Kun Star  ┆ Biografgæ ┆ 2023-06-2 ┆ … ┆ null      ┆ null      ┆ 0.846     ┆ Positive │\n",
       "│           ┆ Wars      ┆ ngerne    ┆ 9         ┆   ┆           ┆           ┆           ┆          │\n",
       "│           ┆ tjente    ┆ strømmer  ┆ 06:20:35  ┆   ┆           ┆           ┆           ┆          │\n",
       "│           ┆ mere      ┆ ind f…    ┆           ┆   ┆           ┆           ┆           ┆          │\n",
       "│ 3012771   ┆ Morten    ┆ FODBOLD:  ┆ 2023-06-2 ┆ … ┆ null      ┆ null      ┆ 0.8241    ┆ Negative │\n",
       "│           ┆ Bruun     ┆ Morten    ┆ 9         ┆   ┆           ┆           ┆           ┆          │\n",
       "│           ┆ fyret i   ┆ Bruun     ┆ 06:20:39  ┆   ┆           ┆           ┆           ┆          │\n",
       "│           ┆ Sønderjys ┆ fyret me… ┆           ┆   ┆           ┆           ┆           ┆          │\n",
       "│           ┆ …         ┆           ┆           ┆   ┆           ┆           ┆           ┆          │\n",
       "│ 3023463   ┆ Luderne   ┆ I landets ┆ 2023-06-2 ┆ … ┆ null      ┆ null      ┆ 0.7053    ┆ Neutral  │\n",
       "│           ┆ flytter   ┆ tyndest   ┆ 9         ┆   ┆           ┆           ┆           ┆          │\n",
       "│           ┆ på landet ┆ befolkede ┆ 06:20:43  ┆   ┆           ┆           ┆           ┆          │\n",
       "│           ┆           ┆ om…       ┆           ┆   ┆           ┆           ┆           ┆          │\n",
       "│ 3032577   ┆ Cybersex: ┆ En        ┆ 2023-06-2 ┆ … ┆ null      ┆ null      ┆ 0.9307    ┆ Neutral  │\n",
       "│           ┆ Hvornår   ┆ flirtende ┆ 9         ┆   ┆           ┆           ┆           ┆          │\n",
       "│           ┆ er man    ┆ sms til   ┆ 06:20:46  ┆   ┆           ┆           ┆           ┆          │\n",
       "│           ┆ utro?     ┆ den       ┆           ┆   ┆           ┆           ┆           ┆          │\n",
       "│           ┆           ┆ flott…    ┆           ┆   ┆           ┆           ┆           ┆          │\n",
       "│ 3033563   ┆ Kniven    ┆ I         ┆ 2023-06-2 ┆ … ┆ null      ┆ null      ┆ 0.9371    ┆ Neutral  │\n",
       "│           ┆ for strub ┆ aftenens  ┆ 9         ┆   ┆           ┆           ┆           ┆          │\n",
       "│           ┆ en-vært   ┆ udgave af ┆ 06:20:47  ┆   ┆           ┆           ┆           ┆          │\n",
       "│           ┆ får se…   ┆ 'Med      ┆           ┆   ┆           ┆           ┆           ┆          │\n",
       "│           ┆           ┆ kniv…     ┆           ┆   ┆           ┆           ┆           ┆          │\n",
       "│ 3034608   ┆ Willy     ┆ Den       ┆ 2023-06-2 ┆ … ┆ null      ┆ null      ┆ 0.9971    ┆ Negative │\n",
       "│           ┆ Strube    ┆ tidligere ┆ 9         ┆   ┆           ┆           ┆           ┆          │\n",
       "│           ┆ har       ┆ SiD-chef  ┆ 06:20:49  ┆   ┆           ┆           ┆           ┆          │\n",
       "│           ┆ begået    ┆ tog sit…  ┆           ┆   ┆           ┆           ┆           ┆          │\n",
       "│           ┆ selvmo…   ┆           ┆           ┆   ┆           ┆           ┆           ┆          │\n",
       "│ 3034849   ┆ Venner    ┆ VK-REGERI ┆ 2023-06-2 ┆ … ┆ null      ┆ null      ┆ 0.8454    ┆ Neutral  │\n",
       "│           ┆ for livet ┆ NGEN      ┆ 9         ┆   ┆           ┆           ┆           ┆          │\n",
       "│           ┆           ┆           ┆ 06:20:50  ┆   ┆           ┆           ┆           ┆          │\n",
       "│ 3035648   ┆ Dronning  ┆ Trine     ┆ 2023-06-2 ┆ … ┆ null      ┆ null      ┆ 0.8814    ┆ Neutral  │\n",
       "│           ┆ af escort ┆ Michelsen ┆ 9         ┆   ┆           ┆           ┆           ┆          │\n",
       "│           ┆ -branchen ┆ hjælper   ┆ 06:20:52  ┆   ┆           ┆           ┆           ┆          │\n",
       "│           ┆           ┆ københ…   ┆           ┆   ┆           ┆           ┆           ┆          │\n",
       "│ 3036444   ┆ Mia       ┆ Hun var   ┆ 2023-06-2 ┆ … ┆ null      ┆ null      ┆ 0.9752    ┆ Negative │\n",
       "│           ┆ kendte    ┆ ikke den  ┆ 9         ┆   ┆           ┆           ┆           ┆          │\n",
       "│           ┆ sandsynli ┆ type, der ┆ 06:20:54  ┆   ┆           ┆           ┆           ┆          │\n",
       "│           ┆ gvis sin  ┆ søg…      ┆           ┆   ┆           ┆           ┆           ┆          │\n",
       "│           ┆ m…        ┆           ┆           ┆   ┆           ┆           ┆           ┆          │\n",
       "└───────────┴───────────┴───────────┴───────────┴───┴───────────┴───────────┴───────────┴──────────┘"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_articles.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "DOCVEC = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the document vector parquet file\n",
    "document_vector_path = Path(\"./Ekstra_Bladet_word2vec/document_vector.parquet\")\n",
    "df_document_vector = pl.read_parquet(document_vector_path)\n",
    "\n",
    "# Display the first few rows of the dataframe\n",
    "\n",
    "article_mapping = create_article_id_to_value_mapping(df=df_document_vector, value_col=\"document_vector\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (5, 2)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>article_id</th><th>document_vector</th></tr><tr><td>i32</td><td>list[f32]</td></tr></thead><tbody><tr><td>3000022</td><td>[0.065424, -0.047425, … 0.035706]</td></tr><tr><td>3000063</td><td>[0.028815, -0.000166, … 0.027167]</td></tr><tr><td>3000613</td><td>[0.037971, 0.033923, … 0.063961]</td></tr><tr><td>3000700</td><td>[0.046524, 0.002913, … 0.023423]</td></tr><tr><td>3000840</td><td>[0.014737, 0.024068, … 0.045991]</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (5, 2)\n",
       "┌────────────┬─────────────────────────────────┐\n",
       "│ article_id ┆ document_vector                 │\n",
       "│ ---        ┆ ---                             │\n",
       "│ i32        ┆ list[f32]                       │\n",
       "╞════════════╪═════════════════════════════════╡\n",
       "│ 3000022    ┆ [0.065424, -0.047425, … 0.0357… │\n",
       "│ 3000063    ┆ [0.028815, -0.000166, … 0.0271… │\n",
       "│ 3000613    ┆ [0.037971, 0.033923, … 0.06396… │\n",
       "│ 3000700    ┆ [0.046524, 0.002913, … 0.02342… │\n",
       "│ 3000840    ┆ [0.014737, 0.024068, … 0.04599… │\n",
       "└────────────┴─────────────────────────────────┘"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_document_vector.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embedding tokenized article title"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reduce dimensionality of embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Uses existing function for both categories and topics\n",
    "* Handles array of topics by joining with separator\n",
    "* Limits to first 3 topics to avoid sequence length issues\n",
    "* Maintains consistent encoding approach across feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sys:1: MapWithoutReturnDtypeWarning: Calling `map_elements` without specifying `return_dtype` can lead to unpredictable results. Specify `return_dtype` to silence this warning.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "MAX_TITLE_LENGTH = 30\n",
    "from transformers import ConvBertTokenizer, ConvBertModel\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(device)\n",
    "\n",
    "\n",
    "transformer_tokenizer = ConvBertTokenizer.from_pretrained(\"sarnikowski/convbert-small-da-cased\")\n",
    "transformer_model = ConvBertModel.from_pretrained(\"sarnikowski/convbert-small-da-cased\")\n",
    "\n",
    "\n",
    "# For categories (keep as is - works with single strings)\n",
    "df_articles, category_encoded_col_name = convert_text2encoding_with_transformers(\n",
    "    df_articles, \n",
    "    transformer_tokenizer,\n",
    "    DEFAULT_CATEGORY_STR_COL,\n",
    "    max_length=MAX_TITLE_LENGTH\n",
    ")\n",
    "\n",
    "# For topics - first join the array into single string\n",
    "df_articles = df_articles.with_columns(\n",
    "    pl.col(DEFAULT_TOPICS_COL).map_elements(lambda x: \" | \".join(x[:3])).alias(f\"{DEFAULT_TOPICS_COL}_joined\")\n",
    ")\n",
    "\n",
    "# Then encode the joined topics\n",
    "df_articles, topics_encoded_col_name = convert_text2encoding_with_transformers(\n",
    "    df_articles,\n",
    "    transformer_tokenizer, \n",
    "    f\"{DEFAULT_TOPICS_COL}_joined\",\n",
    "    max_length=MAX_TITLE_LENGTH\n",
    ")\n",
    "\n",
    "# Create mappings with encoded columns\n",
    "category_mapping = create_article_id_to_value_mapping(\n",
    "    df=df_articles, \n",
    "    value_col=category_encoded_col_name\n",
    ")\n",
    "\n",
    "topic_mapping = create_article_id_to_value_mapping(\n",
    "    df=df_articles, \n",
    "    value_col=topics_encoded_col_name\n",
    ")\n",
    "# Create mappings for numerical features\n",
    "pageviews_mapping = create_article_id_to_value_mapping(\n",
    "    df=df_articles, \n",
    "    value_col=DEFAULT_TOTAL_PAGEVIEWS_COL\n",
    ")\n",
    "\n",
    "read_time_mapping = create_article_id_to_value_mapping(\n",
    "    df=df_articles, \n",
    "    value_col=DEFAULT_TOTAL_READ_TIME_COL\n",
    ")\n",
    "\n",
    "sentiment_mapping = create_article_id_to_value_mapping(\n",
    "    df=df_articles, \n",
    "    value_col=DEFAULT_SENTIMENT_SCORE_COL\n",
    ")\n",
    "\n",
    "timestamp_mapping = create_article_id_to_value_mapping(\n",
    "    df_articles,\n",
    "    value_col=DEFAULT_ARTICLE_PUBLISHED_TIMESTAMP_COL\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Init dataloaders\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting preprocessing...\n",
      "Preprocessing data...\n",
      "(46855, 7)\n",
      "Data preprocessing completed in 19.94 seconds.\n",
      "Starting preprocessing...\n",
      "Preprocessing data...\n",
      "(48639, 7)\n",
      "Data preprocessing completed in 25.36 seconds.\n"
     ]
    }
   ],
   "source": [
    "train_dataset = NRMSDataSet(\n",
    "    behaviors=df_train,\n",
    "    article_dict=article_mapping,\n",
    "    unknown_representation=\"zeros\",\n",
    "    history_column=DEFAULT_HISTORY_ARTICLE_ID_COL,\n",
    "    eval_mode=False,\n",
    "    category_mapping=category_mapping,\n",
    "    topic_mapping=topic_mapping,\n",
    "    pageviews_mapping=pageviews_mapping,\n",
    "    read_time_mapping=read_time_mapping,\n",
    "    sentiment_mapping=sentiment_mapping,\n",
    "    timestamp_mapping=timestamp_mapping\n",
    "\n",
    ")\n",
    "val_dataset = NRMSDataSet(\n",
    "    behaviors=df_validation,\n",
    "    article_dict=article_mapping,\n",
    "    unknown_representation=\"zeros\",\n",
    "    history_column=DEFAULT_HISTORY_ARTICLE_ID_COL,\n",
    "    eval_mode=True,\n",
    "    category_mapping=category_mapping,\n",
    "    topic_mapping=topic_mapping,\n",
    "    pageviews_mapping=pageviews_mapping,\n",
    "    read_time_mapping=read_time_mapping,\n",
    "    sentiment_mapping=sentiment_mapping,\n",
    "    timestamp_mapping=timestamp_mapping,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample 0:\n",
      "his_input_title shape: torch.Size([50, 300])\n",
      "pred_input_title shape: torch.Size([50, 128]) 0.0\n",
      "pred_input_category shape: torch.Size([50, 128]) 0.0\n",
      "pred_input_topic shape: torch.Size([50]) 0.0\n",
      "pred_input_pageviews shape: torch.Size([50]) 0.0\n",
      "Targets shape: torch.Size([5]) , torch.float32 1.0\n",
      "impression id: 337281027\n",
      "Sample 1:\n",
      "his_input_title shape: torch.Size([50, 300])\n",
      "pred_input_title shape: torch.Size([50, 128]) 0.0\n",
      "pred_input_category shape: torch.Size([50, 128]) 0.0\n",
      "pred_input_topic shape: torch.Size([50]) 0.0\n",
      "pred_input_pageviews shape: torch.Size([50]) 0.0\n",
      "Targets shape: torch.Size([5]) , torch.float32 1.0\n",
      "impression id: 279245246\n",
      "Sample 2:\n",
      "his_input_title shape: torch.Size([50, 300])\n",
      "pred_input_title shape: torch.Size([50, 128]) 0.0\n",
      "pred_input_category shape: torch.Size([50, 128]) 0.0\n",
      "pred_input_topic shape: torch.Size([50]) 0.0\n",
      "pred_input_pageviews shape: torch.Size([50]) 0.0\n",
      "Targets shape: torch.Size([5]) , torch.float32 1.0\n",
      "impression id: 142964085\n",
      "Sample 3:\n",
      "his_input_title shape: torch.Size([50, 300])\n",
      "pred_input_title shape: torch.Size([50, 128]) 0.0\n",
      "pred_input_category shape: torch.Size([50, 128]) 0.0\n",
      "pred_input_topic shape: torch.Size([50]) 0.0\n",
      "pred_input_pageviews shape: torch.Size([50]) 0.0\n",
      "Targets shape: torch.Size([5]) , torch.float32 1.0\n",
      "impression id: 263407383\n",
      "Sample 4:\n",
      "his_input_title shape: torch.Size([50, 300])\n",
      "pred_input_title shape: torch.Size([50, 128]) 0.0\n",
      "pred_input_category shape: torch.Size([50, 128]) 0.0\n",
      "pred_input_topic shape: torch.Size([50]) 0.0\n",
      "pred_input_pageviews shape: torch.Size([50]) 0.0\n",
      "Targets shape: torch.Size([5]) , torch.float32 1.0\n",
      "impression id: 316436953\n"
     ]
    }
   ],
   "source": [
    "for idx in range(5):\n",
    "    sample = train_dataset[idx]\n",
    "    print(f\"Sample {idx}:\")\n",
    "    print(f\"his_input_title shape: {sample[0][0].shape}\")\n",
    "    print(f\"pred_input_title shape: {sample[0][1].shape} {sample[0][1].sum()}\")\n",
    "    print(f\"pred_input_category shape: {sample[0][2].shape} {sample[0][2].sum()}\")\n",
    "    print(f\"pred_input_topic shape: {sample[0][3].shape} {sample[0][3].sum()}\")\n",
    "    print(f\"pred_input_pageviews shape: {sample[0][4].shape} {sample[0][4].sum()}\")\n",
    "    print(f\"Targets shape: {sample[1].shape} , {sample[1].dtype} {sample[1].sum()}\")\n",
    "    print(f\"impression id: {sample[2]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import torch\n",
    "\n",
    "def collate_fn_with_global_padding(batch, max_len_pred, apply_padding_to_targets=True):\n",
    "    try:\n",
    "        # Unpack all features including timestamps from dataset samples\n",
    "        his_input_titles = [item[0][0] for item in batch]\n",
    "        his_category_emb = [item[0][1] for item in batch]\n",
    "        his_topic_emb = [item[0][2] for item in batch]\n",
    "        his_sentiment_scores = [item[0][3] for item in batch]\n",
    "        his_read_times = [item[0][4] for item in batch]\n",
    "        his_pageviews = [item[0][5] for item in batch]\n",
    "        his_timestamps = [item[0][6] for item in batch]\n",
    "\n",
    "        pred_input_titles = [item[0][7] for item in batch]\n",
    "        pred_category_emb = [item[0][8] for item in batch]\n",
    "        pred_topic_emb = [item[0][9] for item in batch]\n",
    "        pred_sentiment_scores = [item[0][10] for item in batch]\n",
    "        pred_read_times = [item[0][11] for item in batch]\n",
    "        pred_pageviews = [item[0][12] for item in batch]\n",
    "        pred_timestamps = [item[0][13] for item in batch]\n",
    "        impression_timestamps = [item[0][14] for item in batch]\n",
    "\n",
    "        batch_ys = [item[1] for item in batch]\n",
    "        impression_id = torch.tensor([item[2] for item in batch], dtype=torch.int64)\n",
    "\n",
    "        # Pad user history features\n",
    "        his_input_titles_padded = pad_sequence(his_input_titles, batch_first=True, padding_value=0)\n",
    "        his_category_emb_padded = pad_sequence(his_category_emb, batch_first=True, padding_value=0)\n",
    "        his_topic_emb_padded = pad_sequence(his_topic_emb, batch_first=True, padding_value=0)\n",
    "        his_sentiment_padded = pad_sequence(his_sentiment_scores, batch_first=True, padding_value=0)\n",
    "        his_read_times_padded = pad_sequence(his_read_times, batch_first=True, padding_value=0)\n",
    "        his_pageviews_padded = pad_sequence(his_pageviews, batch_first=True, padding_value=0)\n",
    "        his_timestamps_padded = pad_sequence(his_timestamps, batch_first=True, padding_value=0)\n",
    "\n",
    "        # Pad candidate features\n",
    "        pred_input_titles_padded = pad_sequence(pred_input_titles, batch_first=True, padding_value=0)\n",
    "        pred_category_emb_padded = pad_sequence(pred_category_emb, batch_first=True, padding_value=0)\n",
    "        pred_topic_emb_padded = pad_sequence(pred_topic_emb, batch_first=True, padding_value=0)\n",
    "        pred_sentiment_padded = pad_sequence(pred_sentiment_scores, batch_first=True, padding_value=0)\n",
    "        pred_read_times_padded = pad_sequence(pred_read_times, batch_first=True, padding_value=0)\n",
    "        pred_pageviews_padded = pad_sequence(pred_pageviews, batch_first=True, padding_value=0)\n",
    "        pred_timestamps_padded = pad_sequence(pred_timestamps, batch_first=True, padding_value=0)\n",
    "        \n",
    "        # Convert impression timestamps to tensor\n",
    "        impression_timestamps = torch.stack(impression_timestamps)\n",
    "\n",
    "        # Handle max_len_pred for candidate sequences\n",
    "        if pred_input_titles_padded.size(1) < max_len_pred:\n",
    "            pad_size = max_len_pred - pred_input_titles_padded.size(1)\n",
    "            \n",
    "            pred_input_titles_padded = torch.nn.functional.pad(pred_input_titles_padded, (0, 0, 0, pad_size), value=0)\n",
    "            pred_category_emb_padded = torch.nn.functional.pad(pred_category_emb_padded, (0, 0, 0, pad_size), value=0)\n",
    "            pred_topic_emb_padded = torch.nn.functional.pad(pred_topic_emb_padded, (0, 0, 0, pad_size), value=0)\n",
    "            pred_sentiment_padded = torch.nn.functional.pad(pred_sentiment_padded, (0, pad_size), value=0)\n",
    "            pred_read_times_padded = torch.nn.functional.pad(pred_read_times_padded, (0, pad_size), value=0)\n",
    "            pred_pageviews_padded = torch.nn.functional.pad(pred_pageviews_padded, (0, pad_size), value=0)\n",
    "            pred_timestamps_padded = torch.nn.functional.pad(pred_timestamps_padded, (0, pad_size), value=0)\n",
    "\n",
    "        elif pred_input_titles_padded.size(1) > max_len_pred:\n",
    "            pred_input_titles_padded = pred_input_titles_padded[:, :max_len_pred, :]\n",
    "            pred_category_emb_padded = pred_category_emb_padded[:, :max_len_pred, :]\n",
    "            pred_topic_emb_padded = pred_topic_emb_padded[:, :max_len_pred, :]\n",
    "            pred_sentiment_padded = pred_sentiment_padded[:, :max_len_pred]\n",
    "            pred_read_times_padded = pred_read_times_padded[:, :max_len_pred]\n",
    "            pred_pageviews_padded = pred_pageviews_padded[:, :max_len_pred]\n",
    "            pred_timestamps_padded = pred_timestamps_padded[:, :max_len_pred]\n",
    "\n",
    "        # Handle targets padding\n",
    "        if apply_padding_to_targets:\n",
    "            batch_ys_padded = pad_sequence(batch_ys, batch_first=True, padding_value=-1)\n",
    "            if batch_ys_padded.size(1) < max_len_pred:\n",
    "                pad_size = max_len_pred - batch_ys_padded.size(1)\n",
    "                batch_ys_padded = torch.nn.functional.pad(batch_ys_padded, (0, pad_size), value=-1)\n",
    "            elif batch_ys_padded.size(1) > max_len_pred:\n",
    "                batch_ys_padded = batch_ys_padded[:, :max_len_pred]\n",
    "\n",
    "            return (\n",
    "                his_input_titles_padded,\n",
    "                his_category_emb_padded,\n",
    "                his_topic_emb_padded, \n",
    "                his_sentiment_padded,\n",
    "                his_read_times_padded,\n",
    "                his_pageviews_padded,\n",
    "                his_timestamps_padded,\n",
    "                pred_input_titles_padded,\n",
    "                pred_category_emb_padded,\n",
    "                pred_topic_emb_padded,\n",
    "                pred_sentiment_padded,\n",
    "                pred_read_times_padded,\n",
    "                pred_pageviews_padded,\n",
    "                pred_timestamps_padded,\n",
    "                impression_timestamps\n",
    "            ), batch_ys_padded, impression_id\n",
    "        else:\n",
    "            return (\n",
    "                his_input_titles_padded,\n",
    "                his_category_emb_padded,\n",
    "                his_topic_emb_padded,\n",
    "                his_sentiment_padded,\n",
    "                his_read_times_padded,\n",
    "                his_pageviews_padded,\n",
    "                his_timestamps_padded,\n",
    "                pred_input_titles_padded,\n",
    "                pred_category_emb_padded,\n",
    "                pred_topic_emb_padded,\n",
    "                pred_sentiment_padded,\n",
    "                pred_read_times_padded,\n",
    "                pred_pageviews_padded,\n",
    "                pred_timestamps_padded,\n",
    "                impression_timestamps\n",
    "            ), batch_ys, impression_id\n",
    "    except Exception as e:\n",
    "        print(f\"Error in collate_fn: {e}\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wrap the dataset with DataLoader\n",
    "train_dataloader_temp = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=32,    # Set your desired batch size\n",
    "    shuffle=True,\n",
    "    num_workers=0,\n",
    "    collate_fn=lambda batch: collate_fn_with_global_padding(batch, max_inview_length_train)\n",
    ")\n",
    "\n",
    "val_dataloader_temp = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=64,    # Set your desired batch size\n",
    "    shuffle=False,\n",
    "    num_workers=0,\n",
    "    collate_fn=lambda batch: collate_fn_with_global_padding(batch, max_inview_length_validation)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "History features shapes:\n",
      "his_input_titles: torch.Size([64, 50, 300])\n",
      "his_category_emb: torch.Size([64, 50, 128])\n",
      "his_topic_emb: torch.Size([64, 50, 128])\n",
      "his_sentiment: torch.Size([64, 50])\n",
      "his_read_times: torch.Size([64, 50])\n",
      "his_pageviews: torch.Size([64, 50])\n",
      "his_timestamps: torch.Size([64, 50])\n",
      "\n",
      "Candidate features shapes:\n",
      "pred_input_titles: torch.Size([64, 89, 300])\n",
      "pred_category_emb: torch.Size([64, 89, 128])\n",
      "pred_topic_emb: torch.Size([64, 89, 128])\n",
      "pred_sentiment: torch.Size([64, 89])\n",
      "pred_read_times: torch.Size([64, 89])\n",
      "pred_pageviews: torch.Size([64, 89])\n",
      "pred_timestamps: torch.Size([64, 89])\n",
      "\n",
      "Other tensors:\n",
      "impression_timestamps: torch.Size([64])\n",
      "batch_ys: torch.Size([64, 89])\n",
      "impression_id: torch.Size([64])\n",
      "\n",
      "Batch loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "for batch in val_dataloader_temp:\n",
    "    (\n",
    "        his_input_titles_padded, \n",
    "        his_category_emb_padded,\n",
    "        his_topic_emb_padded,\n",
    "        his_sentiment_padded,\n",
    "        his_read_times_padded,\n",
    "        his_pageviews_padded,\n",
    "        his_timestamps_padded,\n",
    "        pred_input_titles_padded,\n",
    "        pred_category_emb_padded,\n",
    "        pred_topic_emb_padded,\n",
    "        pred_sentiment_padded,\n",
    "        pred_read_times_padded,\n",
    "        pred_pageviews_padded,\n",
    "        pred_timestamps_padded,\n",
    "        impression_timestamps\n",
    "    ), batch_ys_padded, impression_id = batch\n",
    "\n",
    "    # Original tensor shapes\n",
    "    print(\"History features shapes:\")\n",
    "    print(f\"his_input_titles: {his_input_titles_padded.shape}\")\n",
    "    print(f\"his_category_emb: {his_category_emb_padded.shape}\")\n",
    "    print(f\"his_topic_emb: {his_topic_emb_padded.shape}\")\n",
    "    print(f\"his_sentiment: {his_sentiment_padded.shape}\")\n",
    "    print(f\"his_read_times: {his_read_times_padded.shape}\")\n",
    "    print(f\"his_pageviews: {his_pageviews_padded.shape}\")\n",
    "    print(f\"his_timestamps: {his_timestamps_padded.shape}\")\n",
    "\n",
    "    print(\"\\nCandidate features shapes:\")\n",
    "    print(f\"pred_input_titles: {pred_input_titles_padded.shape}\")\n",
    "    print(f\"pred_category_emb: {pred_category_emb_padded.shape}\")\n",
    "    print(f\"pred_topic_emb: {pred_topic_emb_padded.shape}\")\n",
    "    print(f\"pred_sentiment: {pred_sentiment_padded.shape}\")\n",
    "    print(f\"pred_read_times: {pred_read_times_padded.shape}\")\n",
    "    print(f\"pred_pageviews: {pred_pageviews_padded.shape}\")\n",
    "    print(f\"pred_timestamps: {pred_timestamps_padded.shape}\")\n",
    "    \n",
    "    print(\"\\nOther tensors:\")\n",
    "    print(f\"impression_timestamps: {impression_timestamps.shape}\")\n",
    "    print(f\"batch_ys: {batch_ys_padded.shape}\")\n",
    "    print(f\"impression_id: {impression_id.shape}\")\n",
    "\n",
    "    print(\"\\nBatch loaded successfully!\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explanation of the shapes.\n",
    "\n",
    "1. `his_input_titles_padded: [128, 20, 300]`\n",
    "   - 128: batch size\n",
    "   - 20: max history length (user's reading history)\n",
    "   - 300: title embedding dimension from ConvBERT\n",
    "\n",
    "2. `pred_input_titles_padded: [128, 90, 300]`\n",
    "   - 128: batch size\n",
    "   - 90: max candidate articles to predict\n",
    "   - 300: title embedding dimension from ConvBERT\n",
    "\n",
    "3. `pred_category_emb_padded: [128, 90, 128]`\n",
    "   - 128: batch size\n",
    "   - 90: max candidate articles\n",
    "   - 128: category embedding dimension from ConvBERT\n",
    "\n",
    "4. `pred_topic_emb_padded: [128, 90, 128]`\n",
    "   - 128: batch size\n",
    "   - 90: max candidate articles\n",
    "   - 128: topic embedding dimension from ConvBERT\n",
    "\n",
    "5. `pred_sentiment_padded: [128, 90]`\n",
    "   - 128: batch size\n",
    "   - 90: max candidate articles\n",
    "   - Single sentiment score per article\n",
    "\n",
    "6. `pred_read_times_padded: [128, 90]`\n",
    "   - 128: batch size\n",
    "   - 90: max candidate articles\n",
    "   - Single read time value per article\n",
    "\n",
    "7. `pred_pageviews_padded: [128, 90]`\n",
    "   - 128: batch size\n",
    "   - 90: max candidate articles\n",
    "   - Single pageview count per article\n",
    "\n",
    "8. `batch_ys_padded: [128, 90]`\n",
    "   - 128: batch size\n",
    "   - 90: max candidate articles\n",
    "   - Binary labels (1 for clicked, 0 for not clicked)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create hyper-parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'__module__': 'models_pytorch.model_config', '__annotations__': {'title_size': <class 'int'>, 'embedding_dim': <class 'int'>, 'word_emb_dim': <class 'int'>, 'vocab_size': <class 'int'>, 'head_num': <class 'int'>, 'head_dim': <class 'int'>, 'attention_hidden_dim': <class 'int'>, 'optimizer': <class 'str'>, 'loss': <class 'str'>, 'dropout': <class 'float'>, 'learning_rate': <class 'float'>, 'weight_decay': <class 'float'>, 'units_per_layer': list[int], 'use_category': <class 'bool'>, 'use_topic': <class 'bool'>, 'use_numeric': <class 'bool'>, 'use_session_discount': <class 'bool'>, 'use_publication_discount': <class 'bool'>, 'doc_out_dim': <class 'int'>, 'cat_out_dim': <class 'int'>, 'top_out_dim': <class 'int'>, 'numeric_proj_dim': <class 'int'>}, 'title_size': 300, 'embedding_dim': 32, 'word_emb_dim': 8, 'vocab_size': 10000, 'head_num': 16, 'head_dim': 128, 'attention_hidden_dim': 128, 'hidden_dim': 4, 'optimizer': 'adam', 'loss': 'cross_entropy_loss', 'dropout': 0.3, 'learning_rate': 0.0001, 'weight_decay': 0.001, 'news_output_dim': 128, 'units_per_layer': [512, 256], 'use_category': True, 'use_topic': True, 'use_numeric': True, 'use_session_discount': True, 'use_publication_discount': True, 'doc_out_dim': 128, 'cat_out_dim': 128, 'top_out_dim': 128, 'numeric_proj_dim': 16, '__dict__': <attribute '__dict__' of 'hparams_nrms' objects>, '__weakref__': <attribute '__weakref__' of 'hparams_nrms' objects>, '__doc__': None}\n"
     ]
    }
   ],
   "source": [
    "# see the model parameters: \n",
    "hparams_nrms.attention_hidden_dim = 128\n",
    "hparams_nrms.title_size = 300\n",
    "hparams_nrms.head_num = 16\n",
    "hparams_nrms.head_dim = 128\n",
    "hparams_nrms.units_per_layer = [512,256]\n",
    "hparams_nrms.news_output_dim = 128\n",
    "hparams_nrms.dropout = 0.3\n",
    "hparams_nrms.learning_rate = 1e-4\n",
    "hparams_nrms.use_category = True\n",
    "hparams_nrms.use_topic = True\n",
    "hparams_nrms.use_numeric = True\n",
    "hparams_nrms.use_publication_discount = True\n",
    "hparams_nrms.use_session_discount = True\n",
    "print(hparams_nrms.__dict__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Best trial:\n",
    "  Value:  3.8567059058842696\n",
    "  Params: \n",
    "    head_num: 16\n",
    "    shared_dim: 99\n",
    "    dropout: 0.17498541169326048\n",
    "    learning_rate: 1.0454050068420554e-05\n",
    "    weight_decay: 0.006587407554797856\n",
    "    unit_layer_1: 421\n",
    "    unit_layer_2: 386\n",
    "    use_category: False\n",
    "    use_topic: False\n",
    "    use_numeric: True\n",
    "    use_publication_discount: False\n",
    "    use_session_discount: True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update hyperparameters with best trial results\n",
    "\n",
    "# Keep original parameters\n",
    "hparams_nrms.title_size = 300\n",
    "\n",
    "# Update with optimized parameters\n",
    "hparams_nrms.head_num = 4\n",
    "shared_dim = 140  # Rounded up 137 to nearest multiple of head_num\n",
    "hparams_nrms.head_dim = shared_dim\n",
    "hparams_nrms.attention_hidden_dim = shared_dim\n",
    "hparams_nrms.news_output_dim = shared_dim\n",
    "\n",
    "# Update layers\n",
    "hparams_nrms.units_per_layer = [257, 475]  # From optimization\n",
    "\n",
    "# Update learning parameters\n",
    "hparams_nrms.dropout = 0.208\n",
    "hparams_nrms.learning_rate = 8.486e-4\n",
    "hparams_nrms.weight_decay = 1.697e-3\n",
    "\n",
    "# Update feature flags based on optimization\n",
    "hparams_nrms.use_category = True\n",
    "hparams_nrms.use_topic = False\n",
    "hparams_nrms.use_numeric = False\n",
    "hparams_nrms.use_publication_discount = False\n",
    "hparams_nrms.use_session_discount = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update hyperparameters with best trial results\n",
    "\n",
    "# Keep original parameters\n",
    "hparams_nrms.title_size = 300\n",
    "\n",
    "# Update with optimized parameters\n",
    "hparams_nrms.head_num = 16\n",
    "shared_dim = 128  # Rounded up 137 to nearest multiple of head_num\n",
    "hparams_nrms.head_dim = shared_dim\n",
    "hparams_nrms.attention_hidden_dim = shared_dim\n",
    "hparams_nrms.news_output_dim = shared_dim\n",
    "\n",
    "# Update layers\n",
    "hparams_nrms.units_per_layer = [421, 386]  # From optimization\n",
    "\n",
    "# Update learning parameters\n",
    "hparams_nrms.dropout = 0.1749\n",
    "hparams_nrms.learning_rate = 1.0454050068420554e-5\n",
    "hparams_nrms.weight_decay = 0.006587407554797856\n",
    "\n",
    "# Update feature flags based on optimization\n",
    "hparams_nrms.use_category = True\n",
    "hparams_nrms.use_topic = True\n",
    "hparams_nrms.use_numeric = False\n",
    "hparams_nrms.use_publication_discount = True\n",
    "hparams_nrms.use_session_discount = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hyper optimization \n",
    "\n",
    "Best trial:\n",
    "  Value:  2.2587267407595015\n",
    "  Params: \n",
    "    head_num: 32\n",
    "    use_extra_layer: False\n",
    "    shared_dim: 197\n",
    "    unit_layer_1: 262\n",
    "    unit_layer_2: 84\n",
    "    dropout: 0.262803257540282\n",
    "    learning_rate: 0.00013197622102333815\n",
    "    weight_decay: 0.00016061208774452676"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "192\n"
     ]
    }
   ],
   "source": [
    "shared_dim = (197 // 32) * 32\n",
    "print(shared_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update hyperparameters with best trial results\n",
    "\n",
    "# Keep original parameters\n",
    "hparams_nrms.title_size = 300\n",
    "\n",
    "# Update with optimized parameters\n",
    "hparams_nrms.head_num = 32\n",
    "shared_dim = 192  # Rounded up 137 to nearest multiple of head_num\n",
    "hparams_nrms.head_dim = shared_dim\n",
    "hparams_nrms.attention_hidden_dim = shared_dim\n",
    "hparams_nrms.news_output_dim = shared_dim\n",
    "\n",
    "# Update layers\n",
    "hparams_nrms.units_per_layer = [262, 84]  # From optimization\n",
    "\n",
    "# Update learning parameters\n",
    "hparams_nrms.dropout = 0.262803257540282\n",
    "hparams_nrms.learning_rate = 1.0454050068420554e-5\n",
    "hparams_nrms.weight_decay = 0.00016061208774452676\n",
    "\n",
    "# Update feature flags based on optimization\n",
    "hparams_nrms.use_category = True\n",
    "hparams_nrms.use_topic = True\n",
    "hparams_nrms.use_numeric = False\n",
    "hparams_nrms.use_publication_discount = True\n",
    "hparams_nrms.use_session_discount = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the NRMS model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Model device: cuda:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\gusta\\Documents\\Deeplearning-RecSys-Challenge-2024\\models_pytorch\\NRMSDocVecModel.py:419: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  self.scaler = amp.GradScaler()\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "# Define paths\n",
    "MODEL_NAME = \"NRMS\"\n",
    "LOG_DIR = os.path.join(\"downloads\", \"runs\", MODEL_NAME)\n",
    "MODEL_WEIGHTS = os.path.join(\"downloads\", \"data\", \"state_dict\", MODEL_NAME, \"weights.pth\")\n",
    "\n",
    "# Create directories if they don't exist\n",
    "os.makedirs(LOG_DIR, exist_ok=True)\n",
    "os.makedirs(os.path.dirname(MODEL_WEIGHTS), exist_ok=True)\n",
    "\n",
    "# Define ModelCheckpoint class\n",
    "class ModelCheckpoint:\n",
    "    \"\"\"Saves the model after every epoch if it has the best performance so far.\"\"\"\n",
    "    def __init__(self, filepath, verbose=False, save_best_only=True):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            filepath (str): Path to save the model checkpoint.\n",
    "            verbose (bool): If True, prints a message when the model is saved.\n",
    "            save_best_only (bool): If True, saves only when the model is better than before.\n",
    "        \"\"\"\n",
    "        self.filepath = filepath\n",
    "        self.verbose = verbose\n",
    "        self.save_best_only = save_best_only\n",
    "        self.best_loss = None\n",
    "\n",
    "    def __call__(self, model, val_loss):\n",
    "        if self.best_loss is None:\n",
    "            self.best_loss = val_loss\n",
    "            self.save_checkpoint(model)\n",
    "        elif val_loss < self.best_loss:\n",
    "            self.best_loss = val_loss\n",
    "            self.save_checkpoint(model)\n",
    "\n",
    "    def save_checkpoint(self, model):\n",
    "        torch.save(model.state_dict(), self.filepath)\n",
    "        if self.verbose:\n",
    "            print(f\"Model saved to {self.filepath}\")\n",
    "\n",
    "# Define EarlyStopping class\n",
    "class EarlyStopping:\n",
    "    \"\"\"Early stops the training if validation loss doesn't improve by a given percentage over a patience period.\"\"\"\n",
    "    def __init__(self, patience=3, min_delta=0.05, verbose=False):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            patience (int): Number of epochs to wait after last time validation loss improved by min_delta.\n",
    "            min_delta (float): Minimum percentage improvement required to reset patience.\n",
    "            verbose (bool): If True, prints a message when early stopping is triggered.\n",
    "        \"\"\"\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta  # Minimum percentage improvement\n",
    "        self.verbose = verbose\n",
    "        self.counter = 0\n",
    "        self.best_loss = None\n",
    "        self.early_stop = False\n",
    "\n",
    "    def __call__(self, val_loss):\n",
    "        if self.best_loss is None:\n",
    "            # Initialize best_loss with the first validation loss\n",
    "            self.best_loss = val_loss\n",
    "            self.counter = 0\n",
    "        elif val_loss < self.best_loss * (1 - self.min_delta):\n",
    "            # Significant improvement found\n",
    "            self.best_loss = val_loss\n",
    "            self.counter = 0\n",
    "            if self.verbose:\n",
    "                print(f\"Validation loss improved by at least {self.min_delta*100:.1f}%\")\n",
    "        else:\n",
    "            # No significant improvement\n",
    "            self.counter += 1\n",
    "            if self.verbose:\n",
    "                print(f\"No significant improvement in validation loss. Counter: {self.counter}/{self.patience}\")\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "\n",
    "# Initialize TensorBoard SummaryWriter\n",
    "writer = SummaryWriter(log_dir=LOG_DIR)\n",
    "\n",
    "# Initialize callbacks\n",
    "model_checkpoint = ModelCheckpoint(filepath=MODEL_WEIGHTS, verbose=True, save_best_only=True)\n",
    "# Initialize EarlyStopping\n",
    "early_stopping = EarlyStopping(patience=3, min_delta=0.05, verbose=True)\n",
    "\n",
    "# Initialize your model\n",
    "# Ensure that NRMSModel is a PyTorch nn.Module\n",
    "\n",
    "# CUDA checks\n",
    "#print(f\"CUDA Available: {torch.cuda.is_available()}\")\n",
    "#print(f\"Current Device: {torch.cuda.current_device()}\")\n",
    "#print(f\"Device Name: {torch.cuda.get_device_name()}\")\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    \n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Initialize model\n",
    "# model = NRMSModel(\n",
    "#     hparams=hparams_nrms.__dict__,\n",
    "#     word2vec_embedding=word2vec_embedding,\n",
    "#     vocab_size=30000,\n",
    "#     word_emb_dim=8,\n",
    "#     device=device,\n",
    "#     feed_forward_layers_after_3rd_layer=False,\n",
    "# )\n",
    "\n",
    "model = NRMSDocVecModel(hparams=hparams_nrms.__dict__,\n",
    "                        device=device)\n",
    "\n",
    "# model = NRMSModel(hparams=hparams_nrms.__dict__,\n",
    "#                   word2vec_embedding=word2vec_embedding,\n",
    "#                         device=device)\n",
    "print(f\"Model device: {next(model.parameters()).device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NRMSDocVecModel(\n",
      "  (newsencoder): NewsEncoderDocVec(\n",
      "    (doc_encoder): DocEncoder(\n",
      "      (layer): Sequential(\n",
      "        (0): Linear(in_features=300, out_features=128, bias=True)\n",
      "        (1): ReLU()\n",
      "        (2): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (3): Dropout(p=0.262803257540282, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (category_encoder): CategoryEncoder(\n",
      "      (layer): Sequential(\n",
      "        (0): Linear(in_features=128, out_features=128, bias=True)\n",
      "        (1): ReLU()\n",
      "        (2): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (3): Dropout(p=0.262803257540282, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (topic_encoder): TopicEncoder(\n",
      "      (layer): Sequential(\n",
      "        (0): Linear(in_features=128, out_features=128, bias=True)\n",
      "        (1): ReLU()\n",
      "        (2): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (3): Dropout(p=0.262803257540282, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (feature_fusion): FeatureFusion(\n",
      "      (linears): ModuleList(\n",
      "        (0-2): 3 x Linear(in_features=128, out_features=192, bias=True)\n",
      "      )\n",
      "      (gates): ModuleList(\n",
      "        (0-2): 3 x Linear(in_features=128, out_features=192, bias=True)\n",
      "      )\n",
      "      (activation): Sigmoid()\n",
      "    )\n",
      "    (layer_norm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
      "    (time_discount): TimeDiscount(\n",
      "      (layer): Sequential(\n",
      "        (0): Linear(in_features=1, out_features=1, bias=True)\n",
      "        (1): Sigmoid()\n",
      "      )\n",
      "    )\n",
      "    (residual_blocks): ModuleList(\n",
      "      (0): Sequential(\n",
      "        (0): Linear(in_features=192, out_features=262, bias=True)\n",
      "        (1): ReLU()\n",
      "        (2): LayerNorm((262,), eps=1e-05, elementwise_affine=True)\n",
      "        (3): Dropout(p=0.262803257540282, inplace=False)\n",
      "        (4): Linear(in_features=262, out_features=192, bias=True)\n",
      "      )\n",
      "      (1): Sequential(\n",
      "        (0): Linear(in_features=192, out_features=84, bias=True)\n",
      "        (1): ReLU()\n",
      "        (2): LayerNorm((84,), eps=1e-05, elementwise_affine=True)\n",
      "        (3): Dropout(p=0.262803257540282, inplace=False)\n",
      "        (4): Linear(in_features=84, out_features=192, bias=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (userencoder): UserEncoderDocVec(\n",
      "    (titleencoder): NewsEncoderDocVec(\n",
      "      (doc_encoder): DocEncoder(\n",
      "        (layer): Sequential(\n",
      "          (0): Linear(in_features=300, out_features=128, bias=True)\n",
      "          (1): ReLU()\n",
      "          (2): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (3): Dropout(p=0.262803257540282, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (category_encoder): CategoryEncoder(\n",
      "        (layer): Sequential(\n",
      "          (0): Linear(in_features=128, out_features=128, bias=True)\n",
      "          (1): ReLU()\n",
      "          (2): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (3): Dropout(p=0.262803257540282, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (topic_encoder): TopicEncoder(\n",
      "        (layer): Sequential(\n",
      "          (0): Linear(in_features=128, out_features=128, bias=True)\n",
      "          (1): ReLU()\n",
      "          (2): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (3): Dropout(p=0.262803257540282, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (feature_fusion): FeatureFusion(\n",
      "        (linears): ModuleList(\n",
      "          (0-2): 3 x Linear(in_features=128, out_features=192, bias=True)\n",
      "        )\n",
      "        (gates): ModuleList(\n",
      "          (0-2): 3 x Linear(in_features=128, out_features=192, bias=True)\n",
      "        )\n",
      "        (activation): Sigmoid()\n",
      "      )\n",
      "      (layer_norm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
      "      (time_discount): TimeDiscount(\n",
      "        (layer): Sequential(\n",
      "          (0): Linear(in_features=1, out_features=1, bias=True)\n",
      "          (1): Sigmoid()\n",
      "        )\n",
      "      )\n",
      "      (residual_blocks): ModuleList(\n",
      "        (0): Sequential(\n",
      "          (0): Linear(in_features=192, out_features=262, bias=True)\n",
      "          (1): ReLU()\n",
      "          (2): LayerNorm((262,), eps=1e-05, elementwise_affine=True)\n",
      "          (3): Dropout(p=0.262803257540282, inplace=False)\n",
      "          (4): Linear(in_features=262, out_features=192, bias=True)\n",
      "        )\n",
      "        (1): Sequential(\n",
      "          (0): Linear(in_features=192, out_features=84, bias=True)\n",
      "          (1): ReLU()\n",
      "          (2): LayerNorm((84,), eps=1e-05, elementwise_affine=True)\n",
      "          (3): Dropout(p=0.262803257540282, inplace=False)\n",
      "          (4): Linear(in_features=84, out_features=192, bias=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (self_attention): SelfAttention(\n",
      "      (multihead_attention): MultiheadAttention(\n",
      "        (out_proj): NonDynamicallyQuantizableLinear(in_features=192, out_features=192, bias=True)\n",
      "      )\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (time_gate): TimeGate(\n",
      "      (gate_network): Sequential(\n",
      "        (0): Linear(in_features=193, out_features=192, bias=True)\n",
      "        (1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
      "        (2): ReLU()\n",
      "        (3): Linear(in_features=192, out_features=192, bias=True)\n",
      "        (4): Sigmoid()\n",
      "      )\n",
      "    )\n",
      "    (time_aware_attention): TimeAwareAttention(\n",
      "      (time_mlp): Sequential(\n",
      "        (0): Linear(in_features=1, out_features=192, bias=True)\n",
      "        (1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
      "        (2): ReLU()\n",
      "        (3): Linear(in_features=192, out_features=192, bias=True)\n",
      "        (4): Tanh()\n",
      "      )\n",
      "      (attention): Sequential(\n",
      "        (0): Linear(in_features=384, out_features=192, bias=True)\n",
      "        (1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
      "        (2): ReLU()\n",
      "        (3): Linear(in_features=192, out_features=1, bias=True)\n",
      "      )\n",
      "    )\n",
      "    (recency_enhancer): RecencyEnhancer()\n",
      "    (attention_layer): AttLayer2(\n",
      "      (V_w): Linear(in_features=192, out_features=192, bias=True)\n",
      "      (q_w): Linear(in_features=192, out_features=1, bias=False)\n",
      "    )\n",
      "    (user_projection): Linear(in_features=192, out_features=192, bias=True)\n",
      "    (layer_norm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
      "    (time_discount): TimeDiscount(\n",
      "      (layer): Sequential(\n",
      "        (0): Linear(in_features=1, out_features=1, bias=True)\n",
      "        (1): Sigmoid()\n",
      "      )\n",
      "    )\n",
      "  )\n",
      ")\n",
      "Layer: newsencoder.doc_encoder.layer.0.weight | Size: torch.Size([128, 300])\n",
      "Layer: newsencoder.doc_encoder.layer.0.bias | Size: torch.Size([128])\n",
      "Layer: newsencoder.doc_encoder.layer.2.weight | Size: torch.Size([128])\n",
      "Layer: newsencoder.doc_encoder.layer.2.bias | Size: torch.Size([128])\n",
      "Layer: newsencoder.category_encoder.layer.0.weight | Size: torch.Size([128, 128])\n",
      "Layer: newsencoder.category_encoder.layer.0.bias | Size: torch.Size([128])\n",
      "Layer: newsencoder.category_encoder.layer.2.weight | Size: torch.Size([128])\n",
      "Layer: newsencoder.category_encoder.layer.2.bias | Size: torch.Size([128])\n",
      "Layer: newsencoder.topic_encoder.layer.0.weight | Size: torch.Size([128, 128])\n",
      "Layer: newsencoder.topic_encoder.layer.0.bias | Size: torch.Size([128])\n",
      "Layer: newsencoder.topic_encoder.layer.2.weight | Size: torch.Size([128])\n",
      "Layer: newsencoder.topic_encoder.layer.2.bias | Size: torch.Size([128])\n",
      "Layer: newsencoder.feature_fusion.linears.0.weight | Size: torch.Size([192, 128])\n",
      "Layer: newsencoder.feature_fusion.linears.0.bias | Size: torch.Size([192])\n",
      "Layer: newsencoder.feature_fusion.linears.1.weight | Size: torch.Size([192, 128])\n",
      "Layer: newsencoder.feature_fusion.linears.1.bias | Size: torch.Size([192])\n",
      "Layer: newsencoder.feature_fusion.linears.2.weight | Size: torch.Size([192, 128])\n",
      "Layer: newsencoder.feature_fusion.linears.2.bias | Size: torch.Size([192])\n",
      "Layer: newsencoder.feature_fusion.gates.0.weight | Size: torch.Size([192, 128])\n",
      "Layer: newsencoder.feature_fusion.gates.0.bias | Size: torch.Size([192])\n",
      "Layer: newsencoder.feature_fusion.gates.1.weight | Size: torch.Size([192, 128])\n",
      "Layer: newsencoder.feature_fusion.gates.1.bias | Size: torch.Size([192])\n",
      "Layer: newsencoder.feature_fusion.gates.2.weight | Size: torch.Size([192, 128])\n",
      "Layer: newsencoder.feature_fusion.gates.2.bias | Size: torch.Size([192])\n",
      "Layer: newsencoder.layer_norm.weight | Size: torch.Size([192])\n",
      "Layer: newsencoder.layer_norm.bias | Size: torch.Size([192])\n",
      "Layer: newsencoder.time_discount.layer.0.weight | Size: torch.Size([1, 1])\n",
      "Layer: newsencoder.time_discount.layer.0.bias | Size: torch.Size([1])\n",
      "Layer: newsencoder.residual_blocks.0.0.weight | Size: torch.Size([262, 192])\n",
      "Layer: newsencoder.residual_blocks.0.0.bias | Size: torch.Size([262])\n",
      "Layer: newsencoder.residual_blocks.0.2.weight | Size: torch.Size([262])\n",
      "Layer: newsencoder.residual_blocks.0.2.bias | Size: torch.Size([262])\n",
      "Layer: newsencoder.residual_blocks.0.4.weight | Size: torch.Size([192, 262])\n",
      "Layer: newsencoder.residual_blocks.0.4.bias | Size: torch.Size([192])\n",
      "Layer: newsencoder.residual_blocks.1.0.weight | Size: torch.Size([84, 192])\n",
      "Layer: newsencoder.residual_blocks.1.0.bias | Size: torch.Size([84])\n",
      "Layer: newsencoder.residual_blocks.1.2.weight | Size: torch.Size([84])\n",
      "Layer: newsencoder.residual_blocks.1.2.bias | Size: torch.Size([84])\n",
      "Layer: newsencoder.residual_blocks.1.4.weight | Size: torch.Size([192, 84])\n",
      "Layer: newsencoder.residual_blocks.1.4.bias | Size: torch.Size([192])\n",
      "Layer: userencoder.self_attention.multihead_attention.in_proj_weight | Size: torch.Size([576, 192])\n",
      "Layer: userencoder.self_attention.multihead_attention.in_proj_bias | Size: torch.Size([576])\n",
      "Layer: userencoder.self_attention.multihead_attention.out_proj.weight | Size: torch.Size([192, 192])\n",
      "Layer: userencoder.self_attention.multihead_attention.out_proj.bias | Size: torch.Size([192])\n",
      "Layer: userencoder.time_gate.gate_network.0.weight | Size: torch.Size([192, 193])\n",
      "Layer: userencoder.time_gate.gate_network.0.bias | Size: torch.Size([192])\n",
      "Layer: userencoder.time_gate.gate_network.1.weight | Size: torch.Size([192])\n",
      "Layer: userencoder.time_gate.gate_network.1.bias | Size: torch.Size([192])\n",
      "Layer: userencoder.time_gate.gate_network.3.weight | Size: torch.Size([192, 192])\n",
      "Layer: userencoder.time_gate.gate_network.3.bias | Size: torch.Size([192])\n",
      "Layer: userencoder.time_aware_attention.time_mlp.0.weight | Size: torch.Size([192, 1])\n",
      "Layer: userencoder.time_aware_attention.time_mlp.0.bias | Size: torch.Size([192])\n",
      "Layer: userencoder.time_aware_attention.time_mlp.1.weight | Size: torch.Size([192])\n",
      "Layer: userencoder.time_aware_attention.time_mlp.1.bias | Size: torch.Size([192])\n",
      "Layer: userencoder.time_aware_attention.time_mlp.3.weight | Size: torch.Size([192, 192])\n",
      "Layer: userencoder.time_aware_attention.time_mlp.3.bias | Size: torch.Size([192])\n",
      "Layer: userencoder.time_aware_attention.attention.0.weight | Size: torch.Size([192, 384])\n",
      "Layer: userencoder.time_aware_attention.attention.0.bias | Size: torch.Size([192])\n",
      "Layer: userencoder.time_aware_attention.attention.1.weight | Size: torch.Size([192])\n",
      "Layer: userencoder.time_aware_attention.attention.1.bias | Size: torch.Size([192])\n",
      "Layer: userencoder.time_aware_attention.attention.3.weight | Size: torch.Size([1, 192])\n",
      "Layer: userencoder.time_aware_attention.attention.3.bias | Size: torch.Size([1])\n",
      "Layer: userencoder.attention_layer.V_w.weight | Size: torch.Size([192, 192])\n",
      "Layer: userencoder.attention_layer.V_w.bias | Size: torch.Size([192])\n",
      "Layer: userencoder.attention_layer.q_w.weight | Size: torch.Size([1, 192])\n",
      "Layer: userencoder.user_projection.weight | Size: torch.Size([192, 192])\n",
      "Layer: userencoder.user_projection.bias | Size: torch.Size([192])\n",
      "Layer: userencoder.layer_norm.weight | Size: torch.Size([192])\n",
      "Layer: userencoder.layer_norm.bias | Size: torch.Size([192])\n",
      "Layer: userencoder.time_discount.layer.0.weight | Size: torch.Size([1, 1])\n",
      "Layer: userencoder.time_discount.layer.0.bias | Size: torch.Size([1])\n",
      "\n",
      "Total parameters: 765,523\n"
     ]
    }
   ],
   "source": [
    "# 1. Print model architecture\n",
    "print(model)\n",
    "\n",
    "# 2. Print specific layer sizes\n",
    "for name, param in model.named_parameters():\n",
    "    print(f\"Layer: {name} | Size: {param.size()}\")\n",
    "\n",
    "# 3. Get total number of parameters\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"\\nTotal parameters: {total_params:,}\")\n",
    "\n",
    "# 4. Print layer by layer with shapes\n",
    "def print_model_structure(model):\n",
    "    print(\"\\nDetailed Model Structure:\")\n",
    "    for name, module in model.named_children():\n",
    "        print(f\"\\nLayer: {name}\")\n",
    "        print(f\"Type: {type(module).__name__}\")\n",
    "        if hasattr(module, 'weight'):\n",
    "            print(f\"Shape: {module.weight.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###\n",
    "Added gradient clipping to avoid exploiding gradients. The paramter MAX_GRAD_NORM is set to 5.0 as this is a common value used in transformers. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperoptimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import optuna\n",
    "\n",
    "# NUM_EPOCHS = 20\n",
    "# WEIGHT_DECAY = 1e-3\n",
    "\n",
    "# def objective(trial):\n",
    "#     # Define fixed valid head numbers\n",
    "#     possible_head_nums = [2, 4, 8, 16, 32]\n",
    "    \n",
    "#     # First suggest head_num from fixed choices\n",
    "#     head_num = trial.suggest_categorical('head_num', possible_head_nums)\n",
    "#     use_extra_layer = trial.suggest_categorical('use_extra_layer', [True, False])\n",
    "\n",
    "#     # Suggest shared_dim that's divisible by head_num\n",
    "#     min_dim = max(64, head_num)  # Ensure minimum dimension works with head_num\n",
    "#     max_dim = 256\n",
    "#     shared_dim = trial.suggest_int('shared_dim', min_dim, max_dim)\n",
    "    \n",
    "#     # Round shared_dim to nearest multiple of head_num\n",
    "#     shared_dim = (shared_dim // head_num) * head_num\n",
    "    \n",
    "#     # Skip if dimensions don't work\n",
    "#     if shared_dim < head_num:\n",
    "#         raise optuna.TrialPruned()\n",
    "    \n",
    "#     # Create base layers\n",
    "#     units_per_layer = [\n",
    "#         trial.suggest_int('unit_layer_1', 64, 512),\n",
    "#         trial.suggest_int('unit_layer_2', 64, 512),\n",
    "#     ]\n",
    "    \n",
    "#     # Conditionally add third layer\n",
    "#     if use_extra_layer:\n",
    "#         units_per_layer.append(\n",
    "#             trial.suggest_int('unit_layer_3', 64, 512)\n",
    "#         )\n",
    "    \n",
    "#     hparams = {\n",
    "#         'title_size': 300,\n",
    "#         'head_num': head_num,\n",
    "#         'head_dim': shared_dim,\n",
    "#         'attention_hidden_dim':shared_dim,\n",
    "#         'news_output_dim': shared_dim,\n",
    "#         'dropout': trial.suggest_float('dropout', 0.1, 0.5),\n",
    "#         'learning_rate': trial.suggest_float('learning_rate', 1e-6, 1e-3, log=True),\n",
    "#         'weight_decay': trial.suggest_float('weight_decay', 1e-4, 1e-2, log=True),\n",
    "#         'units_per_layer': [\n",
    "#             trial.suggest_int('unit_layer_1', 256, 512),\n",
    "#             trial.suggest_int('unit_layer_2', 256, 512),\n",
    "#         ],\n",
    "#         'use_category': True,\n",
    "#         'use_topic': True,\n",
    "#         'use_numeric':False,\n",
    "#         'use_publication_discount': True,\n",
    "#         'use_session_discount': True,\n",
    "#     }\n",
    "\n",
    "#     # Initialize model and training components\n",
    "#     model = NRMSDocVecModel(hparams=hparams, device=device)\n",
    "#     criterion = model.get_loss().to(device)\n",
    "#     optimizer = optim.Adam(model.parameters(), \n",
    "#                           lr=hparams['learning_rate'], \n",
    "#                           weight_decay=hparams['weight_decay'])\n",
    "    \n",
    "#     # Initialize EarlyStopping\n",
    "#     early_stopping = EarlyStopping(patience=4, min_delta=0.025, verbose=True)\n",
    "#     best_val_loss = float('inf')\n",
    "\n",
    "#     # Training loop\n",
    "#     for epoch in range(NUM_EPOCHS):\n",
    "#         # Training phase\n",
    "#         model.train()\n",
    "#         running_loss = 0.0\n",
    "#         train_batch_count = 0\n",
    "\n",
    "#         for batch in train_dataloader_temp:\n",
    "#             inputs, targets, impression_id = batch\n",
    "#             inputs = [inp.to(device) for inp in inputs]\n",
    "#             targets = targets.to(device)\n",
    "#             positive_indices = (targets == 1).nonzero(as_tuple=False)\n",
    "#             targets = positive_indices[:, 1].long()\n",
    "\n",
    "#             optimizer.zero_grad()\n",
    "#             outputs = model(*inputs)\n",
    "#             loss = criterion(outputs, targets)\n",
    "#             loss.backward()\n",
    "#             optimizer.step()\n",
    "\n",
    "#             running_loss += loss.item()\n",
    "#             train_batch_count += 1\n",
    "\n",
    "#         # Validation phase\n",
    "#         model.eval()\n",
    "#         val_loss = 0.0\n",
    "#         val_batch_count = 0\n",
    "        \n",
    "#         with torch.no_grad():\n",
    "#             for batch in val_dataloader_temp:\n",
    "#                 inputs, targets, impression_id = batch\n",
    "#                 inputs = [inp.to(device) for inp in inputs]\n",
    "#                 targets = targets.to(device)\n",
    "#                 positive_indices = (targets == 1).nonzero(as_tuple=False)\n",
    "#                 targets = positive_indices[:, 1].long()\n",
    "#                 outputs = model(*inputs)\n",
    "#                 loss = criterion(outputs, targets)\n",
    "#                 val_loss += loss.item()\n",
    "#                 val_batch_count += 1\n",
    "\n",
    "#         avg_val_loss = val_loss / val_batch_count if val_batch_count > 0 else float('inf')\n",
    "\n",
    "#         # Update best validation loss\n",
    "#         if avg_val_loss < best_val_loss:\n",
    "#             best_val_loss = avg_val_loss\n",
    "\n",
    "#         # Early stopping check\n",
    "#         early_stopping(avg_val_loss)\n",
    "#         if early_stopping.early_stop:\n",
    "#             break\n",
    "\n",
    "#         # Report to Optuna\n",
    "#         trial.report(avg_val_loss, epoch)\n",
    "#         if trial.should_prune():\n",
    "#             raise optuna.TrialPruned()\n",
    "\n",
    "#     return best_val_loss\n",
    "\n",
    "# # Create study and optimize\n",
    "# study = optuna.create_study(direction='minimize')\n",
    "# study.optimize(objective, n_trials=20)  # Increased from 3 to 50 trials\n",
    "\n",
    "# # Print results\n",
    "# print(\"Best trial:\")\n",
    "# trial = study.best_trial\n",
    "# print(\"  Value: \", trial.value)\n",
    "# print(\"  Params: \")\n",
    "# for key, value in trial.params.items():\n",
    "#     print(f\"    {key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Ensure you have defined or imported the EarlyStopping class\n",
    "class EarlyStopping:\n",
    "    def __init__(self, patience=3, min_delta=0.05, verbose=False):\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.verbose = verbose\n",
    "        self.counter = 0\n",
    "        self.best_score = None\n",
    "        self.early_stop = False\n",
    "\n",
    "    def __call__(self, current_score):\n",
    "        if self.best_score is None:\n",
    "            self.best_score = current_score\n",
    "        elif current_score < self.best_score - self.min_delta:\n",
    "            self.best_score = current_score\n",
    "            self.counter = 0\n",
    "        else:\n",
    "            self.counter += 1\n",
    "            if self.verbose:\n",
    "                print(f'EarlyStopping counter: {self.counter} out of {self.patience}')\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "\n",
    "# Define model_checkpoint function if not already defined\n",
    "def model_checkpoint(model, val_loss, epoch, path='model_checkpoint.pth'):\n",
    "    torch.save({\n",
    "        'epoch': epoch,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'val_loss': val_loss,\n",
    "    }, path)\n",
    "    print(f\"Model checkpoint saved at epoch {epoch} with validation loss: {val_loss:.4f}\")\n",
    "\n",
    "# Initialize components\n",
    "NUM_EPOCHS = 30\n",
    "early_stopping = EarlyStopping(patience=4, min_delta=0.025, verbose=True)\n",
    "criterion = nn.CrossEntropyLoss().to(device)  # Use appropriate loss function\n",
    "optimizer = optim.Adam(\n",
    "    model.parameters(),\n",
    "    lr=hparams_nrms.learning_rate,  # Ensure hparams_nrms is defined and has learning_rate\n",
    "    weight_decay=hparams_nrms.weight_decay  # Ensure hparams_nrms has weight_decay\n",
    ")\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer,\n",
    "    mode='min',\n",
    "    factor=0.1,\n",
    "    patience=5,\n",
    "    verbose=True\n",
    ")\n",
    "writer = SummaryWriter()\n",
    "\n",
    "train_losses, val_losses = [], []\n",
    "\n",
    "# Ensure your dataloaders are defined\n",
    "# train_dataloader_temp = ...\n",
    "# val_dataloader_temp = ...\n",
    "\n",
    "epoch_pbar = tqdm(range(1, NUM_EPOCHS + 1), desc=\"Training Progress\", dynamic_ncols=True)\n",
    "for epoch in epoch_pbar:\n",
    "    # Training Phase\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    train_batch_count = 0\n",
    "\n",
    "    for batch in train_dataloader_temp:\n",
    "        inputs, targets, _ = batch  # Assuming the third element is not used\n",
    "        inputs = [inp.to(device) for inp in inputs]\n",
    "        targets = targets.to(device)\n",
    "\n",
    "        # Extract positive indices\n",
    "        positive_indices = (targets == 1).nonzero(as_tuple=False)\n",
    "        if positive_indices.numel() == 0:\n",
    "            raise ValueError(\"No positive samples in the batch\")\n",
    "            continue  # Skip if no positive samples in the batch\n",
    "        targets = positive_indices[:, 1].long()\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(*inputs)  # Shape: (N, C)\n",
    "\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        # torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)  # Gradient clipping\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        train_batch_count += 1\n",
    "\n",
    "    avg_train_loss = running_loss / train_batch_count if train_batch_count > 0 else float('inf')\n",
    "    train_losses.append(avg_train_loss)\n",
    "\n",
    "    # Validation Phase\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    val_batch_count = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in val_dataloader_temp:\n",
    "            inputs, targets, _ = batch\n",
    "            inputs = [inp.to(device) for inp in inputs]\n",
    "            targets = targets.to(device)\n",
    "\n",
    "            positive_indices = (targets == 1).nonzero(as_tuple=False)\n",
    "            if positive_indices.numel() == 0:\n",
    "                continue\n",
    "            targets = positive_indices[:, 1].long()\n",
    "\n",
    "            outputs = model(*inputs)  # Shape: (N, C)\n",
    "            loss = criterion(outputs, targets)\n",
    "\n",
    "            val_loss += loss.item()\n",
    "            val_batch_count += 1\n",
    "\n",
    "    avg_val_loss = val_loss / val_batch_count if val_batch_count > 0 else float('inf')\n",
    "    val_losses.append(avg_val_loss)\n",
    "\n",
    "    # Logging\n",
    "    writer.add_scalar('Loss/Train', avg_train_loss, epoch)\n",
    "    writer.add_scalar('Loss/Validation', avg_val_loss, epoch)\n",
    "\n",
    "    # Update Progress Bar\n",
    "    epoch_pbar.set_postfix({\n",
    "        'train_loss': f'{avg_train_loss:.4f}',\n",
    "        'val_loss': f'{avg_val_loss:.4f}',\n",
    "    })\n",
    "\n",
    "    # Save Checkpoint\n",
    "    if epoch % 10 == 0:\n",
    "        model_checkpoint(model, avg_val_loss, epoch)\n",
    "\n",
    "    # Scheduler Step\n",
    "    scheduler.step(avg_val_loss)\n",
    "\n",
    "    # Early Stopping\n",
    "    early_stopping(avg_val_loss)\n",
    "    if early_stopping.early_stop:\n",
    "        print(\"Early stopping triggered. Stopping training.\")\n",
    "        break  # Exit the training loop\n",
    "\n",
    "writer.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the NRMS model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\gusta\\Documents\\Deeplearning-RecSys-Challenge-2024\\.venv\\Lib\\site-packages\\torch\\optim\\lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n",
      "Training Progress:   0%|          | 0/30 [00:00<?, ?it/s]c:\\Users\\gusta\\Documents\\Deeplearning-RecSys-Challenge-2024\\models_pytorch\\NRMSDocVecModel.py:444: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(enabled=False):\n",
      "Training Progress:  30%|███       | 9/30 [24:07<58:19, 166.64s/it, train_loss=1.8189, val_loss=2.5439]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EarlyStopping counter: 1 out of 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  33%|███▎      | 10/30 [27:02<56:20, 169.03s/it, train_loss=1.7597, val_loss=2.5197]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model checkpoint saved at epoch 10 with validation loss: 2.5197\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  40%|████      | 12/30 [33:22<54:12, 180.71s/it, train_loss=1.6857, val_loss=2.4506]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EarlyStopping counter: 1 out of 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  43%|████▎     | 13/30 [36:24<51:16, 180.95s/it, train_loss=1.6615, val_loss=2.4538]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EarlyStopping counter: 2 out of 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  43%|████▎     | 13/30 [39:22<51:28, 181.69s/it, train_loss=1.6615, val_loss=2.4538]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[33], line 99\u001b[0m\n\u001b[0;32m     96\u001b[0m val_batch_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m     98\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m---> 99\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mval_dataloader_temp\u001b[49m\u001b[43m:\u001b[49m\n\u001b[0;32m    100\u001b[0m \u001b[43m        \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtargets\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\n\u001b[0;32m    101\u001b[0m \u001b[43m        \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43minp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43minp\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m]\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\gusta\\Documents\\Deeplearning-RecSys-Challenge-2024\\.venv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:701\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    698\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    699\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    700\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 701\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    702\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    703\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m    704\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable\n\u001b[0;32m    705\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    706\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called\n\u001b[0;32m    707\u001b[0m ):\n",
      "File \u001b[1;32mc:\\Users\\gusta\\Documents\\Deeplearning-RecSys-Challenge-2024\\.venv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:757\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    755\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    756\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 757\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    758\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[0;32m    759\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32mc:\\Users\\gusta\\Documents\\Deeplearning-RecSys-Challenge-2024\\.venv\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:55\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n\u001b[1;32m---> 55\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollate_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[22], line 15\u001b[0m, in \u001b[0;36m<lambda>\u001b[1;34m(batch)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Wrap the dataset with DataLoader\u001b[39;00m\n\u001b[0;32m      2\u001b[0m train_dataloader_temp \u001b[38;5;241m=\u001b[39m DataLoader(\n\u001b[0;32m      3\u001b[0m     train_dataset,\n\u001b[0;32m      4\u001b[0m     batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m32\u001b[39m,    \u001b[38;5;66;03m# Set your desired batch size\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m      7\u001b[0m     collate_fn\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mlambda\u001b[39;00m batch: collate_fn_with_global_padding(batch, max_inview_length_train)\n\u001b[0;32m      8\u001b[0m )\n\u001b[0;32m     10\u001b[0m val_dataloader_temp \u001b[38;5;241m=\u001b[39m DataLoader(\n\u001b[0;32m     11\u001b[0m     val_dataset,\n\u001b[0;32m     12\u001b[0m     batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m64\u001b[39m,    \u001b[38;5;66;03m# Set your desired batch size\u001b[39;00m\n\u001b[0;32m     13\u001b[0m     shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m     14\u001b[0m     num_workers\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m,\n\u001b[1;32m---> 15\u001b[0m     collate_fn\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mlambda\u001b[39;00m batch: \u001b[43mcollate_fn_with_global_padding\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_inview_length_validation\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     16\u001b[0m )\n",
      "Cell \u001b[1;32mIn[21], line 28\u001b[0m, in \u001b[0;36mcollate_fn_with_global_padding\u001b[1;34m(batch, max_len_pred, apply_padding_to_targets)\u001b[0m\n\u001b[0;32m     25\u001b[0m impression_id \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor([item[\u001b[38;5;241m2\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m batch], dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mint64)\n\u001b[0;32m     27\u001b[0m \u001b[38;5;66;03m# Pad user history features\u001b[39;00m\n\u001b[1;32m---> 28\u001b[0m his_input_titles_padded \u001b[38;5;241m=\u001b[39m \u001b[43mpad_sequence\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhis_input_titles\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_first\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     29\u001b[0m his_category_emb_padded \u001b[38;5;241m=\u001b[39m pad_sequence(his_category_emb, batch_first\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, padding_value\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m     30\u001b[0m his_topic_emb_padded \u001b[38;5;241m=\u001b[39m pad_sequence(his_topic_emb, batch_first\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, padding_value\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\gusta\\Documents\\Deeplearning-RecSys-Challenge-2024\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\rnn.py:478\u001b[0m, in \u001b[0;36mpad_sequence\u001b[1;34m(sequences, batch_first, padding_value, padding_side)\u001b[0m\n\u001b[0;32m    474\u001b[0m         sequences \u001b[38;5;241m=\u001b[39m sequences\u001b[38;5;241m.\u001b[39munbind(\u001b[38;5;241m0\u001b[39m)  \u001b[38;5;66;03m# type: ignore[assignment]\u001b[39;00m\n\u001b[0;32m    476\u001b[0m \u001b[38;5;66;03m# assuming trailing dimensions and type of all the Tensors\u001b[39;00m\n\u001b[0;32m    477\u001b[0m \u001b[38;5;66;03m# in sequences are same and fetching those from sequences[0]\u001b[39;00m\n\u001b[1;32m--> 478\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_nn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpad_sequence\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    479\u001b[0m \u001b[43m    \u001b[49m\u001b[43msequences\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_first\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding_value\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding_side\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[arg-type]\u001b[39;49;00m\n\u001b[0;32m    480\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Ensure you have defined or imported the EarlyStopping class\n",
    "class EarlyStopping:\n",
    "    def __init__(self, patience=3, min_delta=0.05, verbose=False):\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.verbose = verbose\n",
    "        self.counter = 0\n",
    "        self.best_score = None\n",
    "        self.early_stop = False\n",
    "\n",
    "    def __call__(self, current_score):\n",
    "        if self.best_score is None:\n",
    "            self.best_score = current_score\n",
    "        elif current_score < self.best_score - self.min_delta:\n",
    "            self.best_score = current_score\n",
    "            self.counter = 0\n",
    "        else:\n",
    "            self.counter += 1\n",
    "            if self.verbose:\n",
    "                print(f'EarlyStopping counter: {self.counter} out of {self.patience}')\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "\n",
    "# Define model_checkpoint function if not already defined\n",
    "def model_checkpoint(model, val_loss, epoch, path='model_checkpoint.pth'):\n",
    "    torch.save({\n",
    "        'epoch': epoch,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'val_loss': val_loss,\n",
    "    }, path)\n",
    "    print(f\"Model checkpoint saved at epoch {epoch} with validation loss: {val_loss:.4f}\")\n",
    "\n",
    "# Initialize components\n",
    "NUM_EPOCHS = 30\n",
    "early_stopping = EarlyStopping(patience=4, min_delta=0.025, verbose=True)\n",
    "criterion = nn.CrossEntropyLoss().to(device)  # Use appropriate loss function\n",
    "optimizer = optim.Adam(\n",
    "    model.parameters(),\n",
    "    lr=hparams_nrms.learning_rate,  # Ensure hparams_nrms is defined and has learning_rate\n",
    "    weight_decay=hparams_nrms.weight_decay  # Ensure hparams_nrms has weight_decay\n",
    ")\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer,\n",
    "    mode='min',\n",
    "    factor=0.1,\n",
    "    patience=5,\n",
    "    verbose=True\n",
    ")\n",
    "writer = SummaryWriter()\n",
    "\n",
    "train_losses, val_losses = [], []\n",
    "\n",
    "\n",
    "epoch_pbar = tqdm(range(1, NUM_EPOCHS + 1), desc=\"Training Progress\", dynamic_ncols=True)\n",
    "for epoch in epoch_pbar:\n",
    "    # Training Phase\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    train_batch_count = 0\n",
    "\n",
    "    for batch in train_dataloader_temp:\n",
    "        inputs, targets, _ = batch  # Assuming the third element is not used\n",
    "        inputs = [inp.to(device) for inp in inputs]\n",
    "        targets = targets.to(device)\n",
    "\n",
    "        # Extract positive indices\n",
    "        positive_indices = (targets == 1).nonzero(as_tuple=False)\n",
    "        if positive_indices.numel() == 0:\n",
    "            raise ValueError(\"No positive samples in the batch\")\n",
    "            continue  # Skip if no positive samples in the batch\n",
    "        targets = positive_indices[:, 1].long()\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(*inputs)  # Shape: (N, C)\n",
    "\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        # torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)  # Gradient clipping\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        train_batch_count += 1\n",
    "\n",
    "    avg_train_loss = running_loss / train_batch_count if train_batch_count > 0 else float('inf')\n",
    "    train_losses.append(avg_train_loss)\n",
    "\n",
    "    # Validation Phase\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    val_batch_count = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in val_dataloader_temp:\n",
    "            inputs, targets, _ = batch\n",
    "            inputs = [inp.to(device) for inp in inputs]\n",
    "            targets = targets.to(device)\n",
    "\n",
    "            positive_indices = (targets == 1).nonzero(as_tuple=False)\n",
    "            if positive_indices.numel() == 0:\n",
    "                continue\n",
    "            targets = positive_indices[:, 1].long()\n",
    "\n",
    "            outputs = model(*inputs)  # Shape: (N, C)\n",
    "            loss = criterion(outputs, targets)\n",
    "\n",
    "            val_loss += loss.item()\n",
    "            val_batch_count += 1\n",
    "\n",
    "    avg_val_loss = val_loss / val_batch_count if val_batch_count > 0 else float('inf')\n",
    "    val_losses.append(avg_val_loss)\n",
    "\n",
    "    # Logging\n",
    "    writer.add_scalar('Loss/Train', avg_train_loss, epoch)\n",
    "    writer.add_scalar('Loss/Validation', avg_val_loss, epoch)\n",
    "\n",
    "    # Update Progress Bar\n",
    "    epoch_pbar.set_postfix({\n",
    "        'train_loss': f'{avg_train_loss:.4f}',\n",
    "        'val_loss': f'{avg_val_loss:.4f}',\n",
    "    })\n",
    "\n",
    "    # Save Checkpoint\n",
    "    if epoch % 10 == 0:\n",
    "        model_checkpoint(model, avg_val_loss, epoch)\n",
    "\n",
    "    # Scheduler Step\n",
    "    scheduler.step(avg_val_loss)\n",
    "\n",
    "    # Early Stopping\n",
    "    early_stopping(avg_val_loss)\n",
    "    if early_stopping.early_stop:\n",
    "        print(\"Early stopping triggered. Stopping training.\")\n",
    "        break  # Exit the training loop\n",
    "\n",
    "writer.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Summary:\n",
      "Initial Training Loss: 9.7917\n",
      "Final Training Loss: 1.6243\n",
      "Best Training Loss: 1.6243\n",
      "\n",
      "Initial Validation Loss: 8.0197\n",
      "Final Validation Loss: 2.4538\n",
      "Best Validation Loss: 2.4506\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Plot the losses\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(range(1, len(train_losses) + 1), train_losses, label='Training Loss')\n",
    "plt.plot(range(1, len(val_losses) + 1), val_losses, label='Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training and Validation Losses')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.savefig(\"nrms_training_loss.png\")\n",
    "plt.show()\n",
    "\n",
    "# Print final statistics\n",
    "print(\"\\nTraining Summary:\")\n",
    "print(f\"Initial Training Loss: {train_losses[0]:.4f}\")\n",
    "print(f\"Final Training Loss: {train_losses[-1]:.4f}\")\n",
    "print(f\"Best Training Loss: {min(train_losses):.4f}\")\n",
    "print(f\"\\nInitial Validation Loss: {val_losses[0]:.4f}\")\n",
    "print(f\"Final Validation Loss: {val_losses[-1]:.4f}\")\n",
    "print(f\"Best Validation Loss: {min(val_losses):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FRACTION: 0.2, HISTORY_SIZE: 50\n",
      "Hyperparameters:\n",
      "title_size: 300\n",
      "embedding_dim: 32\n",
      "word_emb_dim: 8\n",
      "vocab_size: 10000\n",
      "head_num: 32\n",
      "head_dim: 192\n",
      "attention_hidden_dim: 192\n",
      "hidden_dim: 4\n",
      "optimizer: adam\n",
      "loss: cross_entropy_loss\n",
      "dropout: 0.262803257540282\n",
      "learning_rate: 1.0454050068420554e-05\n",
      "weight_decay: 0.00016061208774452676\n",
      "news_output_dim: 192\n",
      "units_per_layer: [262, 84]\n",
      "use_category: True\n",
      "use_topic: True\n",
      "use_numeric: False\n",
      "use_session_discount: True\n",
      "use_publication_discount: True\n",
      "doc_out_dim: 128\n",
      "cat_out_dim: 128\n",
      "top_out_dim: 128\n",
      "numeric_proj_dim: 16\n"
     ]
    }
   ],
   "source": [
    "print(f\"FRACTION: {FRACTION}, HISTORY_SIZE: {HISTORY_SIZE}\")\n",
    "\n",
    "# Filter out special Python attributes and print parameters\n",
    "params = {k: v for k, v in hparams_nrms.__dict__.items() if not k.startswith('__')}\n",
    "print(\"Hyperparameters:\")\n",
    "for key, value in params.items():\n",
    "    print(f\"{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_length_of_labels(df: pl.DataFrame, impression_id: int) -> int:\n",
    "    # Filter for matching impression_id\n",
    "    filtered = df.filter(pl.col('impression_id') == impression_id)\n",
    "    \n",
    "    if filtered.height == 0:\n",
    "        raise ValueError(f\"No row found for impression_id {impression_id}\")\n",
    "    \n",
    "    # Get labels from first row\n",
    "    labels = filtered.select('labels').row(0)[0]\n",
    "    \n",
    "    return len(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get_length_of_labels(df_validation, 349992000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label 0: [0, 0, 0, 0, 0, 0, 1, 0, 0]\n",
      "Label 1: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]\n",
      "Label 2: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]\n",
      "Label 3: [0, 0, 1, 0, 0]\n",
      "Label 4: [0, 0, 0, 1, 0]\n"
     ]
    }
   ],
   "source": [
    "# Get first 5 labels\n",
    "first_5_labels = df_validation.select('labels').head(5)\n",
    "\n",
    "# Print each label with index\n",
    "for i, row in enumerate(first_5_labels.iter_rows()):\n",
    "    print(f\"Label {i}: {row[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipped 0 samples with only one class\n",
      "Remaining valid samples: 48639\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "# Initialize lists\n",
    "all_predictions = []\n",
    "all_labels = []\n",
    "skipped_samples = 0\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for batch in val_dataloader_temp:\n",
    "        inputs, targets, impression_ids = batch\n",
    "        \n",
    "        # Move to device\n",
    "        inputs = [inp.to(device) for inp in inputs]\n",
    "        \n",
    "        # Get predictions\n",
    "        predictions = model.predict(*inputs)\n",
    "        \n",
    "        # Convert to probabilities if needed\n",
    "        if not torch.is_floating_point(predictions):\n",
    "            predictions = torch.sigmoid(predictions)\n",
    "        \n",
    "        # Convert to lists while preserving structure\n",
    "        batch_preds = predictions.cpu().numpy().tolist()\n",
    "        batch_labels = targets.cpu().numpy().tolist()\n",
    "        impression_ids = impression_ids.cpu().numpy().tolist()\n",
    "        \n",
    "        batch_preds_without_padding = []\n",
    "        batch_labels_without_padding = []\n",
    "        \n",
    "        for pred_sample, label_sample, impression_id_sample in zip(batch_preds, batch_labels, impression_ids):\n",
    "            # Remove padding\n",
    "            actual_length = get_length_of_labels(df_validation, impression_id_sample)\n",
    "            pred_sample = pred_sample[:actual_length]\n",
    "            \n",
    "            # Check if sample has both classes before adding\n",
    "            if 1 in label_sample[:actual_length] and 0 in label_sample[:actual_length]:\n",
    "                batch_preds_without_padding.append(pred_sample)\n",
    "                batch_labels_without_padding.append(label_sample[:actual_length])\n",
    "            else:\n",
    "                skipped_samples += 1\n",
    "        \n",
    "        # Add batch predictions and labels\n",
    "        all_predictions.extend(batch_preds_without_padding)\n",
    "        all_labels.extend(batch_labels_without_padding)\n",
    "\n",
    "print(f\"Skipped {skipped_samples} samples with only one class\")\n",
    "print(f\"Remaining valid samples: {len(all_predictions)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.1764\n"
     ]
    }
   ],
   "source": [
    "# Initialize counters\n",
    "correct_predictions = 0\n",
    "total_samples = len(all_predictions)\n",
    "\n",
    "# Iterate over samples\n",
    "for idx, _ in enumerate(all_predictions):\n",
    "    # print(f\"Sample {idx}: Prediction: {all_predictions[idx]}\")\n",
    "    # print(f\"Sample {idx}: Label:      {all_labels[idx]}\")\n",
    "    \n",
    "    # Extract index of maximum value in predictions and labels\n",
    "    pred_max_index = np.argmax(all_predictions[idx])\n",
    "    label_max_index = np.argmax(all_labels[idx])\n",
    "    \n",
    "    # Compare indices to determine if the prediction was correct\n",
    "    if pred_max_index == label_max_index:\n",
    "        # print(f\"Sample {idx}: Prediction was correct.\")\n",
    "        correct_predictions += 1\n",
    "  #  else:\n",
    "        # print(f\"Sample {idx}: Prediction was wrong.\")\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = correct_predictions / total_samples\n",
    "print(f\"Accuracy: {accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Mean AUC: 0.6026\n",
      "Number of valid AUC calculations: 48639\n"
     ]
    }
   ],
   "source": [
    "from evaluation import AucScore\n",
    "\n",
    "# auc_score = AucScore()\n",
    "\n",
    "# auc_score.calculate(all_predictions, all_labels)\n",
    "# print(f\"AUC: {auc_score.score}\")\n",
    "# # Calculate AUC per sample\n",
    "aucs = []\n",
    "for preds, labels in zip(all_predictions, all_labels):\n",
    "    try:\n",
    "        # Only calculate if we have both positive and negative samples\n",
    "        if sum(labels) > 0 and sum(labels) < len(labels):\n",
    "            auc = roc_auc_score(labels, preds)\n",
    "            aucs.append(auc)\n",
    "    except ValueError:\n",
    "        raise ValueError(\"Only one class present in labels. Cannot calculate AUC.\")\n",
    "\n",
    "print(f\"\\nMean AUC: {np.mean(aucs):.4f}\")\n",
    "print(f\"Number of valid AUC calculations: {len(aucs)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Output Prediction File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "https://github.com/recommenders-team/recommenders/blob/main/examples/00_quick_start/nrms_MIND.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
