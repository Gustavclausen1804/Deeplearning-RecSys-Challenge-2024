{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Global settings and imports"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      "Python version: 3.12.1\n",
      "PyTorch version: 2.5.1+cu121\n",
      "CUDA available: True\n",
      "Current GPU device: NVIDIA GeForce RTX 3070\n",
      "Number of GPUs available: 1\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import sys\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import polars as pl\n",
    "import matplotlib.pyplot as plt\n",
    "#import optuna\n",
    "from tqdm import tqdm\n",
    "import torch.optim as optim\n",
    "\n",
    "from utils._constants import (\n",
    "    DEFAULT_HISTORY_ARTICLE_ID_COL,\n",
    "    DEFAULT_CLICKED_ARTICLES_COL,\n",
    "    DEFAULT_INVIEW_ARTICLES_COL,\n",
    "    DEFAULT_IMPRESSION_ID_COL,\n",
    "    DEFAULT_IMPRESSION_TIMESTAMP_COL,\n",
    "    DEFAULT_SUBTITLE_COL,\n",
    "    DEFAULT_LABELS_COL,\n",
    "    DEFAULT_TITLE_COL,\n",
    "    DEFAULT_USER_COL,\n",
    "    DEFAULT_ARTICLE_PUBLISHED_TIMESTAMP_COL,\n",
    "    DEFAULT_CATEGORY_STR_COL,\n",
    "    DEFAULT_TOPICS_COL,\n",
    "    DEFAULT_CATEGORY_STR_COL,\n",
    "    DEFAULT_TOPICS_COL,\n",
    "    DEFAULT_TOTAL_PAGEVIEWS_COL,\n",
    "    DEFAULT_TOTAL_READ_TIME_COL,\n",
    "    DEFAULT_SENTIMENT_SCORE_COL,\n",
    ")\n",
    "\n",
    "from utils._behaviors import (\n",
    "    create_binary_labels_column,\n",
    "    sampling_strategy_wu2019,\n",
    "    add_known_user_column,\n",
    "    add_prediction_scores,\n",
    "    truncate_history,\n",
    ")\n",
    "from evaluation import MetricEvaluator, AucScore, NdcgScore, MrrScore\n",
    "from utils._articles import convert_text2encoding_with_transformers\n",
    "from utils._polars import concat_str_columns, slice_join_dataframes\n",
    "from utils._articles import create_article_id_to_value_mapping\n",
    "from utils._nlp import get_transformers_word_embeddings, generate_embeddings_with_transformers\n",
    "from utils._python import write_submission_file, rank_predictions_by_score\n",
    "from models_pytorch.model_config import hparams_nrms\n",
    "\n",
    "from models_pytorch.nrms import NRMSModel\n",
    "from models_pytorch.NRMSDocVecModel import NRMSDocVecModel\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from models_pytorch.dataloader import NRMSDataSet\n",
    "\n",
    "# Check Python version\n",
    "print(f\"Python version: {sys.version.split()[0]}\")\n",
    "\n",
    "# Check PyTorch version\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "\n",
    "# Check GPU availability\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"Current GPU device: {torch.cuda.get_device_name()}\")\n",
    "    print(f\"Number of GPUs available: {torch.cuda.device_count()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Looking at behaviours and history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download and load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ebnerd_small\n"
     ]
    }
   ],
   "source": [
    "PATH = Path(\"./ebnerd_small\")  # Base path for your data directory\n",
    "print(PATH)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (2, 7)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>user_id</th><th>article_id_fixed</th><th>article_ids_inview</th><th>article_ids_clicked</th><th>impression_id</th><th>impression_time</th><th>labels</th></tr><tr><td>u32</td><td>list[i32]</td><td>list[i64]</td><td>list[i64]</td><td>u32</td><td>datetime[μs]</td><td>list[i8]</td></tr></thead><tbody><tr><td>366197</td><td>[9770327, 9769433, … 9769575]</td><td>[9777319, 9777292, … 9777992]</td><td>[9777804]</td><td>111436368</td><td>2023-05-23 15:08:57</td><td>[0, 0, … 0]</td></tr><tr><td>2238620</td><td>[9766599, 9768451, … 9770425]</td><td>[9777910, 9777769, … 9779747]</td><td>[9777769]</td><td>245249797</td><td>2023-05-24 20:29:16</td><td>[0, 1, … 0]</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (2, 7)\n",
       "┌─────────┬──────────────┬──────────────┬──────────────┬──────────────┬──────────────┬─────────────┐\n",
       "│ user_id ┆ article_id_f ┆ article_ids_ ┆ article_ids_ ┆ impression_i ┆ impression_t ┆ labels      │\n",
       "│ ---     ┆ ixed         ┆ inview       ┆ clicked      ┆ d            ┆ ime          ┆ ---         │\n",
       "│ u32     ┆ ---          ┆ ---          ┆ ---          ┆ ---          ┆ ---          ┆ list[i8]    │\n",
       "│         ┆ list[i32]    ┆ list[i64]    ┆ list[i64]    ┆ u32          ┆ datetime[μs] ┆             │\n",
       "╞═════════╪══════════════╪══════════════╪══════════════╪══════════════╪══════════════╪═════════════╡\n",
       "│ 366197  ┆ [9770327,    ┆ [9777319,    ┆ [9777804]    ┆ 111436368    ┆ 2023-05-23   ┆ [0, 0, … 0] │\n",
       "│         ┆ 9769433, …   ┆ 9777292, …   ┆              ┆              ┆ 15:08:57     ┆             │\n",
       "│         ┆ 9769575]     ┆ 9777992]     ┆              ┆              ┆              ┆             │\n",
       "│ 2238620 ┆ [9766599,    ┆ [9777910,    ┆ [9777769]    ┆ 245249797    ┆ 2023-05-24   ┆ [0, 1, … 0] │\n",
       "│         ┆ 9768451, …   ┆ 9777769, …   ┆              ┆              ┆ 20:29:16     ┆             │\n",
       "│         ┆ 9770425]     ┆ 9779747]     ┆              ┆              ┆              ┆             │\n",
       "└─────────┴──────────────┴──────────────┴──────────────┴──────────────┴──────────────┴─────────────┘"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def ebnerd_from_path(path: Path, history_size: int = 30) -> pl.DataFrame:\n",
    "    \"\"\"\n",
    "    Load ebnerd - function\n",
    "    \"\"\"\n",
    "    df_history = (\n",
    "        pl.scan_parquet(path.joinpath(\"history.parquet\"))\n",
    "        .select(DEFAULT_USER_COL, DEFAULT_HISTORY_ARTICLE_ID_COL)\n",
    "        .pipe(\n",
    "            truncate_history,\n",
    "            column=DEFAULT_HISTORY_ARTICLE_ID_COL,\n",
    "            history_size=history_size,\n",
    "            padding_value=0,\n",
    "            enable_warning=False,\n",
    "        )\n",
    "    )\n",
    "    df_behaviors = (\n",
    "        pl.scan_parquet(path.joinpath(\"behaviors.parquet\"))\n",
    "        .collect()\n",
    "        .pipe(\n",
    "            slice_join_dataframes,\n",
    "            df2=df_history.collect(),\n",
    "            on=DEFAULT_USER_COL,\n",
    "            how=\"left\",\n",
    "        )\n",
    "    )\n",
    "    return df_behaviors\n",
    "\n",
    "COLUMNS = [\n",
    "    DEFAULT_USER_COL,\n",
    "    DEFAULT_HISTORY_ARTICLE_ID_COL,\n",
    "    DEFAULT_INVIEW_ARTICLES_COL,\n",
    "    DEFAULT_CLICKED_ARTICLES_COL,\n",
    "    DEFAULT_IMPRESSION_ID_COL,\n",
    "    DEFAULT_IMPRESSION_TIMESTAMP_COL,\n",
    "]\n",
    "HISTORY_SIZE = 20 # TODO: History size. \n",
    "FRACTION = 0.1\n",
    "\n",
    "df_train = (\n",
    "    ebnerd_from_path(PATH.joinpath(\"train\"), history_size=HISTORY_SIZE)\n",
    "    .select(COLUMNS)\n",
    "    .pipe(\n",
    "        sampling_strategy_wu2019,\n",
    "        npratio=4,\n",
    "        shuffle=True,\n",
    "        with_replacement=True,\n",
    "        seed=123,\n",
    "    )\n",
    "    .pipe(create_binary_labels_column)\n",
    "    .sample(fraction=FRACTION)\n",
    ")\n",
    "# =>\n",
    "df_validation = (\n",
    "    ebnerd_from_path(PATH.joinpath(\"validation\"), history_size=HISTORY_SIZE)\n",
    "    .select(COLUMNS)\n",
    "    .pipe(create_binary_labels_column)\n",
    "    .sample(fraction=FRACTION)\n",
    ")\n",
    "df_train.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = df_train.with_columns([\n",
    "    (\n",
    "        (pl.col(DEFAULT_IMPRESSION_TIMESTAMP_COL)\n",
    "        .cast(pl.Datetime)\n",
    "        .dt.epoch(time_unit='s') / 3600)\n",
    "        .cast(pl.Int64)\n",
    "    ).alias(DEFAULT_IMPRESSION_TIMESTAMP_COL)\n",
    "])\n",
    "\n",
    "df_validation = df_validation.with_columns([\n",
    "    (\n",
    "        (pl.col(DEFAULT_IMPRESSION_TIMESTAMP_COL)\n",
    "        .cast(pl.Datetime)\n",
    "        .dt.epoch(time_unit='s') / 3600)\n",
    "        .cast(pl.Int64)\n",
    "    ).alias(DEFAULT_IMPRESSION_TIMESTAMP_COL)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (5, 7)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>user_id</th><th>article_id_fixed</th><th>article_ids_inview</th><th>article_ids_clicked</th><th>impression_id</th><th>impression_time</th><th>labels</th></tr><tr><td>u32</td><td>list[i32]</td><td>list[i64]</td><td>list[i64]</td><td>u32</td><td>i64</td><td>list[i8]</td></tr></thead><tbody><tr><td>366197</td><td>[9770327, 9769433, … 9769575]</td><td>[9777319, 9777292, … 9777992]</td><td>[9777804]</td><td>111436368</td><td>468015</td><td>[0, 0, … 0]</td></tr><tr><td>2238620</td><td>[9766599, 9768451, … 9770425]</td><td>[9777910, 9777769, … 9779747]</td><td>[9777769]</td><td>245249797</td><td>468044</td><td>[0, 1, … 0]</td></tr><tr><td>460020</td><td>[9770726, 9769404, … 9770604]</td><td>[9649886, 9744216, … 9678391]</td><td>[9678391]</td><td>576081382</td><td>467897</td><td>[0, 0, … 1]</td></tr><tr><td>1141128</td><td>[9769781, 9769828, … 9770568]</td><td>[9142564, 9769497, … 9470078]</td><td>[9770451]</td><td>218355780</td><td>467893</td><td>[0, 0, … 0]</td></tr><tr><td>1748053</td><td>[9769197, 9768002, … 9768260]</td><td>[9778728, 9527795, … 9776420]</td><td>[9772343]</td><td>39396736</td><td>468034</td><td>[0, 0, … 0]</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (5, 7)\n",
       "┌─────────┬──────────────┬──────────────┬──────────────┬──────────────┬──────────────┬─────────────┐\n",
       "│ user_id ┆ article_id_f ┆ article_ids_ ┆ article_ids_ ┆ impression_i ┆ impression_t ┆ labels      │\n",
       "│ ---     ┆ ixed         ┆ inview       ┆ clicked      ┆ d            ┆ ime          ┆ ---         │\n",
       "│ u32     ┆ ---          ┆ ---          ┆ ---          ┆ ---          ┆ ---          ┆ list[i8]    │\n",
       "│         ┆ list[i32]    ┆ list[i64]    ┆ list[i64]    ┆ u32          ┆ i64          ┆             │\n",
       "╞═════════╪══════════════╪══════════════╪══════════════╪══════════════╪══════════════╪═════════════╡\n",
       "│ 366197  ┆ [9770327,    ┆ [9777319,    ┆ [9777804]    ┆ 111436368    ┆ 468015       ┆ [0, 0, … 0] │\n",
       "│         ┆ 9769433, …   ┆ 9777292, …   ┆              ┆              ┆              ┆             │\n",
       "│         ┆ 9769575]     ┆ 9777992]     ┆              ┆              ┆              ┆             │\n",
       "│ 2238620 ┆ [9766599,    ┆ [9777910,    ┆ [9777769]    ┆ 245249797    ┆ 468044       ┆ [0, 1, … 0] │\n",
       "│         ┆ 9768451, …   ┆ 9777769, …   ┆              ┆              ┆              ┆             │\n",
       "│         ┆ 9770425]     ┆ 9779747]     ┆              ┆              ┆              ┆             │\n",
       "│ 460020  ┆ [9770726,    ┆ [9649886,    ┆ [9678391]    ┆ 576081382    ┆ 467897       ┆ [0, 0, … 1] │\n",
       "│         ┆ 9769404, …   ┆ 9744216, …   ┆              ┆              ┆              ┆             │\n",
       "│         ┆ 9770604]     ┆ 9678391]     ┆              ┆              ┆              ┆             │\n",
       "│ 1141128 ┆ [9769781,    ┆ [9142564,    ┆ [9770451]    ┆ 218355780    ┆ 467893       ┆ [0, 0, … 0] │\n",
       "│         ┆ 9769828, …   ┆ 9769497, …   ┆              ┆              ┆              ┆             │\n",
       "│         ┆ 9770568]     ┆ 9470078]     ┆              ┆              ┆              ┆             │\n",
       "│ 1748053 ┆ [9769197,    ┆ [9778728,    ┆ [9772343]    ┆ 39396736     ┆ 468034       ┆ [0, 0, … 0] │\n",
       "│         ┆ 9768002, …   ┆ 9527795, …   ┆              ┆              ┆              ┆             │\n",
       "│         ┆ 9768260]     ┆ 9776420]     ┆              ┆              ┆              ┆             │\n",
       "└─────────┴──────────────┴──────────────┴──────────────┴──────────────┴──────────────┴─────────────┘"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (5, 7)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>user_id</th><th>article_id_fixed</th><th>article_ids_inview</th><th>article_ids_clicked</th><th>impression_id</th><th>impression_time</th><th>labels</th></tr><tr><td>u32</td><td>list[i32]</td><td>list[i32]</td><td>list[i32]</td><td>u32</td><td>i64</td><td>list[i8]</td></tr></thead><tbody><tr><td>2196834</td><td>[9754160, 9482380, … 9779737]</td><td>[9789001, 9676230, … 9789037]</td><td>[9788797]</td><td>552698711</td><td>468199</td><td>[0, 0, … 0]</td></tr><tr><td>2450782</td><td>[9778731, 9778219, … 9779705]</td><td>[9080070, 9789417, … 9781621]</td><td>[9789702]</td><td>567749325</td><td>468207</td><td>[0, 0, … 0]</td></tr><tr><td>2197646</td><td>[9778035, 9778035, … 9779184]</td><td>[9782993, 9786932, … 9787185]</td><td>[9787098]</td><td>526361479</td><td>468164</td><td>[0, 0, … 0]</td></tr><tr><td>654737</td><td>[9778952, 9778973, … 9779873]</td><td>[9781785, 9483850, … 9142581]</td><td>[9781785]</td><td>168024697</td><td>468082</td><td>[1, 0, … 0]</td></tr><tr><td>1966653</td><td>[9775079, 9774542, … 9779045]</td><td>[9785593, 9785835, … 9785828]</td><td>[9785475]</td><td>394992848</td><td>468140</td><td>[0, 0, … 0]</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (5, 7)\n",
       "┌─────────┬──────────────┬──────────────┬──────────────┬──────────────┬──────────────┬─────────────┐\n",
       "│ user_id ┆ article_id_f ┆ article_ids_ ┆ article_ids_ ┆ impression_i ┆ impression_t ┆ labels      │\n",
       "│ ---     ┆ ixed         ┆ inview       ┆ clicked      ┆ d            ┆ ime          ┆ ---         │\n",
       "│ u32     ┆ ---          ┆ ---          ┆ ---          ┆ ---          ┆ ---          ┆ list[i8]    │\n",
       "│         ┆ list[i32]    ┆ list[i32]    ┆ list[i32]    ┆ u32          ┆ i64          ┆             │\n",
       "╞═════════╪══════════════╪══════════════╪══════════════╪══════════════╪══════════════╪═════════════╡\n",
       "│ 2196834 ┆ [9754160,    ┆ [9789001,    ┆ [9788797]    ┆ 552698711    ┆ 468199       ┆ [0, 0, … 0] │\n",
       "│         ┆ 9482380, …   ┆ 9676230, …   ┆              ┆              ┆              ┆             │\n",
       "│         ┆ 9779737]     ┆ 9789037]     ┆              ┆              ┆              ┆             │\n",
       "│ 2450782 ┆ [9778731,    ┆ [9080070,    ┆ [9789702]    ┆ 567749325    ┆ 468207       ┆ [0, 0, … 0] │\n",
       "│         ┆ 9778219, …   ┆ 9789417, …   ┆              ┆              ┆              ┆             │\n",
       "│         ┆ 9779705]     ┆ 9781621]     ┆              ┆              ┆              ┆             │\n",
       "│ 2197646 ┆ [9778035,    ┆ [9782993,    ┆ [9787098]    ┆ 526361479    ┆ 468164       ┆ [0, 0, … 0] │\n",
       "│         ┆ 9778035, …   ┆ 9786932, …   ┆              ┆              ┆              ┆             │\n",
       "│         ┆ 9779184]     ┆ 9787185]     ┆              ┆              ┆              ┆             │\n",
       "│ 654737  ┆ [9778952,    ┆ [9781785,    ┆ [9781785]    ┆ 168024697    ┆ 468082       ┆ [1, 0, … 0] │\n",
       "│         ┆ 9778973, …   ┆ 9483850, …   ┆              ┆              ┆              ┆             │\n",
       "│         ┆ 9779873]     ┆ 9142581]     ┆              ┆              ┆              ┆             │\n",
       "│ 1966653 ┆ [9775079,    ┆ [9785593,    ┆ [9785475]    ┆ 394992848    ┆ 468140       ┆ [0, 0, … 0] │\n",
       "│         ┆ 9774542, …   ┆ 9785835, …   ┆              ┆              ┆              ┆             │\n",
       "│         ┆ 9779045]     ┆ 9785828]     ┆              ┆              ┆              ┆             │\n",
       "└─────────┴──────────────┴──────────────┴──────────────┴──────────────┴──────────────┴─────────────┘"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_validation.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "vscode": {
     "languageId": "ruby"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average length of article_ids_inview in df_train: 5.0\n",
      "Average length of article_ids_inview in df_validation: 11.967748528449967\n"
     ]
    }
   ],
   "source": [
    "def calculate_average_length(df, column):\n",
    "    total_length = sum(len(row) for row in df[column])\n",
    "    average_length = total_length / len(df)\n",
    "    return average_length\n",
    "\n",
    "# Calculate average length for df_train\n",
    "average_length_inview_train = calculate_average_length(df_train, DEFAULT_INVIEW_ARTICLES_COL)\n",
    "print(f\"Average length of article_ids_inview in df_train: {average_length_inview_train}\")\n",
    "\n",
    "# Calculate average length for df_validation\n",
    "average_length_inview_validation = calculate_average_length(df_validation, DEFAULT_INVIEW_ARTICLES_COL)\n",
    "print(f\"Average length of article_ids_inview in df_validation: {average_length_inview_validation}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "ruby"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Longest inview article length in df_train: 5\n",
      "Longest inview article length in df_validation: 90\n",
      "Longest history length in df_train: 20\n",
      "Longest history length in df_validation: 20\n"
     ]
    }
   ],
   "source": [
    "# Function to find the maximum length of arrays in a column\n",
    "def find_max_length(df, column):\n",
    "    max_length = 0\n",
    "    for row in df[column]:\n",
    "        max_length = max(max_length, len(row))\n",
    "    return max_length\n",
    "\n",
    "# Find the longest inview article length in df_train\n",
    "max_inview_length_train = find_max_length(df_train, DEFAULT_INVIEW_ARTICLES_COL)\n",
    "\n",
    "# Find the longest inview article length in df_validation\n",
    "max_inview_length_validation = find_max_length(df_validation, DEFAULT_INVIEW_ARTICLES_COL)\n",
    "\n",
    "print(f\"Longest inview article length in df_train: {max_inview_length_train}\")\n",
    "print(f\"Longest inview article length in df_validation: {max_inview_length_validation}\")\n",
    "\n",
    "max_history_length_train = find_max_length(df_train, DEFAULT_HISTORY_ARTICLE_ID_COL)\n",
    "max_history_length_validation = find_max_length(df_validation, DEFAULT_HISTORY_ARTICLE_ID_COL)\n",
    "\n",
    "print(f\"Longest history length in df_train: {max_history_length_train}\")\n",
    "print(f\"Longest history length in df_validation: {max_history_length_validation}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "vscode": {
     "languageId": "ruby"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows with exactly one clicked article in df_train: 23427\n",
      "Number of rows with exactly one clicked article in df_validation: 24332\n"
     ]
    }
   ],
   "source": [
    "# Function to filter rows with exactly one clicked article\n",
    "def filter_rows_with_one_clicked_article(df, clicked_articles_col):\n",
    "    # Manually filter rows where the array has exactly one element\n",
    "    filtered_rows = []\n",
    "    for row in df.iter_rows(named=True):\n",
    "        if len(row[clicked_articles_col]) == 1:\n",
    "            filtered_rows.append(row)\n",
    "    return pl.DataFrame(filtered_rows)\n",
    "\n",
    "\n",
    "# Filter rows in df_train and df_validation\n",
    "df_train = filter_rows_with_one_clicked_article(df_train, DEFAULT_CLICKED_ARTICLES_COL)\n",
    "df_validation = filter_rows_with_one_clicked_article(df_validation, DEFAULT_CLICKED_ARTICLES_COL)\n",
    "\n",
    "# Print the results\n",
    "print(f\"Number of rows with exactly one clicked article in df_train: {df_train.shape[0]}\")\n",
    "print(f\"Number of rows with exactly one clicked article in df_validation: {df_validation.shape[0]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "vscode": {
     "languageId": "ruby"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (5, 7)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>user_id</th><th>article_id_fixed</th><th>article_ids_inview</th><th>article_ids_clicked</th><th>impression_id</th><th>impression_time</th><th>labels</th></tr><tr><td>i64</td><td>list[i64]</td><td>list[i64]</td><td>list[i64]</td><td>i64</td><td>i64</td><td>list[i64]</td></tr></thead><tbody><tr><td>2196834</td><td>[9754160, 9482380, … 9779737]</td><td>[9789001, 9676230, … 9789037]</td><td>[9788797]</td><td>552698711</td><td>468199</td><td>[0, 0, … 0]</td></tr><tr><td>2450782</td><td>[9778731, 9778219, … 9779705]</td><td>[9080070, 9789417, … 9781621]</td><td>[9789702]</td><td>567749325</td><td>468207</td><td>[0, 0, … 0]</td></tr><tr><td>2197646</td><td>[9778035, 9778035, … 9779184]</td><td>[9782993, 9786932, … 9787185]</td><td>[9787098]</td><td>526361479</td><td>468164</td><td>[0, 0, … 0]</td></tr><tr><td>654737</td><td>[9778952, 9778973, … 9779873]</td><td>[9781785, 9483850, … 9142581]</td><td>[9781785]</td><td>168024697</td><td>468082</td><td>[1, 0, … 0]</td></tr><tr><td>1966653</td><td>[9775079, 9774542, … 9779045]</td><td>[9785593, 9785835, … 9785828]</td><td>[9785475]</td><td>394992848</td><td>468140</td><td>[0, 0, … 0]</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (5, 7)\n",
       "┌─────────┬──────────────┬──────────────┬──────────────┬──────────────┬──────────────┬─────────────┐\n",
       "│ user_id ┆ article_id_f ┆ article_ids_ ┆ article_ids_ ┆ impression_i ┆ impression_t ┆ labels      │\n",
       "│ ---     ┆ ixed         ┆ inview       ┆ clicked      ┆ d            ┆ ime          ┆ ---         │\n",
       "│ i64     ┆ ---          ┆ ---          ┆ ---          ┆ ---          ┆ ---          ┆ list[i64]   │\n",
       "│         ┆ list[i64]    ┆ list[i64]    ┆ list[i64]    ┆ i64          ┆ i64          ┆             │\n",
       "╞═════════╪══════════════╪══════════════╪══════════════╪══════════════╪══════════════╪═════════════╡\n",
       "│ 2196834 ┆ [9754160,    ┆ [9789001,    ┆ [9788797]    ┆ 552698711    ┆ 468199       ┆ [0, 0, … 0] │\n",
       "│         ┆ 9482380, …   ┆ 9676230, …   ┆              ┆              ┆              ┆             │\n",
       "│         ┆ 9779737]     ┆ 9789037]     ┆              ┆              ┆              ┆             │\n",
       "│ 2450782 ┆ [9778731,    ┆ [9080070,    ┆ [9789702]    ┆ 567749325    ┆ 468207       ┆ [0, 0, … 0] │\n",
       "│         ┆ 9778219, …   ┆ 9789417, …   ┆              ┆              ┆              ┆             │\n",
       "│         ┆ 9779705]     ┆ 9781621]     ┆              ┆              ┆              ┆             │\n",
       "│ 2197646 ┆ [9778035,    ┆ [9782993,    ┆ [9787098]    ┆ 526361479    ┆ 468164       ┆ [0, 0, … 0] │\n",
       "│         ┆ 9778035, …   ┆ 9786932, …   ┆              ┆              ┆              ┆             │\n",
       "│         ┆ 9779184]     ┆ 9787185]     ┆              ┆              ┆              ┆             │\n",
       "│ 654737  ┆ [9778952,    ┆ [9781785,    ┆ [9781785]    ┆ 168024697    ┆ 468082       ┆ [1, 0, … 0] │\n",
       "│         ┆ 9778973, …   ┆ 9483850, …   ┆              ┆              ┆              ┆             │\n",
       "│         ┆ 9779873]     ┆ 9142581]     ┆              ┆              ┆              ┆             │\n",
       "│ 1966653 ┆ [9775079,    ┆ [9785593,    ┆ [9785475]    ┆ 394992848    ┆ 468140       ┆ [0, 0, … 0] │\n",
       "│         ┆ 9774542, …   ┆ 9785835, …   ┆              ┆              ┆              ┆             │\n",
       "│         ┆ 9779045]     ┆ 9785828]     ┆              ┆              ┆              ┆             │\n",
       "└─────────┴──────────────┴──────────────┴──────────────┴──────────────┴──────────────┴─────────────┘"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_validation.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "vscode": {
     "languageId": "ruby"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of users in df_train: 8897\n"
     ]
    }
   ],
   "source": [
    "print(f\"Number of users in df_train: {df_train['user_id'].n_unique()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (10, 21)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>article_id</th><th>title</th><th>subtitle</th><th>last_modified_time</th><th>premium</th><th>body</th><th>published_time</th><th>image_ids</th><th>article_type</th><th>url</th><th>ner_clusters</th><th>entity_groups</th><th>topics</th><th>category</th><th>subcategory</th><th>category_str</th><th>total_inviews</th><th>total_pageviews</th><th>total_read_time</th><th>sentiment_score</th><th>sentiment_label</th></tr><tr><td>i32</td><td>str</td><td>str</td><td>datetime[μs]</td><td>bool</td><td>str</td><td>datetime[μs]</td><td>list[i64]</td><td>str</td><td>str</td><td>list[str]</td><td>list[str]</td><td>list[str]</td><td>i16</td><td>list[i16]</td><td>str</td><td>i32</td><td>i32</td><td>f32</td><td>f32</td><td>str</td></tr></thead><tbody><tr><td>3001353</td><td>&quot;Natascha var ikke den første&quot;</td><td>&quot;Politiet frygter nu, at Natasc…</td><td>2023-06-29 06:20:33</td><td>false</td><td>&quot;Sagen om den østriske Natascha…</td><td>2006-08-31 08:06:45</td><td>[3150850]</td><td>&quot;article_default&quot;</td><td>&quot;https://ekstrabladet.dk/krimi/…</td><td>[]</td><td>[]</td><td>[&quot;Kriminalitet&quot;, &quot;Personfarlig kriminalitet&quot;]</td><td>140</td><td>[]</td><td>&quot;krimi&quot;</td><td>null</td><td>null</td><td>null</td><td>0.9955</td><td>&quot;Negative&quot;</td></tr><tr><td>3003065</td><td>&quot;Kun Star Wars tjente mere&quot;</td><td>&quot;Biografgængerne strømmer ind f…</td><td>2023-06-29 06:20:35</td><td>false</td><td>&quot;Vatikanet har opfordret til at…</td><td>2006-05-21 16:57:00</td><td>[3006712]</td><td>&quot;article_default&quot;</td><td>&quot;https://ekstrabladet.dk/underh…</td><td>[]</td><td>[]</td><td>[&quot;Underholdning&quot;, &quot;Film og tv&quot;, &quot;Økonomi&quot;]</td><td>414</td><td>[433, 434]</td><td>&quot;underholdning&quot;</td><td>null</td><td>null</td><td>null</td><td>0.846</td><td>&quot;Positive&quot;</td></tr><tr><td>3012771</td><td>&quot;Morten Bruun fyret i Sønderjys…</td><td>&quot;FODBOLD: Morten Bruun fyret me…</td><td>2023-06-29 06:20:39</td><td>false</td><td>&quot;Kemien mellem spillerne i Supe…</td><td>2006-05-01 14:28:40</td><td>[3177953]</td><td>&quot;article_default&quot;</td><td>&quot;https://ekstrabladet.dk/sport/…</td><td>[]</td><td>[]</td><td>[&quot;Erhverv&quot;, &quot;Kendt&quot;, … &quot;Ansættelsesforhold&quot;]</td><td>142</td><td>[196, 199]</td><td>&quot;sport&quot;</td><td>null</td><td>null</td><td>null</td><td>0.8241</td><td>&quot;Negative&quot;</td></tr><tr><td>3023463</td><td>&quot;Luderne flytter på landet&quot;</td><td>&quot;I landets tyndest befolkede om…</td><td>2023-06-29 06:20:43</td><td>false</td><td>&quot;Det frække erhverv rykker på l…</td><td>2007-03-24 08:27:59</td><td>[3184029]</td><td>&quot;article_default&quot;</td><td>&quot;https://ekstrabladet.dk/nyhede…</td><td>[]</td><td>[]</td><td>[&quot;Livsstil&quot;, &quot;Erotik&quot;]</td><td>118</td><td>[133]</td><td>&quot;nyheder&quot;</td><td>null</td><td>null</td><td>null</td><td>0.7053</td><td>&quot;Neutral&quot;</td></tr><tr><td>3032577</td><td>&quot;Cybersex: Hvornår er man utro?&quot;</td><td>&quot;En flirtende sms til den flott…</td><td>2023-06-29 06:20:46</td><td>false</td><td>&quot;De fleste af os mener, at et t…</td><td>2007-01-18 10:30:37</td><td>[3030463]</td><td>&quot;article_default&quot;</td><td>&quot;https://ekstrabladet.dk/sex_og…</td><td>[]</td><td>[]</td><td>[&quot;Livsstil&quot;, &quot;Partnerskab&quot;]</td><td>565</td><td>[]</td><td>&quot;sex_og_samliv&quot;</td><td>null</td><td>null</td><td>null</td><td>0.9307</td><td>&quot;Neutral&quot;</td></tr><tr><td>3033563</td><td>&quot;Kniven for struben-vært får se…</td><td>&quot;I aftenens udgave af &#x27;Med kniv…</td><td>2023-06-29 06:20:47</td><td>false</td><td>&quot;Når man ser fjerde program i T…</td><td>2007-03-27 10:22:08</td><td>[3005524, 3005525]</td><td>&quot;article_default&quot;</td><td>&quot;https://ekstrabladet.dk/underh…</td><td>[]</td><td>[]</td><td>[&quot;Livsstil&quot;, &quot;Underholdning&quot;, … &quot;Mad og drikke&quot;]</td><td>414</td><td>[433, 436]</td><td>&quot;underholdning&quot;</td><td>null</td><td>null</td><td>null</td><td>0.9371</td><td>&quot;Neutral&quot;</td></tr><tr><td>3034608</td><td>&quot;Willy Strube har begået selvmo…</td><td>&quot;Den tidligere SiD-chef tog sit…</td><td>2023-06-29 06:20:49</td><td>false</td><td>&quot;Den tidligere formand for Indu…</td><td>2001-10-19 12:30:00</td><td>[3204848]</td><td>&quot;article_default&quot;</td><td>&quot;https://ekstrabladet.dk/nyhede…</td><td>[&quot;Willy Strube&quot;, &quot;Willy Strube&quot;, &quot;Willy Strube&quot;]</td><td>[&quot;PER&quot;, &quot;PER&quot;, &quot;PER&quot;]</td><td>[&quot;Kriminalitet&quot;, &quot;Erhverv&quot;, … &quot;Offentlig instans&quot;]</td><td>118</td><td>[130]</td><td>&quot;nyheder&quot;</td><td>null</td><td>null</td><td>null</td><td>0.9971</td><td>&quot;Negative&quot;</td></tr><tr><td>3034849</td><td>&quot;Venner for livet&quot;</td><td>&quot;VK-REGERINGEN&quot;</td><td>2023-06-29 06:20:50</td><td>false</td><td>&quot;VK-REGERINGEN\n",
       "håndplukkede Bjø…</td><td>2003-01-09 06:00:00</td><td>null</td><td>&quot;article_default&quot;</td><td>&quot;https://ekstrabladet.dk/incomi…</td><td>[]</td><td>[]</td><td>[&quot;Kendt&quot;, &quot;Politik&quot;, &quot;National politik&quot;]</td><td>2</td><td>[]</td><td>&quot;incoming&quot;</td><td>null</td><td>null</td><td>null</td><td>0.8454</td><td>&quot;Neutral&quot;</td></tr><tr><td>3035648</td><td>&quot;Dronning af escort-branchen&quot;</td><td>&quot;Trine Michelsen hjælper københ…</td><td>2023-06-29 06:20:52</td><td>false</td><td>&quot;En af escortbranchens største …</td><td>2003-06-17 07:10:00</td><td>[3082573]</td><td>&quot;article_default&quot;</td><td>&quot;https://ekstrabladet.dk/krimi/…</td><td>[]</td><td>[]</td><td>[&quot;Erhverv&quot;, &quot;Livsstil&quot;, … &quot;Erotik&quot;]</td><td>140</td><td>[]</td><td>&quot;krimi&quot;</td><td>null</td><td>null</td><td>null</td><td>0.8814</td><td>&quot;Neutral&quot;</td></tr><tr><td>3036444</td><td>&quot;Mia kendte sandsynligvis sin m…</td><td>&quot;Hun var ikke den type, der søg…</td><td>2023-06-29 06:20:54</td><td>false</td><td>&quot;Den 12-årige Mia Teglgaard Spr…</td><td>2003-07-13 19:50:00</td><td>null</td><td>&quot;article_default&quot;</td><td>&quot;https://ekstrabladet.dk/krimi/…</td><td>[&quot;Mia Teglgaard Sprotte&quot;, &quot;Erik Andersen&quot;, … &quot;Mia Teglgaard Sprotte&quot;]</td><td>[&quot;PER&quot;, &quot;PER&quot;, … &quot;PER&quot;]</td><td>[&quot;Kriminalitet&quot;, &quot;Personfarlig kriminalitet&quot;]</td><td>140</td><td>[]</td><td>&quot;krimi&quot;</td><td>null</td><td>null</td><td>null</td><td>0.9752</td><td>&quot;Negative&quot;</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (10, 21)\n",
       "┌───────────┬───────────┬───────────┬───────────┬───┬───────────┬───────────┬───────────┬──────────┐\n",
       "│ article_i ┆ title     ┆ subtitle  ┆ last_modi ┆ … ┆ total_pag ┆ total_rea ┆ sentiment ┆ sentimen │\n",
       "│ d         ┆ ---       ┆ ---       ┆ fied_time ┆   ┆ eviews    ┆ d_time    ┆ _score    ┆ t_label  │\n",
       "│ ---       ┆ str       ┆ str       ┆ ---       ┆   ┆ ---       ┆ ---       ┆ ---       ┆ ---      │\n",
       "│ i32       ┆           ┆           ┆ datetime[ ┆   ┆ i32       ┆ f32       ┆ f32       ┆ str      │\n",
       "│           ┆           ┆           ┆ μs]       ┆   ┆           ┆           ┆           ┆          │\n",
       "╞═══════════╪═══════════╪═══════════╪═══════════╪═══╪═══════════╪═══════════╪═══════════╪══════════╡\n",
       "│ 3001353   ┆ Natascha  ┆ Politiet  ┆ 2023-06-2 ┆ … ┆ null      ┆ null      ┆ 0.9955    ┆ Negative │\n",
       "│           ┆ var ikke  ┆ frygter   ┆ 9         ┆   ┆           ┆           ┆           ┆          │\n",
       "│           ┆ den       ┆ nu, at    ┆ 06:20:33  ┆   ┆           ┆           ┆           ┆          │\n",
       "│           ┆ første    ┆ Natasc…   ┆           ┆   ┆           ┆           ┆           ┆          │\n",
       "│ 3003065   ┆ Kun Star  ┆ Biografgæ ┆ 2023-06-2 ┆ … ┆ null      ┆ null      ┆ 0.846     ┆ Positive │\n",
       "│           ┆ Wars      ┆ ngerne    ┆ 9         ┆   ┆           ┆           ┆           ┆          │\n",
       "│           ┆ tjente    ┆ strømmer  ┆ 06:20:35  ┆   ┆           ┆           ┆           ┆          │\n",
       "│           ┆ mere      ┆ ind f…    ┆           ┆   ┆           ┆           ┆           ┆          │\n",
       "│ 3012771   ┆ Morten    ┆ FODBOLD:  ┆ 2023-06-2 ┆ … ┆ null      ┆ null      ┆ 0.8241    ┆ Negative │\n",
       "│           ┆ Bruun     ┆ Morten    ┆ 9         ┆   ┆           ┆           ┆           ┆          │\n",
       "│           ┆ fyret i   ┆ Bruun     ┆ 06:20:39  ┆   ┆           ┆           ┆           ┆          │\n",
       "│           ┆ Sønderjys ┆ fyret me… ┆           ┆   ┆           ┆           ┆           ┆          │\n",
       "│           ┆ …         ┆           ┆           ┆   ┆           ┆           ┆           ┆          │\n",
       "│ 3023463   ┆ Luderne   ┆ I landets ┆ 2023-06-2 ┆ … ┆ null      ┆ null      ┆ 0.7053    ┆ Neutral  │\n",
       "│           ┆ flytter   ┆ tyndest   ┆ 9         ┆   ┆           ┆           ┆           ┆          │\n",
       "│           ┆ på landet ┆ befolkede ┆ 06:20:43  ┆   ┆           ┆           ┆           ┆          │\n",
       "│           ┆           ┆ om…       ┆           ┆   ┆           ┆           ┆           ┆          │\n",
       "│ 3032577   ┆ Cybersex: ┆ En        ┆ 2023-06-2 ┆ … ┆ null      ┆ null      ┆ 0.9307    ┆ Neutral  │\n",
       "│           ┆ Hvornår   ┆ flirtende ┆ 9         ┆   ┆           ┆           ┆           ┆          │\n",
       "│           ┆ er man    ┆ sms til   ┆ 06:20:46  ┆   ┆           ┆           ┆           ┆          │\n",
       "│           ┆ utro?     ┆ den       ┆           ┆   ┆           ┆           ┆           ┆          │\n",
       "│           ┆           ┆ flott…    ┆           ┆   ┆           ┆           ┆           ┆          │\n",
       "│ 3033563   ┆ Kniven    ┆ I         ┆ 2023-06-2 ┆ … ┆ null      ┆ null      ┆ 0.9371    ┆ Neutral  │\n",
       "│           ┆ for strub ┆ aftenens  ┆ 9         ┆   ┆           ┆           ┆           ┆          │\n",
       "│           ┆ en-vært   ┆ udgave af ┆ 06:20:47  ┆   ┆           ┆           ┆           ┆          │\n",
       "│           ┆ får se…   ┆ 'Med      ┆           ┆   ┆           ┆           ┆           ┆          │\n",
       "│           ┆           ┆ kniv…     ┆           ┆   ┆           ┆           ┆           ┆          │\n",
       "│ 3034608   ┆ Willy     ┆ Den       ┆ 2023-06-2 ┆ … ┆ null      ┆ null      ┆ 0.9971    ┆ Negative │\n",
       "│           ┆ Strube    ┆ tidligere ┆ 9         ┆   ┆           ┆           ┆           ┆          │\n",
       "│           ┆ har       ┆ SiD-chef  ┆ 06:20:49  ┆   ┆           ┆           ┆           ┆          │\n",
       "│           ┆ begået    ┆ tog sit…  ┆           ┆   ┆           ┆           ┆           ┆          │\n",
       "│           ┆ selvmo…   ┆           ┆           ┆   ┆           ┆           ┆           ┆          │\n",
       "│ 3034849   ┆ Venner    ┆ VK-REGERI ┆ 2023-06-2 ┆ … ┆ null      ┆ null      ┆ 0.8454    ┆ Neutral  │\n",
       "│           ┆ for livet ┆ NGEN      ┆ 9         ┆   ┆           ┆           ┆           ┆          │\n",
       "│           ┆           ┆           ┆ 06:20:50  ┆   ┆           ┆           ┆           ┆          │\n",
       "│ 3035648   ┆ Dronning  ┆ Trine     ┆ 2023-06-2 ┆ … ┆ null      ┆ null      ┆ 0.8814    ┆ Neutral  │\n",
       "│           ┆ af escort ┆ Michelsen ┆ 9         ┆   ┆           ┆           ┆           ┆          │\n",
       "│           ┆ -branchen ┆ hjælper   ┆ 06:20:52  ┆   ┆           ┆           ┆           ┆          │\n",
       "│           ┆           ┆ københ…   ┆           ┆   ┆           ┆           ┆           ┆          │\n",
       "│ 3036444   ┆ Mia       ┆ Hun var   ┆ 2023-06-2 ┆ … ┆ null      ┆ null      ┆ 0.9752    ┆ Negative │\n",
       "│           ┆ kendte    ┆ ikke den  ┆ 9         ┆   ┆           ┆           ┆           ┆          │\n",
       "│           ┆ sandsynli ┆ type, der ┆ 06:20:54  ┆   ┆           ┆           ┆           ┆          │\n",
       "│           ┆ gvis sin  ┆ søg…      ┆           ┆   ┆           ┆           ┆           ┆          │\n",
       "│           ┆ m…        ┆           ┆           ┆   ┆           ┆           ┆           ┆          │\n",
       "└───────────┴───────────┴───────────┴───────────┴───┴───────────┴───────────┴───────────┴──────────┘"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_articles = pl.read_parquet(PATH.joinpath(\"articles.parquet\"))\n",
    "df_articles.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_articles = df_articles.with_columns([\n",
    "    (\n",
    "        pl.col(DEFAULT_ARTICLE_PUBLISHED_TIMESTAMP_COL)\n",
    "        .cast(pl.Datetime)                   # Step 3: Cast to Datetime\n",
    "        .dt.epoch(time_unit='s')            # Step 4: Convert to Epoch Seconds\n",
    "        / 3600                               # Step 5: Convert Seconds to Hours\n",
    "    ).cast(pl.Int64)                         # Step 6: Cast to Integer\n",
    "    .alias(DEFAULT_ARTICLE_PUBLISHED_TIMESTAMP_COL)  # Step 7: Alias the Column\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (10, 21)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>article_id</th><th>title</th><th>subtitle</th><th>last_modified_time</th><th>premium</th><th>body</th><th>published_time</th><th>image_ids</th><th>article_type</th><th>url</th><th>ner_clusters</th><th>entity_groups</th><th>topics</th><th>category</th><th>subcategory</th><th>category_str</th><th>total_inviews</th><th>total_pageviews</th><th>total_read_time</th><th>sentiment_score</th><th>sentiment_label</th></tr><tr><td>i32</td><td>str</td><td>str</td><td>datetime[μs]</td><td>bool</td><td>str</td><td>i64</td><td>list[i64]</td><td>str</td><td>str</td><td>list[str]</td><td>list[str]</td><td>list[str]</td><td>i16</td><td>list[i16]</td><td>str</td><td>i32</td><td>i32</td><td>f32</td><td>f32</td><td>str</td></tr></thead><tbody><tr><td>3001353</td><td>&quot;Natascha var ikke den første&quot;</td><td>&quot;Politiet frygter nu, at Natasc…</td><td>2023-06-29 06:20:33</td><td>false</td><td>&quot;Sagen om den østriske Natascha…</td><td>321392</td><td>[3150850]</td><td>&quot;article_default&quot;</td><td>&quot;https://ekstrabladet.dk/krimi/…</td><td>[]</td><td>[]</td><td>[&quot;Kriminalitet&quot;, &quot;Personfarlig kriminalitet&quot;]</td><td>140</td><td>[]</td><td>&quot;krimi&quot;</td><td>null</td><td>null</td><td>null</td><td>0.9955</td><td>&quot;Negative&quot;</td></tr><tr><td>3003065</td><td>&quot;Kun Star Wars tjente mere&quot;</td><td>&quot;Biografgængerne strømmer ind f…</td><td>2023-06-29 06:20:35</td><td>false</td><td>&quot;Vatikanet har opfordret til at…</td><td>318952</td><td>[3006712]</td><td>&quot;article_default&quot;</td><td>&quot;https://ekstrabladet.dk/underh…</td><td>[]</td><td>[]</td><td>[&quot;Underholdning&quot;, &quot;Film og tv&quot;, &quot;Økonomi&quot;]</td><td>414</td><td>[433, 434]</td><td>&quot;underholdning&quot;</td><td>null</td><td>null</td><td>null</td><td>0.846</td><td>&quot;Positive&quot;</td></tr><tr><td>3012771</td><td>&quot;Morten Bruun fyret i Sønderjys…</td><td>&quot;FODBOLD: Morten Bruun fyret me…</td><td>2023-06-29 06:20:39</td><td>false</td><td>&quot;Kemien mellem spillerne i Supe…</td><td>318470</td><td>[3177953]</td><td>&quot;article_default&quot;</td><td>&quot;https://ekstrabladet.dk/sport/…</td><td>[]</td><td>[]</td><td>[&quot;Erhverv&quot;, &quot;Kendt&quot;, … &quot;Ansættelsesforhold&quot;]</td><td>142</td><td>[196, 199]</td><td>&quot;sport&quot;</td><td>null</td><td>null</td><td>null</td><td>0.8241</td><td>&quot;Negative&quot;</td></tr><tr><td>3023463</td><td>&quot;Luderne flytter på landet&quot;</td><td>&quot;I landets tyndest befolkede om…</td><td>2023-06-29 06:20:43</td><td>false</td><td>&quot;Det frække erhverv rykker på l…</td><td>326312</td><td>[3184029]</td><td>&quot;article_default&quot;</td><td>&quot;https://ekstrabladet.dk/nyhede…</td><td>[]</td><td>[]</td><td>[&quot;Livsstil&quot;, &quot;Erotik&quot;]</td><td>118</td><td>[133]</td><td>&quot;nyheder&quot;</td><td>null</td><td>null</td><td>null</td><td>0.7053</td><td>&quot;Neutral&quot;</td></tr><tr><td>3032577</td><td>&quot;Cybersex: Hvornår er man utro?&quot;</td><td>&quot;En flirtende sms til den flott…</td><td>2023-06-29 06:20:46</td><td>false</td><td>&quot;De fleste af os mener, at et t…</td><td>324754</td><td>[3030463]</td><td>&quot;article_default&quot;</td><td>&quot;https://ekstrabladet.dk/sex_og…</td><td>[]</td><td>[]</td><td>[&quot;Livsstil&quot;, &quot;Partnerskab&quot;]</td><td>565</td><td>[]</td><td>&quot;sex_og_samliv&quot;</td><td>null</td><td>null</td><td>null</td><td>0.9307</td><td>&quot;Neutral&quot;</td></tr><tr><td>3033563</td><td>&quot;Kniven for struben-vært får se…</td><td>&quot;I aftenens udgave af &#x27;Med kniv…</td><td>2023-06-29 06:20:47</td><td>false</td><td>&quot;Når man ser fjerde program i T…</td><td>326386</td><td>[3005524, 3005525]</td><td>&quot;article_default&quot;</td><td>&quot;https://ekstrabladet.dk/underh…</td><td>[]</td><td>[]</td><td>[&quot;Livsstil&quot;, &quot;Underholdning&quot;, … &quot;Mad og drikke&quot;]</td><td>414</td><td>[433, 436]</td><td>&quot;underholdning&quot;</td><td>null</td><td>null</td><td>null</td><td>0.9371</td><td>&quot;Neutral&quot;</td></tr><tr><td>3034608</td><td>&quot;Willy Strube har begået selvmo…</td><td>&quot;Den tidligere SiD-chef tog sit…</td><td>2023-06-29 06:20:49</td><td>false</td><td>&quot;Den tidligere formand for Indu…</td><td>278748</td><td>[3204848]</td><td>&quot;article_default&quot;</td><td>&quot;https://ekstrabladet.dk/nyhede…</td><td>[&quot;Willy Strube&quot;, &quot;Willy Strube&quot;, &quot;Willy Strube&quot;]</td><td>[&quot;PER&quot;, &quot;PER&quot;, &quot;PER&quot;]</td><td>[&quot;Kriminalitet&quot;, &quot;Erhverv&quot;, … &quot;Offentlig instans&quot;]</td><td>118</td><td>[130]</td><td>&quot;nyheder&quot;</td><td>null</td><td>null</td><td>null</td><td>0.9971</td><td>&quot;Negative&quot;</td></tr><tr><td>3034849</td><td>&quot;Venner for livet&quot;</td><td>&quot;VK-REGERINGEN&quot;</td><td>2023-06-29 06:20:50</td><td>false</td><td>&quot;VK-REGERINGEN\n",
       "håndplukkede Bjø…</td><td>289470</td><td>null</td><td>&quot;article_default&quot;</td><td>&quot;https://ekstrabladet.dk/incomi…</td><td>[]</td><td>[]</td><td>[&quot;Kendt&quot;, &quot;Politik&quot;, &quot;National politik&quot;]</td><td>2</td><td>[]</td><td>&quot;incoming&quot;</td><td>null</td><td>null</td><td>null</td><td>0.8454</td><td>&quot;Neutral&quot;</td></tr><tr><td>3035648</td><td>&quot;Dronning af escort-branchen&quot;</td><td>&quot;Trine Michelsen hjælper københ…</td><td>2023-06-29 06:20:52</td><td>false</td><td>&quot;En af escortbranchens største …</td><td>293287</td><td>[3082573]</td><td>&quot;article_default&quot;</td><td>&quot;https://ekstrabladet.dk/krimi/…</td><td>[]</td><td>[]</td><td>[&quot;Erhverv&quot;, &quot;Livsstil&quot;, … &quot;Erotik&quot;]</td><td>140</td><td>[]</td><td>&quot;krimi&quot;</td><td>null</td><td>null</td><td>null</td><td>0.8814</td><td>&quot;Neutral&quot;</td></tr><tr><td>3036444</td><td>&quot;Mia kendte sandsynligvis sin m…</td><td>&quot;Hun var ikke den type, der søg…</td><td>2023-06-29 06:20:54</td><td>false</td><td>&quot;Den 12-årige Mia Teglgaard Spr…</td><td>293923</td><td>null</td><td>&quot;article_default&quot;</td><td>&quot;https://ekstrabladet.dk/krimi/…</td><td>[&quot;Mia Teglgaard Sprotte&quot;, &quot;Erik Andersen&quot;, … &quot;Mia Teglgaard Sprotte&quot;]</td><td>[&quot;PER&quot;, &quot;PER&quot;, … &quot;PER&quot;]</td><td>[&quot;Kriminalitet&quot;, &quot;Personfarlig kriminalitet&quot;]</td><td>140</td><td>[]</td><td>&quot;krimi&quot;</td><td>null</td><td>null</td><td>null</td><td>0.9752</td><td>&quot;Negative&quot;</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (10, 21)\n",
       "┌───────────┬───────────┬───────────┬───────────┬───┬───────────┬───────────┬───────────┬──────────┐\n",
       "│ article_i ┆ title     ┆ subtitle  ┆ last_modi ┆ … ┆ total_pag ┆ total_rea ┆ sentiment ┆ sentimen │\n",
       "│ d         ┆ ---       ┆ ---       ┆ fied_time ┆   ┆ eviews    ┆ d_time    ┆ _score    ┆ t_label  │\n",
       "│ ---       ┆ str       ┆ str       ┆ ---       ┆   ┆ ---       ┆ ---       ┆ ---       ┆ ---      │\n",
       "│ i32       ┆           ┆           ┆ datetime[ ┆   ┆ i32       ┆ f32       ┆ f32       ┆ str      │\n",
       "│           ┆           ┆           ┆ μs]       ┆   ┆           ┆           ┆           ┆          │\n",
       "╞═══════════╪═══════════╪═══════════╪═══════════╪═══╪═══════════╪═══════════╪═══════════╪══════════╡\n",
       "│ 3001353   ┆ Natascha  ┆ Politiet  ┆ 2023-06-2 ┆ … ┆ null      ┆ null      ┆ 0.9955    ┆ Negative │\n",
       "│           ┆ var ikke  ┆ frygter   ┆ 9         ┆   ┆           ┆           ┆           ┆          │\n",
       "│           ┆ den       ┆ nu, at    ┆ 06:20:33  ┆   ┆           ┆           ┆           ┆          │\n",
       "│           ┆ første    ┆ Natasc…   ┆           ┆   ┆           ┆           ┆           ┆          │\n",
       "│ 3003065   ┆ Kun Star  ┆ Biografgæ ┆ 2023-06-2 ┆ … ┆ null      ┆ null      ┆ 0.846     ┆ Positive │\n",
       "│           ┆ Wars      ┆ ngerne    ┆ 9         ┆   ┆           ┆           ┆           ┆          │\n",
       "│           ┆ tjente    ┆ strømmer  ┆ 06:20:35  ┆   ┆           ┆           ┆           ┆          │\n",
       "│           ┆ mere      ┆ ind f…    ┆           ┆   ┆           ┆           ┆           ┆          │\n",
       "│ 3012771   ┆ Morten    ┆ FODBOLD:  ┆ 2023-06-2 ┆ … ┆ null      ┆ null      ┆ 0.8241    ┆ Negative │\n",
       "│           ┆ Bruun     ┆ Morten    ┆ 9         ┆   ┆           ┆           ┆           ┆          │\n",
       "│           ┆ fyret i   ┆ Bruun     ┆ 06:20:39  ┆   ┆           ┆           ┆           ┆          │\n",
       "│           ┆ Sønderjys ┆ fyret me… ┆           ┆   ┆           ┆           ┆           ┆          │\n",
       "│           ┆ …         ┆           ┆           ┆   ┆           ┆           ┆           ┆          │\n",
       "│ 3023463   ┆ Luderne   ┆ I landets ┆ 2023-06-2 ┆ … ┆ null      ┆ null      ┆ 0.7053    ┆ Neutral  │\n",
       "│           ┆ flytter   ┆ tyndest   ┆ 9         ┆   ┆           ┆           ┆           ┆          │\n",
       "│           ┆ på landet ┆ befolkede ┆ 06:20:43  ┆   ┆           ┆           ┆           ┆          │\n",
       "│           ┆           ┆ om…       ┆           ┆   ┆           ┆           ┆           ┆          │\n",
       "│ 3032577   ┆ Cybersex: ┆ En        ┆ 2023-06-2 ┆ … ┆ null      ┆ null      ┆ 0.9307    ┆ Neutral  │\n",
       "│           ┆ Hvornår   ┆ flirtende ┆ 9         ┆   ┆           ┆           ┆           ┆          │\n",
       "│           ┆ er man    ┆ sms til   ┆ 06:20:46  ┆   ┆           ┆           ┆           ┆          │\n",
       "│           ┆ utro?     ┆ den       ┆           ┆   ┆           ┆           ┆           ┆          │\n",
       "│           ┆           ┆ flott…    ┆           ┆   ┆           ┆           ┆           ┆          │\n",
       "│ 3033563   ┆ Kniven    ┆ I         ┆ 2023-06-2 ┆ … ┆ null      ┆ null      ┆ 0.9371    ┆ Neutral  │\n",
       "│           ┆ for strub ┆ aftenens  ┆ 9         ┆   ┆           ┆           ┆           ┆          │\n",
       "│           ┆ en-vært   ┆ udgave af ┆ 06:20:47  ┆   ┆           ┆           ┆           ┆          │\n",
       "│           ┆ får se…   ┆ 'Med      ┆           ┆   ┆           ┆           ┆           ┆          │\n",
       "│           ┆           ┆ kniv…     ┆           ┆   ┆           ┆           ┆           ┆          │\n",
       "│ 3034608   ┆ Willy     ┆ Den       ┆ 2023-06-2 ┆ … ┆ null      ┆ null      ┆ 0.9971    ┆ Negative │\n",
       "│           ┆ Strube    ┆ tidligere ┆ 9         ┆   ┆           ┆           ┆           ┆          │\n",
       "│           ┆ har       ┆ SiD-chef  ┆ 06:20:49  ┆   ┆           ┆           ┆           ┆          │\n",
       "│           ┆ begået    ┆ tog sit…  ┆           ┆   ┆           ┆           ┆           ┆          │\n",
       "│           ┆ selvmo…   ┆           ┆           ┆   ┆           ┆           ┆           ┆          │\n",
       "│ 3034849   ┆ Venner    ┆ VK-REGERI ┆ 2023-06-2 ┆ … ┆ null      ┆ null      ┆ 0.8454    ┆ Neutral  │\n",
       "│           ┆ for livet ┆ NGEN      ┆ 9         ┆   ┆           ┆           ┆           ┆          │\n",
       "│           ┆           ┆           ┆ 06:20:50  ┆   ┆           ┆           ┆           ┆          │\n",
       "│ 3035648   ┆ Dronning  ┆ Trine     ┆ 2023-06-2 ┆ … ┆ null      ┆ null      ┆ 0.8814    ┆ Neutral  │\n",
       "│           ┆ af escort ┆ Michelsen ┆ 9         ┆   ┆           ┆           ┆           ┆          │\n",
       "│           ┆ -branchen ┆ hjælper   ┆ 06:20:52  ┆   ┆           ┆           ┆           ┆          │\n",
       "│           ┆           ┆ københ…   ┆           ┆   ┆           ┆           ┆           ┆          │\n",
       "│ 3036444   ┆ Mia       ┆ Hun var   ┆ 2023-06-2 ┆ … ┆ null      ┆ null      ┆ 0.9752    ┆ Negative │\n",
       "│           ┆ kendte    ┆ ikke den  ┆ 9         ┆   ┆           ┆           ┆           ┆          │\n",
       "│           ┆ sandsynli ┆ type, der ┆ 06:20:54  ┆   ┆           ┆           ┆           ┆          │\n",
       "│           ┆ gvis sin  ┆ søg…      ┆           ┆   ┆           ┆           ┆           ┆          │\n",
       "│           ┆ m…        ┆           ┆           ┆   ┆           ┆           ┆           ┆          │\n",
       "└───────────┴───────────┴───────────┴───────────┴───┴───────────┴───────────┴───────────┴──────────┘"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_articles.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "DOCVEC = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the document vector parquet file\n",
    "document_vector_path = Path(\"./Ekstra_Bladet_word2vec/document_vector.parquet\")\n",
    "df_document_vector = pl.read_parquet(document_vector_path)\n",
    "\n",
    "# Display the first few rows of the dataframe\n",
    "\n",
    "article_mapping = create_article_id_to_value_mapping(df=df_document_vector, value_col=\"document_vector\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (5, 2)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>article_id</th><th>document_vector</th></tr><tr><td>i32</td><td>list[f32]</td></tr></thead><tbody><tr><td>3000022</td><td>[0.065424, -0.047425, … 0.035706]</td></tr><tr><td>3000063</td><td>[0.028815, -0.000166, … 0.027167]</td></tr><tr><td>3000613</td><td>[0.037971, 0.033923, … 0.063961]</td></tr><tr><td>3000700</td><td>[0.046524, 0.002913, … 0.023423]</td></tr><tr><td>3000840</td><td>[0.014737, 0.024068, … 0.045991]</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (5, 2)\n",
       "┌────────────┬─────────────────────────────────┐\n",
       "│ article_id ┆ document_vector                 │\n",
       "│ ---        ┆ ---                             │\n",
       "│ i32        ┆ list[f32]                       │\n",
       "╞════════════╪═════════════════════════════════╡\n",
       "│ 3000022    ┆ [0.065424, -0.047425, … 0.0357… │\n",
       "│ 3000063    ┆ [0.028815, -0.000166, … 0.0271… │\n",
       "│ 3000613    ┆ [0.037971, 0.033923, … 0.06396… │\n",
       "│ 3000700    ┆ [0.046524, 0.002913, … 0.02342… │\n",
       "│ 3000840    ┆ [0.014737, 0.024068, … 0.04599… │\n",
       "└────────────┴─────────────────────────────────┘"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_document_vector.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embedding tokenized article title"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reduce dimensionality of embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Uses existing function for both categories and topics\n",
    "* Handles array of topics by joining with separator\n",
    "* Limits to first 3 topics to avoid sequence length issues\n",
    "* Maintains consistent encoding approach across feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sys:1: MapWithoutReturnDtypeWarning: Calling `map_elements` without specifying `return_dtype` can lead to unpredictable results. Specify `return_dtype` to silence this warning.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "MAX_TITLE_LENGTH = 30\n",
    "from transformers import ConvBertTokenizer, ConvBertModel\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(device)\n",
    "\n",
    "\n",
    "transformer_tokenizer = ConvBertTokenizer.from_pretrained(\"sarnikowski/convbert-small-da-cased\")\n",
    "transformer_model = ConvBertModel.from_pretrained(\"sarnikowski/convbert-small-da-cased\")\n",
    "\n",
    "\n",
    "# For categories (keep as is - works with single strings)\n",
    "df_articles, category_encoded_col_name = convert_text2encoding_with_transformers(\n",
    "    df_articles, \n",
    "    transformer_tokenizer,\n",
    "    DEFAULT_CATEGORY_STR_COL,\n",
    "    max_length=MAX_TITLE_LENGTH\n",
    ")\n",
    "\n",
    "# For topics - first join the array into single string\n",
    "df_articles = df_articles.with_columns(\n",
    "    pl.col(DEFAULT_TOPICS_COL).map_elements(lambda x: \" | \".join(x[:3])).alias(f\"{DEFAULT_TOPICS_COL}_joined\")\n",
    ")\n",
    "\n",
    "# Then encode the joined topics\n",
    "df_articles, topics_encoded_col_name = convert_text2encoding_with_transformers(\n",
    "    df_articles,\n",
    "    transformer_tokenizer, \n",
    "    f\"{DEFAULT_TOPICS_COL}_joined\",\n",
    "    max_length=MAX_TITLE_LENGTH\n",
    ")\n",
    "\n",
    "# Create mappings with encoded columns\n",
    "category_mapping = create_article_id_to_value_mapping(\n",
    "    df=df_articles, \n",
    "    value_col=category_encoded_col_name\n",
    ")\n",
    "\n",
    "topic_mapping = create_article_id_to_value_mapping(\n",
    "    df=df_articles, \n",
    "    value_col=topics_encoded_col_name\n",
    ")\n",
    "# Create mappings for numerical features\n",
    "pageviews_mapping = create_article_id_to_value_mapping(\n",
    "    df=df_articles, \n",
    "    value_col=DEFAULT_TOTAL_PAGEVIEWS_COL\n",
    ")\n",
    "\n",
    "read_time_mapping = create_article_id_to_value_mapping(\n",
    "    df=df_articles, \n",
    "    value_col=DEFAULT_TOTAL_READ_TIME_COL\n",
    ")\n",
    "\n",
    "sentiment_mapping = create_article_id_to_value_mapping(\n",
    "    df=df_articles, \n",
    "    value_col=DEFAULT_SENTIMENT_SCORE_COL\n",
    ")\n",
    "\n",
    "timestamp_mapping = create_article_id_to_value_mapping(\n",
    "    df_articles,\n",
    "    value_col=DEFAULT_ARTICLE_PUBLISHED_TIMESTAMP_COL\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Init dataloaders\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting preprocessing...\n",
      "Preprocessing data...\n",
      "(23427, 7)\n",
      "Data preprocessing completed in 8.62 seconds.\n",
      "Starting preprocessing...\n",
      "Preprocessing data...\n",
      "(24332, 7)\n",
      "Data preprocessing completed in 7.67 seconds.\n"
     ]
    }
   ],
   "source": [
    "train_dataset = NRMSDataSet(\n",
    "    behaviors=df_train,\n",
    "    article_dict=article_mapping,\n",
    "    unknown_representation=\"zeros\",\n",
    "    history_column=DEFAULT_HISTORY_ARTICLE_ID_COL,\n",
    "    eval_mode=False,\n",
    "    category_mapping=category_mapping,\n",
    "    topic_mapping=topic_mapping,\n",
    "    pageviews_mapping=pageviews_mapping,\n",
    "    read_time_mapping=read_time_mapping,\n",
    "    sentiment_mapping=sentiment_mapping,\n",
    "    timestamp_mapping=timestamp_mapping\n",
    "\n",
    ")\n",
    "val_dataset = NRMSDataSet(\n",
    "    behaviors=df_validation,\n",
    "    article_dict=article_mapping,\n",
    "    unknown_representation=\"zeros\",\n",
    "    history_column=DEFAULT_HISTORY_ARTICLE_ID_COL,\n",
    "    eval_mode=True,\n",
    "    category_mapping=category_mapping,\n",
    "    topic_mapping=topic_mapping,\n",
    "    pageviews_mapping=pageviews_mapping,\n",
    "    read_time_mapping=read_time_mapping,\n",
    "    sentiment_mapping=sentiment_mapping,\n",
    "    timestamp_mapping=timestamp_mapping,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample 0:\n",
      "his_input_title shape: torch.Size([20, 300])\n",
      "pred_input_title shape: torch.Size([20, 128]) 0.0\n",
      "pred_input_category shape: torch.Size([20, 128]) 0.0\n",
      "pred_input_topic shape: torch.Size([20]) 0.0\n",
      "pred_input_pageviews shape: torch.Size([20]) 0.0\n",
      "Targets shape: torch.Size([5]) , torch.float32 1.0\n",
      "impression id: 111436368\n",
      "Sample 1:\n",
      "his_input_title shape: torch.Size([20, 300])\n",
      "pred_input_title shape: torch.Size([20, 128]) 0.0\n",
      "pred_input_category shape: torch.Size([20, 128]) 0.0\n",
      "pred_input_topic shape: torch.Size([20]) 0.0\n",
      "pred_input_pageviews shape: torch.Size([20]) 0.0\n",
      "Targets shape: torch.Size([5]) , torch.float32 1.0\n",
      "impression id: 245249797\n",
      "Sample 2:\n",
      "his_input_title shape: torch.Size([20, 300])\n",
      "pred_input_title shape: torch.Size([20, 128]) 0.0\n",
      "pred_input_category shape: torch.Size([20, 128]) 0.0\n",
      "pred_input_topic shape: torch.Size([20]) 0.0\n",
      "pred_input_pageviews shape: torch.Size([20]) 0.0\n",
      "Targets shape: torch.Size([5]) , torch.float32 1.0\n",
      "impression id: 576081382\n",
      "Sample 3:\n",
      "his_input_title shape: torch.Size([20, 300])\n",
      "pred_input_title shape: torch.Size([20, 128]) 0.0\n",
      "pred_input_category shape: torch.Size([20, 128]) 0.0\n",
      "pred_input_topic shape: torch.Size([20]) 0.0\n",
      "pred_input_pageviews shape: torch.Size([20]) 0.0\n",
      "Targets shape: torch.Size([5]) , torch.float32 1.0\n",
      "impression id: 218355780\n",
      "Sample 4:\n",
      "his_input_title shape: torch.Size([20, 300])\n",
      "pred_input_title shape: torch.Size([20, 128]) 0.0\n",
      "pred_input_category shape: torch.Size([20, 128]) 0.0\n",
      "pred_input_topic shape: torch.Size([20]) 0.0\n",
      "pred_input_pageviews shape: torch.Size([20]) 0.0\n",
      "Targets shape: torch.Size([5]) , torch.float32 1.0\n",
      "impression id: 39396736\n"
     ]
    }
   ],
   "source": [
    "for idx in range(5):\n",
    "    sample = train_dataset[idx]\n",
    "    print(f\"Sample {idx}:\")\n",
    "    print(f\"his_input_title shape: {sample[0][0].shape}\")\n",
    "    print(f\"pred_input_title shape: {sample[0][1].shape} {sample[0][1].sum()}\")\n",
    "    print(f\"pred_input_category shape: {sample[0][2].shape} {sample[0][2].sum()}\")\n",
    "    print(f\"pred_input_topic shape: {sample[0][3].shape} {sample[0][3].sum()}\")\n",
    "    print(f\"pred_input_pageviews shape: {sample[0][4].shape} {sample[0][4].sum()}\")\n",
    "    print(f\"Targets shape: {sample[1].shape} , {sample[1].dtype} {sample[1].sum()}\")\n",
    "    print(f\"impression id: {sample[2]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import torch\n",
    "\n",
    "def collate_fn_with_global_padding(batch, max_len_pred, apply_padding_to_targets=True):\n",
    "    try:\n",
    "        # Unpack all features including timestamps from dataset samples\n",
    "        his_input_titles = [item[0][0] for item in batch]\n",
    "        his_category_emb = [item[0][1] for item in batch]\n",
    "        his_topic_emb = [item[0][2] for item in batch]\n",
    "        his_sentiment_scores = [item[0][3] for item in batch]\n",
    "        his_read_times = [item[0][4] for item in batch]\n",
    "        his_pageviews = [item[0][5] for item in batch]\n",
    "        his_timestamps = [item[0][6] for item in batch]\n",
    "\n",
    "        pred_input_titles = [item[0][7] for item in batch]\n",
    "        pred_category_emb = [item[0][8] for item in batch]\n",
    "        pred_topic_emb = [item[0][9] for item in batch]\n",
    "        pred_sentiment_scores = [item[0][10] for item in batch]\n",
    "        pred_read_times = [item[0][11] for item in batch]\n",
    "        pred_pageviews = [item[0][12] for item in batch]\n",
    "        pred_timestamps = [item[0][13] for item in batch]\n",
    "        impression_timestamps = [item[0][14] for item in batch]\n",
    "\n",
    "        batch_ys = [item[1] for item in batch]\n",
    "        impression_id = torch.tensor([item[2] for item in batch], dtype=torch.int64)\n",
    "\n",
    "        # Pad user history features\n",
    "        his_input_titles_padded = pad_sequence(his_input_titles, batch_first=True, padding_value=0)\n",
    "        his_category_emb_padded = pad_sequence(his_category_emb, batch_first=True, padding_value=0)\n",
    "        his_topic_emb_padded = pad_sequence(his_topic_emb, batch_first=True, padding_value=0)\n",
    "        his_sentiment_padded = pad_sequence(his_sentiment_scores, batch_first=True, padding_value=0)\n",
    "        his_read_times_padded = pad_sequence(his_read_times, batch_first=True, padding_value=0)\n",
    "        his_pageviews_padded = pad_sequence(his_pageviews, batch_first=True, padding_value=0)\n",
    "        his_timestamps_padded = pad_sequence(his_timestamps, batch_first=True, padding_value=0)\n",
    "\n",
    "        # Pad candidate features\n",
    "        pred_input_titles_padded = pad_sequence(pred_input_titles, batch_first=True, padding_value=0)\n",
    "        pred_category_emb_padded = pad_sequence(pred_category_emb, batch_first=True, padding_value=0)\n",
    "        pred_topic_emb_padded = pad_sequence(pred_topic_emb, batch_first=True, padding_value=0)\n",
    "        pred_sentiment_padded = pad_sequence(pred_sentiment_scores, batch_first=True, padding_value=0)\n",
    "        pred_read_times_padded = pad_sequence(pred_read_times, batch_first=True, padding_value=0)\n",
    "        pred_pageviews_padded = pad_sequence(pred_pageviews, batch_first=True, padding_value=0)\n",
    "        pred_timestamps_padded = pad_sequence(pred_timestamps, batch_first=True, padding_value=0)\n",
    "        \n",
    "        # Convert impression timestamps to tensor\n",
    "        impression_timestamps = torch.stack(impression_timestamps)\n",
    "\n",
    "        # Handle max_len_pred for candidate sequences\n",
    "        if pred_input_titles_padded.size(1) < max_len_pred:\n",
    "            pad_size = max_len_pred - pred_input_titles_padded.size(1)\n",
    "            \n",
    "            pred_input_titles_padded = torch.nn.functional.pad(pred_input_titles_padded, (0, 0, 0, pad_size), value=0)\n",
    "            pred_category_emb_padded = torch.nn.functional.pad(pred_category_emb_padded, (0, 0, 0, pad_size), value=0)\n",
    "            pred_topic_emb_padded = torch.nn.functional.pad(pred_topic_emb_padded, (0, 0, 0, pad_size), value=0)\n",
    "            pred_sentiment_padded = torch.nn.functional.pad(pred_sentiment_padded, (0, pad_size), value=0)\n",
    "            pred_read_times_padded = torch.nn.functional.pad(pred_read_times_padded, (0, pad_size), value=0)\n",
    "            pred_pageviews_padded = torch.nn.functional.pad(pred_pageviews_padded, (0, pad_size), value=0)\n",
    "            pred_timestamps_padded = torch.nn.functional.pad(pred_timestamps_padded, (0, pad_size), value=0)\n",
    "\n",
    "        elif pred_input_titles_padded.size(1) > max_len_pred:\n",
    "            pred_input_titles_padded = pred_input_titles_padded[:, :max_len_pred, :]\n",
    "            pred_category_emb_padded = pred_category_emb_padded[:, :max_len_pred, :]\n",
    "            pred_topic_emb_padded = pred_topic_emb_padded[:, :max_len_pred, :]\n",
    "            pred_sentiment_padded = pred_sentiment_padded[:, :max_len_pred]\n",
    "            pred_read_times_padded = pred_read_times_padded[:, :max_len_pred]\n",
    "            pred_pageviews_padded = pred_pageviews_padded[:, :max_len_pred]\n",
    "            pred_timestamps_padded = pred_timestamps_padded[:, :max_len_pred]\n",
    "\n",
    "        # Handle targets padding\n",
    "        if apply_padding_to_targets:\n",
    "            batch_ys_padded = pad_sequence(batch_ys, batch_first=True, padding_value=-1)\n",
    "            if batch_ys_padded.size(1) < max_len_pred:\n",
    "                pad_size = max_len_pred - batch_ys_padded.size(1)\n",
    "                batch_ys_padded = torch.nn.functional.pad(batch_ys_padded, (0, pad_size), value=-1)\n",
    "            elif batch_ys_padded.size(1) > max_len_pred:\n",
    "                batch_ys_padded = batch_ys_padded[:, :max_len_pred]\n",
    "\n",
    "            return (\n",
    "                his_input_titles_padded,\n",
    "                his_category_emb_padded,\n",
    "                his_topic_emb_padded, \n",
    "                his_sentiment_padded,\n",
    "                his_read_times_padded,\n",
    "                his_pageviews_padded,\n",
    "                his_timestamps_padded,\n",
    "                pred_input_titles_padded,\n",
    "                pred_category_emb_padded,\n",
    "                pred_topic_emb_padded,\n",
    "                pred_sentiment_padded,\n",
    "                pred_read_times_padded,\n",
    "                pred_pageviews_padded,\n",
    "                pred_timestamps_padded,\n",
    "                impression_timestamps\n",
    "            ), batch_ys_padded, impression_id\n",
    "        else:\n",
    "            return (\n",
    "                his_input_titles_padded,\n",
    "                his_category_emb_padded,\n",
    "                his_topic_emb_padded,\n",
    "                his_sentiment_padded,\n",
    "                his_read_times_padded,\n",
    "                his_pageviews_padded,\n",
    "                his_timestamps_padded,\n",
    "                pred_input_titles_padded,\n",
    "                pred_category_emb_padded,\n",
    "                pred_topic_emb_padded,\n",
    "                pred_sentiment_padded,\n",
    "                pred_read_times_padded,\n",
    "                pred_pageviews_padded,\n",
    "                pred_timestamps_padded,\n",
    "                impression_timestamps\n",
    "            ), batch_ys, impression_id\n",
    "    except Exception as e:\n",
    "        print(f\"Error in collate_fn: {e}\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wrap the dataset with DataLoader\n",
    "train_dataloader_temp = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=32,    # Set your desired batch size\n",
    "    shuffle=True,\n",
    "    num_workers=0,\n",
    "    collate_fn=lambda batch: collate_fn_with_global_padding(batch, max_inview_length_train)\n",
    ")\n",
    "\n",
    "val_dataloader_temp = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=64,    # Set your desired batch size\n",
    "    shuffle=False,\n",
    "    num_workers=0,\n",
    "    collate_fn=lambda batch: collate_fn_with_global_padding(batch, max_inview_length_validation)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "History features shapes:\n",
      "his_input_titles: torch.Size([64, 20, 300])\n",
      "his_category_emb: torch.Size([64, 20, 128])\n",
      "his_topic_emb: torch.Size([64, 20, 128])\n",
      "his_sentiment: torch.Size([64, 20])\n",
      "his_read_times: torch.Size([64, 20])\n",
      "his_pageviews: torch.Size([64, 20])\n",
      "his_timestamps: torch.Size([64, 20])\n",
      "\n",
      "Candidate features shapes:\n",
      "pred_input_titles: torch.Size([64, 90, 300])\n",
      "pred_category_emb: torch.Size([64, 90, 128])\n",
      "pred_topic_emb: torch.Size([64, 90, 128])\n",
      "pred_sentiment: torch.Size([64, 90])\n",
      "pred_read_times: torch.Size([64, 90])\n",
      "pred_pageviews: torch.Size([64, 90])\n",
      "pred_timestamps: torch.Size([64, 90])\n",
      "\n",
      "Other tensors:\n",
      "impression_timestamps: torch.Size([64])\n",
      "batch_ys: torch.Size([64, 90])\n",
      "impression_id: torch.Size([64])\n",
      "\n",
      "Batch loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "for batch in val_dataloader_temp:\n",
    "    (\n",
    "        his_input_titles_padded, \n",
    "        his_category_emb_padded,\n",
    "        his_topic_emb_padded,\n",
    "        his_sentiment_padded,\n",
    "        his_read_times_padded,\n",
    "        his_pageviews_padded,\n",
    "        his_timestamps_padded,\n",
    "        pred_input_titles_padded,\n",
    "        pred_category_emb_padded,\n",
    "        pred_topic_emb_padded,\n",
    "        pred_sentiment_padded,\n",
    "        pred_read_times_padded,\n",
    "        pred_pageviews_padded,\n",
    "        pred_timestamps_padded,\n",
    "        impression_timestamps\n",
    "    ), batch_ys_padded, impression_id = batch\n",
    "\n",
    "    # Original tensor shapes\n",
    "    print(\"History features shapes:\")\n",
    "    print(f\"his_input_titles: {his_input_titles_padded.shape}\")\n",
    "    print(f\"his_category_emb: {his_category_emb_padded.shape}\")\n",
    "    print(f\"his_topic_emb: {his_topic_emb_padded.shape}\")\n",
    "    print(f\"his_sentiment: {his_sentiment_padded.shape}\")\n",
    "    print(f\"his_read_times: {his_read_times_padded.shape}\")\n",
    "    print(f\"his_pageviews: {his_pageviews_padded.shape}\")\n",
    "    print(f\"his_timestamps: {his_timestamps_padded.shape}\")\n",
    "\n",
    "    print(\"\\nCandidate features shapes:\")\n",
    "    print(f\"pred_input_titles: {pred_input_titles_padded.shape}\")\n",
    "    print(f\"pred_category_emb: {pred_category_emb_padded.shape}\")\n",
    "    print(f\"pred_topic_emb: {pred_topic_emb_padded.shape}\")\n",
    "    print(f\"pred_sentiment: {pred_sentiment_padded.shape}\")\n",
    "    print(f\"pred_read_times: {pred_read_times_padded.shape}\")\n",
    "    print(f\"pred_pageviews: {pred_pageviews_padded.shape}\")\n",
    "    print(f\"pred_timestamps: {pred_timestamps_padded.shape}\")\n",
    "    \n",
    "    print(\"\\nOther tensors:\")\n",
    "    print(f\"impression_timestamps: {impression_timestamps.shape}\")\n",
    "    print(f\"batch_ys: {batch_ys_padded.shape}\")\n",
    "    print(f\"impression_id: {impression_id.shape}\")\n",
    "\n",
    "    print(\"\\nBatch loaded successfully!\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explanation of the shapes.\n",
    "\n",
    "1. `his_input_titles_padded: [128, 20, 300]`\n",
    "   - 128: batch size\n",
    "   - 20: max history length (user's reading history)\n",
    "   - 300: title embedding dimension from ConvBERT\n",
    "\n",
    "2. `pred_input_titles_padded: [128, 90, 300]`\n",
    "   - 128: batch size\n",
    "   - 90: max candidate articles to predict\n",
    "   - 300: title embedding dimension from ConvBERT\n",
    "\n",
    "3. `pred_category_emb_padded: [128, 90, 128]`\n",
    "   - 128: batch size\n",
    "   - 90: max candidate articles\n",
    "   - 128: category embedding dimension from ConvBERT\n",
    "\n",
    "4. `pred_topic_emb_padded: [128, 90, 128]`\n",
    "   - 128: batch size\n",
    "   - 90: max candidate articles\n",
    "   - 128: topic embedding dimension from ConvBERT\n",
    "\n",
    "5. `pred_sentiment_padded: [128, 90]`\n",
    "   - 128: batch size\n",
    "   - 90: max candidate articles\n",
    "   - Single sentiment score per article\n",
    "\n",
    "6. `pred_read_times_padded: [128, 90]`\n",
    "   - 128: batch size\n",
    "   - 90: max candidate articles\n",
    "   - Single read time value per article\n",
    "\n",
    "7. `pred_pageviews_padded: [128, 90]`\n",
    "   - 128: batch size\n",
    "   - 90: max candidate articles\n",
    "   - Single pageview count per article\n",
    "\n",
    "8. `batch_ys_padded: [128, 90]`\n",
    "   - 128: batch size\n",
    "   - 90: max candidate articles\n",
    "   - Binary labels (1 for clicked, 0 for not clicked)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create hyper-parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'__module__': 'models_pytorch.model_config', '__annotations__': {'title_size': <class 'int'>, 'embedding_dim': <class 'int'>, 'word_emb_dim': <class 'int'>, 'vocab_size': <class 'int'>, 'head_num': <class 'int'>, 'head_dim': <class 'int'>, 'attention_hidden_dim': <class 'int'>, 'optimizer': <class 'str'>, 'loss': <class 'str'>, 'dropout': <class 'float'>, 'learning_rate': <class 'float'>, 'weight_decay': <class 'float'>, 'units_per_layer': list[int], 'use_category': <class 'bool'>, 'use_topic': <class 'bool'>, 'use_numeric': <class 'bool'>, 'use_session_discount': <class 'bool'>, 'use_publication_discount': <class 'bool'>, 'doc_out_dim': <class 'int'>, 'cat_out_dim': <class 'int'>, 'top_out_dim': <class 'int'>, 'numeric_proj_dim': <class 'int'>}, 'title_size': 300, 'embedding_dim': 32, 'word_emb_dim': 8, 'vocab_size': 10000, 'head_num': 16, 'head_dim': 128, 'attention_hidden_dim': 128, 'hidden_dim': 4, 'optimizer': 'adam', 'loss': 'cross_entropy_loss', 'dropout': 0.3, 'learning_rate': 0.0001, 'weight_decay': 0.006587407554797856, 'news_output_dim': 128, 'units_per_layer': [512, 256], 'use_category': True, 'use_topic': True, 'use_numeric': True, 'use_session_discount': True, 'use_publication_discount': True, 'doc_out_dim': 128, 'cat_out_dim': 128, 'top_out_dim': 128, 'numeric_proj_dim': 16, '__dict__': <attribute '__dict__' of 'hparams_nrms' objects>, '__weakref__': <attribute '__weakref__' of 'hparams_nrms' objects>, '__doc__': None}\n"
     ]
    }
   ],
   "source": [
    "# see the model parameters: \n",
    "hparams_nrms.attention_hidden_dim = 128\n",
    "hparams_nrms.title_size = 300\n",
    "hparams_nrms.head_num = 16\n",
    "hparams_nrms.head_dim = 128\n",
    "hparams_nrms.units_per_layer = [512,256]\n",
    "hparams_nrms.news_output_dim = 128\n",
    "hparams_nrms.dropout = 0.3\n",
    "hparams_nrms.learning_rate = 1e-4\n",
    "hparams_nrms.use_category = True\n",
    "hparams_nrms.use_topic = True\n",
    "hparams_nrms.use_numeric = True\n",
    "hparams_nrms.use_publication_discount = True\n",
    "hparams_nrms.use_session_discount = True\n",
    "print(hparams_nrms.__dict__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Best trial:\n",
    "  Value:  3.8567059058842696\n",
    "  Params: \n",
    "    head_num: 16\n",
    "    shared_dim: 99\n",
    "    dropout: 0.17498541169326048\n",
    "    learning_rate: 1.0454050068420554e-05\n",
    "    weight_decay: 0.006587407554797856\n",
    "    unit_layer_1: 421\n",
    "    unit_layer_2: 386\n",
    "    use_category: False\n",
    "    use_topic: False\n",
    "    use_numeric: True\n",
    "    use_publication_discount: False\n",
    "    use_session_discount: True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update hyperparameters with best trial results\n",
    "\n",
    "# Keep original parameters\n",
    "hparams_nrms.title_size = 300\n",
    "\n",
    "# Update with optimized parameters\n",
    "hparams_nrms.head_num = 4\n",
    "shared_dim = 140  # Rounded up 137 to nearest multiple of head_num\n",
    "hparams_nrms.head_dim = shared_dim\n",
    "hparams_nrms.attention_hidden_dim = shared_dim\n",
    "hparams_nrms.news_output_dim = shared_dim\n",
    "\n",
    "# Update layers\n",
    "hparams_nrms.units_per_layer = [257, 475]  # From optimization\n",
    "\n",
    "# Update learning parameters\n",
    "hparams_nrms.dropout = 0.208\n",
    "hparams_nrms.learning_rate = 8.486e-4\n",
    "hparams_nrms.weight_decay = 1.697e-3\n",
    "\n",
    "# Update feature flags based on optimization\n",
    "hparams_nrms.use_category = True\n",
    "hparams_nrms.use_topic = False\n",
    "hparams_nrms.use_numeric = False\n",
    "hparams_nrms.use_publication_discount = False\n",
    "hparams_nrms.use_session_discount = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update hyperparameters with best trial results\n",
    "\n",
    "# Keep original parameters\n",
    "hparams_nrms.title_size = 300\n",
    "\n",
    "# Update with optimized parameters\n",
    "hparams_nrms.head_num = 16\n",
    "shared_dim = 128  # Rounded up 137 to nearest multiple of head_num\n",
    "hparams_nrms.head_dim = shared_dim\n",
    "hparams_nrms.attention_hidden_dim = shared_dim\n",
    "hparams_nrms.news_output_dim = shared_dim\n",
    "\n",
    "# Update layers\n",
    "hparams_nrms.units_per_layer = [421, 386]  # From optimization\n",
    "\n",
    "# Update learning parameters\n",
    "hparams_nrms.dropout = 0.1749\n",
    "hparams_nrms.learning_rate = 1.0454050068420554e-5\n",
    "hparams_nrms.weight_decay = 0.006587407554797856\n",
    "\n",
    "# Update feature flags based on optimization\n",
    "hparams_nrms.use_category = True\n",
    "hparams_nrms.use_topic = True\n",
    "hparams_nrms.use_numeric = False\n",
    "hparams_nrms.use_publication_discount = True\n",
    "hparams_nrms.use_session_discount = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hyper optimization \n",
    "\n",
    "Best trial:\n",
    "  Value:  2.2587267407595015\n",
    "  Params: \n",
    "    head_num: 32\n",
    "    use_extra_layer: False\n",
    "    shared_dim: 197\n",
    "    unit_layer_1: 262\n",
    "    unit_layer_2: 84\n",
    "    dropout: 0.262803257540282\n",
    "    learning_rate: 0.00013197622102333815\n",
    "    weight_decay: 0.00016061208774452676"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "192\n"
     ]
    }
   ],
   "source": [
    "shared_dim = (197 // 32) * 32\n",
    "print(shared_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update hyperparameters with best trial results\n",
    "\n",
    "# Keep original parameters\n",
    "hparams_nrms.title_size = 300\n",
    "\n",
    "# Update with optimized parameters\n",
    "hparams_nrms.head_num = 32\n",
    "shared_dim = 192  # Rounded up 137 to nearest multiple of head_num\n",
    "hparams_nrms.head_dim = shared_dim\n",
    "hparams_nrms.attention_hidden_dim = shared_dim\n",
    "hparams_nrms.news_output_dim = shared_dim\n",
    "\n",
    "# Update layers\n",
    "hparams_nrms.units_per_layer = [262, 84]  # From optimization\n",
    "\n",
    "# Update learning parameters\n",
    "hparams_nrms.dropout = 0.262803257540282\n",
    "hparams_nrms.learning_rate = 1.0454050068420554e-5\n",
    "hparams_nrms.weight_decay = 0.00016061208774452676\n",
    "\n",
    "# Update feature flags based on optimization\n",
    "hparams_nrms.use_category = True\n",
    "hparams_nrms.use_topic = True\n",
    "hparams_nrms.use_numeric = False\n",
    "hparams_nrms.use_publication_discount = True\n",
    "hparams_nrms.use_session_discount = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the NRMS model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Model device: cuda:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\gusta\\Documents\\Deeplearning-RecSys-Challenge-2024\\models_pytorch\\NRMSDocVecModel.py:419: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  self.scaler = amp.GradScaler()\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "# Define paths\n",
    "MODEL_NAME = \"NRMS\"\n",
    "LOG_DIR = os.path.join(\"downloads\", \"runs\", MODEL_NAME)\n",
    "MODEL_WEIGHTS = os.path.join(\"downloads\", \"data\", \"state_dict\", MODEL_NAME, \"weights.pth\")\n",
    "\n",
    "# Create directories if they don't exist\n",
    "os.makedirs(LOG_DIR, exist_ok=True)\n",
    "os.makedirs(os.path.dirname(MODEL_WEIGHTS), exist_ok=True)\n",
    "\n",
    "# Define ModelCheckpoint class\n",
    "class ModelCheckpoint:\n",
    "    \"\"\"Saves the model after every epoch if it has the best performance so far.\"\"\"\n",
    "    def __init__(self, filepath, verbose=False, save_best_only=True):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            filepath (str): Path to save the model checkpoint.\n",
    "            verbose (bool): If True, prints a message when the model is saved.\n",
    "            save_best_only (bool): If True, saves only when the model is better than before.\n",
    "        \"\"\"\n",
    "        self.filepath = filepath\n",
    "        self.verbose = verbose\n",
    "        self.save_best_only = save_best_only\n",
    "        self.best_loss = None\n",
    "\n",
    "    def __call__(self, model, val_loss):\n",
    "        if self.best_loss is None:\n",
    "            self.best_loss = val_loss\n",
    "            self.save_checkpoint(model)\n",
    "        elif val_loss < self.best_loss:\n",
    "            self.best_loss = val_loss\n",
    "            self.save_checkpoint(model)\n",
    "\n",
    "    def save_checkpoint(self, model):\n",
    "        torch.save(model.state_dict(), self.filepath)\n",
    "        if self.verbose:\n",
    "            print(f\"Model saved to {self.filepath}\")\n",
    "\n",
    "# Define EarlyStopping class\n",
    "class EarlyStopping:\n",
    "    \"\"\"Early stops the training if validation loss doesn't improve by a given percentage over a patience period.\"\"\"\n",
    "    def __init__(self, patience=3, min_delta=0.05, verbose=False):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            patience (int): Number of epochs to wait after last time validation loss improved by min_delta.\n",
    "            min_delta (float): Minimum percentage improvement required to reset patience.\n",
    "            verbose (bool): If True, prints a message when early stopping is triggered.\n",
    "        \"\"\"\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta  # Minimum percentage improvement\n",
    "        self.verbose = verbose\n",
    "        self.counter = 0\n",
    "        self.best_loss = None\n",
    "        self.early_stop = False\n",
    "\n",
    "    def __call__(self, val_loss):\n",
    "        if self.best_loss is None:\n",
    "            # Initialize best_loss with the first validation loss\n",
    "            self.best_loss = val_loss\n",
    "            self.counter = 0\n",
    "        elif val_loss < self.best_loss * (1 - self.min_delta):\n",
    "            # Significant improvement found\n",
    "            self.best_loss = val_loss\n",
    "            self.counter = 0\n",
    "            if self.verbose:\n",
    "                print(f\"Validation loss improved by at least {self.min_delta*100:.1f}%\")\n",
    "        else:\n",
    "            # No significant improvement\n",
    "            self.counter += 1\n",
    "            if self.verbose:\n",
    "                print(f\"No significant improvement in validation loss. Counter: {self.counter}/{self.patience}\")\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "\n",
    "# Initialize TensorBoard SummaryWriter\n",
    "writer = SummaryWriter(log_dir=LOG_DIR)\n",
    "\n",
    "# Initialize callbacks\n",
    "model_checkpoint = ModelCheckpoint(filepath=MODEL_WEIGHTS, verbose=True, save_best_only=True)\n",
    "# Initialize EarlyStopping\n",
    "early_stopping = EarlyStopping(patience=3, min_delta=0.05, verbose=True)\n",
    "\n",
    "# Initialize your model\n",
    "# Ensure that NRMSModel is a PyTorch nn.Module\n",
    "\n",
    "# CUDA checks\n",
    "#print(f\"CUDA Available: {torch.cuda.is_available()}\")\n",
    "#print(f\"Current Device: {torch.cuda.current_device()}\")\n",
    "#print(f\"Device Name: {torch.cuda.get_device_name()}\")\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    \n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Initialize model\n",
    "# model = NRMSModel(\n",
    "#     hparams=hparams_nrms.__dict__,\n",
    "#     word2vec_embedding=word2vec_embedding,\n",
    "#     vocab_size=30000,\n",
    "#     word_emb_dim=8,\n",
    "#     device=device,\n",
    "#     feed_forward_layers_after_3rd_layer=False,\n",
    "# )\n",
    "\n",
    "model = NRMSDocVecModel(hparams=hparams_nrms.__dict__,\n",
    "                        device=device)\n",
    "\n",
    "# model = NRMSModel(hparams=hparams_nrms.__dict__,\n",
    "#                   word2vec_embedding=word2vec_embedding,\n",
    "#                         device=device)\n",
    "print(f\"Model device: {next(model.parameters()).device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NRMSDocVecModel(\n",
      "  (newsencoder): NewsEncoderDocVec(\n",
      "    (doc_encoder): DocEncoder(\n",
      "      (layer): Sequential(\n",
      "        (0): Linear(in_features=300, out_features=128, bias=True)\n",
      "        (1): ReLU()\n",
      "        (2): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (3): Dropout(p=0.1749, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (category_encoder): CategoryEncoder(\n",
      "      (layer): Sequential(\n",
      "        (0): Linear(in_features=128, out_features=128, bias=True)\n",
      "        (1): ReLU()\n",
      "        (2): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (3): Dropout(p=0.1749, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (topic_encoder): TopicEncoder(\n",
      "      (layer): Sequential(\n",
      "        (0): Linear(in_features=128, out_features=128, bias=True)\n",
      "        (1): ReLU()\n",
      "        (2): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (3): Dropout(p=0.1749, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (feature_fusion): FeatureFusion(\n",
      "      (linears): ModuleList(\n",
      "        (0-2): 3 x Linear(in_features=128, out_features=128, bias=True)\n",
      "      )\n",
      "      (gates): ModuleList(\n",
      "        (0-2): 3 x Linear(in_features=128, out_features=128, bias=True)\n",
      "      )\n",
      "      (activation): Sigmoid()\n",
      "    )\n",
      "    (layer_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "    (time_discount): TimeDiscount(\n",
      "      (layer): Sequential(\n",
      "        (0): Linear(in_features=1, out_features=1, bias=True)\n",
      "        (1): Sigmoid()\n",
      "      )\n",
      "    )\n",
      "    (residual_blocks): ModuleList(\n",
      "      (0): Sequential(\n",
      "        (0): Linear(in_features=128, out_features=421, bias=True)\n",
      "        (1): ReLU()\n",
      "        (2): LayerNorm((421,), eps=1e-05, elementwise_affine=True)\n",
      "        (3): Dropout(p=0.1749, inplace=False)\n",
      "        (4): Linear(in_features=421, out_features=128, bias=True)\n",
      "      )\n",
      "      (1): Sequential(\n",
      "        (0): Linear(in_features=128, out_features=386, bias=True)\n",
      "        (1): ReLU()\n",
      "        (2): LayerNorm((386,), eps=1e-05, elementwise_affine=True)\n",
      "        (3): Dropout(p=0.1749, inplace=False)\n",
      "        (4): Linear(in_features=386, out_features=128, bias=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (userencoder): UserEncoderDocVec(\n",
      "    (titleencoder): NewsEncoderDocVec(\n",
      "      (doc_encoder): DocEncoder(\n",
      "        (layer): Sequential(\n",
      "          (0): Linear(in_features=300, out_features=128, bias=True)\n",
      "          (1): ReLU()\n",
      "          (2): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (3): Dropout(p=0.1749, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (category_encoder): CategoryEncoder(\n",
      "        (layer): Sequential(\n",
      "          (0): Linear(in_features=128, out_features=128, bias=True)\n",
      "          (1): ReLU()\n",
      "          (2): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (3): Dropout(p=0.1749, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (topic_encoder): TopicEncoder(\n",
      "        (layer): Sequential(\n",
      "          (0): Linear(in_features=128, out_features=128, bias=True)\n",
      "          (1): ReLU()\n",
      "          (2): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (3): Dropout(p=0.1749, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (feature_fusion): FeatureFusion(\n",
      "        (linears): ModuleList(\n",
      "          (0-2): 3 x Linear(in_features=128, out_features=128, bias=True)\n",
      "        )\n",
      "        (gates): ModuleList(\n",
      "          (0-2): 3 x Linear(in_features=128, out_features=128, bias=True)\n",
      "        )\n",
      "        (activation): Sigmoid()\n",
      "      )\n",
      "      (layer_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "      (time_discount): TimeDiscount(\n",
      "        (layer): Sequential(\n",
      "          (0): Linear(in_features=1, out_features=1, bias=True)\n",
      "          (1): Sigmoid()\n",
      "        )\n",
      "      )\n",
      "      (residual_blocks): ModuleList(\n",
      "        (0): Sequential(\n",
      "          (0): Linear(in_features=128, out_features=421, bias=True)\n",
      "          (1): ReLU()\n",
      "          (2): LayerNorm((421,), eps=1e-05, elementwise_affine=True)\n",
      "          (3): Dropout(p=0.1749, inplace=False)\n",
      "          (4): Linear(in_features=421, out_features=128, bias=True)\n",
      "        )\n",
      "        (1): Sequential(\n",
      "          (0): Linear(in_features=128, out_features=386, bias=True)\n",
      "          (1): ReLU()\n",
      "          (2): LayerNorm((386,), eps=1e-05, elementwise_affine=True)\n",
      "          (3): Dropout(p=0.1749, inplace=False)\n",
      "          (4): Linear(in_features=386, out_features=128, bias=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (self_attention): SelfAttention(\n",
      "      (multihead_attention): MultiheadAttention(\n",
      "        (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
      "      )\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (time_gate): TimeGate(\n",
      "      (gate_network): Sequential(\n",
      "        (0): Linear(in_features=129, out_features=128, bias=True)\n",
      "        (1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "        (2): ReLU()\n",
      "        (3): Linear(in_features=128, out_features=128, bias=True)\n",
      "        (4): Sigmoid()\n",
      "      )\n",
      "    )\n",
      "    (time_aware_attention): TimeAwareAttention(\n",
      "      (time_mlp): Sequential(\n",
      "        (0): Linear(in_features=1, out_features=128, bias=True)\n",
      "        (1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "        (2): ReLU()\n",
      "        (3): Linear(in_features=128, out_features=128, bias=True)\n",
      "        (4): Tanh()\n",
      "      )\n",
      "      (attention): Sequential(\n",
      "        (0): Linear(in_features=256, out_features=128, bias=True)\n",
      "        (1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "        (2): ReLU()\n",
      "        (3): Linear(in_features=128, out_features=1, bias=True)\n",
      "      )\n",
      "    )\n",
      "    (recency_enhancer): RecencyEnhancer()\n",
      "    (attention_layer): AttLayer2(\n",
      "      (V_w): Linear(in_features=128, out_features=128, bias=True)\n",
      "      (q_w): Linear(in_features=128, out_features=1, bias=False)\n",
      "    )\n",
      "    (user_projection): Linear(in_features=128, out_features=128, bias=True)\n",
      "    (layer_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "    (time_discount): TimeDiscount(\n",
      "      (layer): Sequential(\n",
      "        (0): Linear(in_features=1, out_features=1, bias=True)\n",
      "        (1): Sigmoid()\n",
      "      )\n",
      "    )\n",
      "  )\n",
      ")\n",
      "Layer: newsencoder.doc_encoder.layer.0.weight | Size: torch.Size([128, 300])\n",
      "Layer: newsencoder.doc_encoder.layer.0.bias | Size: torch.Size([128])\n",
      "Layer: newsencoder.doc_encoder.layer.2.weight | Size: torch.Size([128])\n",
      "Layer: newsencoder.doc_encoder.layer.2.bias | Size: torch.Size([128])\n",
      "Layer: newsencoder.category_encoder.layer.0.weight | Size: torch.Size([128, 128])\n",
      "Layer: newsencoder.category_encoder.layer.0.bias | Size: torch.Size([128])\n",
      "Layer: newsencoder.category_encoder.layer.2.weight | Size: torch.Size([128])\n",
      "Layer: newsencoder.category_encoder.layer.2.bias | Size: torch.Size([128])\n",
      "Layer: newsencoder.topic_encoder.layer.0.weight | Size: torch.Size([128, 128])\n",
      "Layer: newsencoder.topic_encoder.layer.0.bias | Size: torch.Size([128])\n",
      "Layer: newsencoder.topic_encoder.layer.2.weight | Size: torch.Size([128])\n",
      "Layer: newsencoder.topic_encoder.layer.2.bias | Size: torch.Size([128])\n",
      "Layer: newsencoder.feature_fusion.linears.0.weight | Size: torch.Size([128, 128])\n",
      "Layer: newsencoder.feature_fusion.linears.0.bias | Size: torch.Size([128])\n",
      "Layer: newsencoder.feature_fusion.linears.1.weight | Size: torch.Size([128, 128])\n",
      "Layer: newsencoder.feature_fusion.linears.1.bias | Size: torch.Size([128])\n",
      "Layer: newsencoder.feature_fusion.linears.2.weight | Size: torch.Size([128, 128])\n",
      "Layer: newsencoder.feature_fusion.linears.2.bias | Size: torch.Size([128])\n",
      "Layer: newsencoder.feature_fusion.gates.0.weight | Size: torch.Size([128, 128])\n",
      "Layer: newsencoder.feature_fusion.gates.0.bias | Size: torch.Size([128])\n",
      "Layer: newsencoder.feature_fusion.gates.1.weight | Size: torch.Size([128, 128])\n",
      "Layer: newsencoder.feature_fusion.gates.1.bias | Size: torch.Size([128])\n",
      "Layer: newsencoder.feature_fusion.gates.2.weight | Size: torch.Size([128, 128])\n",
      "Layer: newsencoder.feature_fusion.gates.2.bias | Size: torch.Size([128])\n",
      "Layer: newsencoder.layer_norm.weight | Size: torch.Size([128])\n",
      "Layer: newsencoder.layer_norm.bias | Size: torch.Size([128])\n",
      "Layer: newsencoder.time_discount.layer.0.weight | Size: torch.Size([1, 1])\n",
      "Layer: newsencoder.time_discount.layer.0.bias | Size: torch.Size([1])\n",
      "Layer: newsencoder.residual_blocks.0.0.weight | Size: torch.Size([421, 128])\n",
      "Layer: newsencoder.residual_blocks.0.0.bias | Size: torch.Size([421])\n",
      "Layer: newsencoder.residual_blocks.0.2.weight | Size: torch.Size([421])\n",
      "Layer: newsencoder.residual_blocks.0.2.bias | Size: torch.Size([421])\n",
      "Layer: newsencoder.residual_blocks.0.4.weight | Size: torch.Size([128, 421])\n",
      "Layer: newsencoder.residual_blocks.0.4.bias | Size: torch.Size([128])\n",
      "Layer: newsencoder.residual_blocks.1.0.weight | Size: torch.Size([386, 128])\n",
      "Layer: newsencoder.residual_blocks.1.0.bias | Size: torch.Size([386])\n",
      "Layer: newsencoder.residual_blocks.1.2.weight | Size: torch.Size([386])\n",
      "Layer: newsencoder.residual_blocks.1.2.bias | Size: torch.Size([386])\n",
      "Layer: newsencoder.residual_blocks.1.4.weight | Size: torch.Size([128, 386])\n",
      "Layer: newsencoder.residual_blocks.1.4.bias | Size: torch.Size([128])\n",
      "Layer: userencoder.self_attention.multihead_attention.in_proj_weight | Size: torch.Size([384, 128])\n",
      "Layer: userencoder.self_attention.multihead_attention.in_proj_bias | Size: torch.Size([384])\n",
      "Layer: userencoder.self_attention.multihead_attention.out_proj.weight | Size: torch.Size([128, 128])\n",
      "Layer: userencoder.self_attention.multihead_attention.out_proj.bias | Size: torch.Size([128])\n",
      "Layer: userencoder.time_gate.gate_network.0.weight | Size: torch.Size([128, 129])\n",
      "Layer: userencoder.time_gate.gate_network.0.bias | Size: torch.Size([128])\n",
      "Layer: userencoder.time_gate.gate_network.1.weight | Size: torch.Size([128])\n",
      "Layer: userencoder.time_gate.gate_network.1.bias | Size: torch.Size([128])\n",
      "Layer: userencoder.time_gate.gate_network.3.weight | Size: torch.Size([128, 128])\n",
      "Layer: userencoder.time_gate.gate_network.3.bias | Size: torch.Size([128])\n",
      "Layer: userencoder.time_aware_attention.time_mlp.0.weight | Size: torch.Size([128, 1])\n",
      "Layer: userencoder.time_aware_attention.time_mlp.0.bias | Size: torch.Size([128])\n",
      "Layer: userencoder.time_aware_attention.time_mlp.1.weight | Size: torch.Size([128])\n",
      "Layer: userencoder.time_aware_attention.time_mlp.1.bias | Size: torch.Size([128])\n",
      "Layer: userencoder.time_aware_attention.time_mlp.3.weight | Size: torch.Size([128, 128])\n",
      "Layer: userencoder.time_aware_attention.time_mlp.3.bias | Size: torch.Size([128])\n",
      "Layer: userencoder.time_aware_attention.attention.0.weight | Size: torch.Size([128, 256])\n",
      "Layer: userencoder.time_aware_attention.attention.0.bias | Size: torch.Size([128])\n",
      "Layer: userencoder.time_aware_attention.attention.1.weight | Size: torch.Size([128])\n",
      "Layer: userencoder.time_aware_attention.attention.1.bias | Size: torch.Size([128])\n",
      "Layer: userencoder.time_aware_attention.attention.3.weight | Size: torch.Size([1, 128])\n",
      "Layer: userencoder.time_aware_attention.attention.3.bias | Size: torch.Size([1])\n",
      "Layer: userencoder.attention_layer.V_w.weight | Size: torch.Size([128, 128])\n",
      "Layer: userencoder.attention_layer.V_w.bias | Size: torch.Size([128])\n",
      "Layer: userencoder.attention_layer.q_w.weight | Size: torch.Size([1, 128])\n",
      "Layer: userencoder.user_projection.weight | Size: torch.Size([128, 128])\n",
      "Layer: userencoder.user_projection.bias | Size: torch.Size([128])\n",
      "Layer: userencoder.layer_norm.weight | Size: torch.Size([128])\n",
      "Layer: userencoder.layer_norm.bias | Size: torch.Size([128])\n",
      "Layer: userencoder.time_discount.layer.0.weight | Size: torch.Size([1, 1])\n",
      "Layer: userencoder.time_discount.layer.0.bias | Size: torch.Size([1])\n",
      "\n",
      "Total parameters: 564,090\n"
     ]
    }
   ],
   "source": [
    "# 1. Print model architecture\n",
    "print(model)\n",
    "\n",
    "# 2. Print specific layer sizes\n",
    "for name, param in model.named_parameters():\n",
    "    print(f\"Layer: {name} | Size: {param.size()}\")\n",
    "\n",
    "# 3. Get total number of parameters\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"\\nTotal parameters: {total_params:,}\")\n",
    "\n",
    "# 4. Print layer by layer with shapes\n",
    "def print_model_structure(model):\n",
    "    print(\"\\nDetailed Model Structure:\")\n",
    "    for name, module in model.named_children():\n",
    "        print(f\"\\nLayer: {name}\")\n",
    "        print(f\"Type: {type(module).__name__}\")\n",
    "        if hasattr(module, 'weight'):\n",
    "            print(f\"Shape: {module.weight.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###\n",
    "Added gradient clipping to avoid exploiding gradients. The paramter MAX_GRAD_NORM is set to 5.0 as this is a common value used in transformers. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperoptimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-21 11:51:29,198] A new study created in memory with name: no-name-0da6fc5b-b98e-4213-9910-9afabcf684a5\n",
      "c:\\Users\\gusta\\Documents\\Deeplearning-RecSys-Challenge-2024\\.venv\\Lib\\site-packages\\optuna\\trial\\_trial.py:678: RuntimeWarning: Inconsistent parameter values for distribution with name \"unit_layer_1\"! This might be a configuration mistake. Optuna allows to call the same distribution with the same name more than once in a trial. When the parameter values are inconsistent optuna only uses the values of the first call and ignores all following. Using these values: {'log': False, 'step': 1, 'low': 64, 'high': 512}\n",
      "  warnings.warn(\n",
      "c:\\Users\\gusta\\Documents\\Deeplearning-RecSys-Challenge-2024\\.venv\\Lib\\site-packages\\optuna\\trial\\_trial.py:678: RuntimeWarning: Inconsistent parameter values for distribution with name \"unit_layer_2\"! This might be a configuration mistake. Optuna allows to call the same distribution with the same name more than once in a trial. When the parameter values are inconsistent optuna only uses the values of the first call and ignores all following. Using these values: {'log': False, 'step': 1, 'low': 64, 'high': 512}\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss improved by at least 2.5%\n",
      "Validation loss improved by at least 2.5%\n",
      "No significant improvement in validation loss. Counter: 1/4\n",
      "No significant improvement in validation loss. Counter: 2/4\n",
      "No significant improvement in validation loss. Counter: 3/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-21 11:54:21,712] Trial 0 finished with value: 2.6253080862400724 and parameters: {'head_num': 32, 'use_extra_layer': False, 'shared_dim': 206, 'unit_layer_1': 275, 'unit_layer_2': 159, 'dropout': 0.20780807344081317, 'learning_rate': 0.0001415219778173765, 'weight_decay': 0.00010950044554118308}. Best is trial 0 with value: 2.6253080862400724.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No significant improvement in validation loss. Counter: 4/4\n",
      "Validation loss improved by at least 2.5%\n",
      "Validation loss improved by at least 2.5%\n",
      "Validation loss improved by at least 2.5%\n",
      "Validation loss improved by at least 2.5%\n",
      "Validation loss improved by at least 2.5%\n",
      "Validation loss improved by at least 2.5%\n",
      "Validation loss improved by at least 2.5%\n",
      "No significant improvement in validation loss. Counter: 1/4\n",
      "Validation loss improved by at least 2.5%\n",
      "Validation loss improved by at least 2.5%\n",
      "No significant improvement in validation loss. Counter: 1/4\n",
      "No significant improvement in validation loss. Counter: 2/4\n",
      "No significant improvement in validation loss. Counter: 3/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-21 12:00:40,723] Trial 1 finished with value: 5.65117333004168 and parameters: {'head_num': 2, 'use_extra_layer': True, 'shared_dim': 72, 'unit_layer_1': 350, 'unit_layer_2': 383, 'unit_layer_3': 120, 'dropout': 0.41516977714252434, 'learning_rate': 3.0425629954330524e-06, 'weight_decay': 0.0009401969233108921}. Best is trial 0 with value: 2.6253080862400724.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No significant improvement in validation loss. Counter: 4/4\n",
      "Validation loss improved by at least 2.5%\n",
      "No significant improvement in validation loss. Counter: 1/4\n",
      "Validation loss improved by at least 2.5%\n",
      "No significant improvement in validation loss. Counter: 1/4\n",
      "No significant improvement in validation loss. Counter: 2/4\n",
      "Validation loss improved by at least 2.5%\n",
      "No significant improvement in validation loss. Counter: 1/4\n",
      "No significant improvement in validation loss. Counter: 2/4\n",
      "No significant improvement in validation loss. Counter: 3/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-21 12:05:40,774] Trial 2 finished with value: 2.4107168465461633 and parameters: {'head_num': 32, 'use_extra_layer': True, 'shared_dim': 101, 'unit_layer_1': 383, 'unit_layer_2': 328, 'unit_layer_3': 167, 'dropout': 0.4047341855709433, 'learning_rate': 6.575786051240232e-05, 'weight_decay': 0.00929483248472294}. Best is trial 2 with value: 2.4107168465461633.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No significant improvement in validation loss. Counter: 4/4\n",
      "No significant improvement in validation loss. Counter: 1/4\n",
      "No significant improvement in validation loss. Counter: 2/4\n",
      "No significant improvement in validation loss. Counter: 3/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-21 12:07:47,059] Trial 3 finished with value: 8.263712447459303 and parameters: {'head_num': 32, 'use_extra_layer': False, 'shared_dim': 203, 'unit_layer_1': 359, 'unit_layer_2': 300, 'dropout': 0.3264797389050114, 'learning_rate': 4.141085934093144e-05, 'weight_decay': 0.006985523058897579}. Best is trial 2 with value: 2.4107168465461633.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No significant improvement in validation loss. Counter: 4/4\n",
      "No significant improvement in validation loss. Counter: 1/4\n",
      "Validation loss improved by at least 2.5%\n",
      "Validation loss improved by at least 2.5%\n",
      "Validation loss improved by at least 2.5%\n",
      "Validation loss improved by at least 2.5%\n",
      "No significant improvement in validation loss. Counter: 1/4\n",
      "Validation loss improved by at least 2.5%\n",
      "No significant improvement in validation loss. Counter: 1/4\n",
      "No significant improvement in validation loss. Counter: 2/4\n",
      "Validation loss improved by at least 2.5%\n",
      "Validation loss improved by at least 2.5%\n",
      "Validation loss improved by at least 2.5%\n",
      "Validation loss improved by at least 2.5%\n",
      "Validation loss improved by at least 2.5%\n",
      "No significant improvement in validation loss. Counter: 1/4\n",
      "Validation loss improved by at least 2.5%\n",
      "No significant improvement in validation loss. Counter: 1/4\n",
      "No significant improvement in validation loss. Counter: 2/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-21 12:16:37,164] Trial 4 finished with value: 7.70556907453562 and parameters: {'head_num': 2, 'use_extra_layer': False, 'shared_dim': 176, 'unit_layer_1': 181, 'unit_layer_2': 205, 'dropout': 0.4491078155350421, 'learning_rate': 1.8414186407393578e-06, 'weight_decay': 0.000262038264147566}. Best is trial 2 with value: 2.4107168465461633.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No significant improvement in validation loss. Counter: 3/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-21 12:17:03,615] Trial 5 pruned. \n",
      "[I 2024-12-21 12:17:28,591] Trial 6 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No significant improvement in validation loss. Counter: 1/4\n",
      "Validation loss improved by at least 2.5%\n",
      "Validation loss improved by at least 2.5%\n",
      "No significant improvement in validation loss. Counter: 1/4\n",
      "No significant improvement in validation loss. Counter: 2/4\n",
      "No significant improvement in validation loss. Counter: 3/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-21 12:20:47,341] Trial 7 finished with value: 3.818657370689973 and parameters: {'head_num': 32, 'use_extra_layer': False, 'shared_dim': 237, 'unit_layer_1': 373, 'unit_layer_2': 502, 'dropout': 0.2577201943313121, 'learning_rate': 0.0007361997371870954, 'weight_decay': 0.0030479505458630364}. Best is trial 2 with value: 2.4107168465461633.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No significant improvement in validation loss. Counter: 4/4\n",
      "No significant improvement in validation loss. Counter: 1/4\n",
      "No significant improvement in validation loss. Counter: 2/4\n",
      "Validation loss improved by at least 2.5%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-21 12:23:05,049] Trial 8 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss improved by at least 2.5%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-21 12:23:38,578] Trial 9 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No significant improvement in validation loss. Counter: 1/4\n",
      "Validation loss improved by at least 2.5%\n",
      "Validation loss improved by at least 2.5%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-21 12:26:23,056] Trial 10 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No significant improvement in validation loss. Counter: 1/4\n",
      "Validation loss improved by at least 2.5%\n",
      "Validation loss improved by at least 2.5%\n",
      "No significant improvement in validation loss. Counter: 1/4\n",
      "Validation loss improved by at least 2.5%\n",
      "Validation loss improved by at least 2.5%\n",
      "No significant improvement in validation loss. Counter: 1/4\n",
      "No significant improvement in validation loss. Counter: 2/4\n",
      "No significant improvement in validation loss. Counter: 3/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-21 12:30:35,749] Trial 11 finished with value: 2.2587267407595015 and parameters: {'head_num': 32, 'use_extra_layer': False, 'shared_dim': 197, 'unit_layer_1': 262, 'unit_layer_2': 84, 'dropout': 0.262803257540282, 'learning_rate': 0.00013197622102333815, 'weight_decay': 0.00016061208774452676}. Best is trial 11 with value: 2.2587267407595015.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No significant improvement in validation loss. Counter: 4/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-21 12:30:59,393] Trial 12 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No significant improvement in validation loss. Counter: 1/4\n",
      "No significant improvement in validation loss. Counter: 2/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-21 12:32:31,566] Trial 13 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No significant improvement in validation loss. Counter: 3/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-21 12:32:55,232] Trial 14 pruned. \n",
      "[I 2024-12-21 12:33:18,072] Trial 15 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss improved by at least 2.5%\n",
      "No significant improvement in validation loss. Counter: 1/4\n",
      "No significant improvement in validation loss. Counter: 2/4\n",
      "No significant improvement in validation loss. Counter: 3/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-21 12:35:36,313] Trial 16 finished with value: 2.3532427195801824 and parameters: {'head_num': 32, 'use_extra_layer': False, 'shared_dim': 154, 'unit_layer_1': 428, 'unit_layer_2': 69, 'dropout': 0.2741565380493835, 'learning_rate': 0.0003326024677777736, 'weight_decay': 0.0003251121256304186}. Best is trial 11 with value: 2.2587267407595015.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No significant improvement in validation loss. Counter: 4/4\n",
      "Validation loss improved by at least 2.5%\n",
      "No significant improvement in validation loss. Counter: 1/4\n",
      "No significant improvement in validation loss. Counter: 2/4\n",
      "No significant improvement in validation loss. Counter: 3/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-21 12:37:58,004] Trial 17 finished with value: 2.371160731228005 and parameters: {'head_num': 32, 'use_extra_layer': False, 'shared_dim': 159, 'unit_layer_1': 429, 'unit_layer_2': 76, 'dropout': 0.23873176612307184, 'learning_rate': 0.00031985319652846986, 'weight_decay': 0.0002284766433409422}. Best is trial 11 with value: 2.2587267407595015.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No significant improvement in validation loss. Counter: 4/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-21 12:38:21,387] Trial 18 pruned. \n",
      "[I 2024-12-21 12:38:45,452] Trial 19 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best trial:\n",
      "  Value:  2.2587267407595015\n",
      "  Params: \n",
      "    head_num: 32\n",
      "    use_extra_layer: False\n",
      "    shared_dim: 197\n",
      "    unit_layer_1: 262\n",
      "    unit_layer_2: 84\n",
      "    dropout: 0.262803257540282\n",
      "    learning_rate: 0.00013197622102333815\n",
      "    weight_decay: 0.00016061208774452676\n"
     ]
    }
   ],
   "source": [
    "# import optuna\n",
    "\n",
    "# NUM_EPOCHS = 20\n",
    "# WEIGHT_DECAY = 1e-3\n",
    "\n",
    "# def objective(trial):\n",
    "#     # Define fixed valid head numbers\n",
    "#     possible_head_nums = [2, 4, 8, 16, 32]\n",
    "    \n",
    "#     # First suggest head_num from fixed choices\n",
    "#     head_num = trial.suggest_categorical('head_num', possible_head_nums)\n",
    "#     use_extra_layer = trial.suggest_categorical('use_extra_layer', [True, False])\n",
    "\n",
    "#     # Suggest shared_dim that's divisible by head_num\n",
    "#     min_dim = max(64, head_num)  # Ensure minimum dimension works with head_num\n",
    "#     max_dim = 256\n",
    "#     shared_dim = trial.suggest_int('shared_dim', min_dim, max_dim)\n",
    "    \n",
    "#     # Round shared_dim to nearest multiple of head_num\n",
    "#     shared_dim = (shared_dim // head_num) * head_num\n",
    "    \n",
    "#     # Skip if dimensions don't work\n",
    "#     if shared_dim < head_num:\n",
    "#         raise optuna.TrialPruned()\n",
    "    \n",
    "#     # Create base layers\n",
    "#     units_per_layer = [\n",
    "#         trial.suggest_int('unit_layer_1', 64, 512),\n",
    "#         trial.suggest_int('unit_layer_2', 64, 512),\n",
    "#     ]\n",
    "    \n",
    "#     # Conditionally add third layer\n",
    "#     if use_extra_layer:\n",
    "#         units_per_layer.append(\n",
    "#             trial.suggest_int('unit_layer_3', 64, 512)\n",
    "#         )\n",
    "    \n",
    "#     hparams = {\n",
    "#         'title_size': 300,\n",
    "#         'head_num': head_num,\n",
    "#         'head_dim': shared_dim,\n",
    "#         'attention_hidden_dim':shared_dim,\n",
    "#         'news_output_dim': shared_dim,\n",
    "#         'dropout': trial.suggest_float('dropout', 0.1, 0.5),\n",
    "#         'learning_rate': trial.suggest_float('learning_rate', 1e-6, 1e-3, log=True),\n",
    "#         'weight_decay': trial.suggest_float('weight_decay', 1e-4, 1e-2, log=True),\n",
    "#         'units_per_layer': [\n",
    "#             trial.suggest_int('unit_layer_1', 256, 512),\n",
    "#             trial.suggest_int('unit_layer_2', 256, 512),\n",
    "#         ],\n",
    "#         'use_category': True,\n",
    "#         'use_topic': True,\n",
    "#         'use_numeric':False,\n",
    "#         'use_publication_discount': True,\n",
    "#         'use_session_discount': True,\n",
    "#     }\n",
    "\n",
    "#     # Initialize model and training components\n",
    "#     model = NRMSDocVecModel(hparams=hparams, device=device)\n",
    "#     criterion = model.get_loss().to(device)\n",
    "#     optimizer = optim.Adam(model.parameters(), \n",
    "#                           lr=hparams['learning_rate'], \n",
    "#                           weight_decay=hparams['weight_decay'])\n",
    "    \n",
    "#     # Initialize EarlyStopping\n",
    "#     early_stopping = EarlyStopping(patience=4, min_delta=0.025, verbose=True)\n",
    "#     best_val_loss = float('inf')\n",
    "\n",
    "#     # Training loop\n",
    "#     for epoch in range(NUM_EPOCHS):\n",
    "#         # Training phase\n",
    "#         model.train()\n",
    "#         running_loss = 0.0\n",
    "#         train_batch_count = 0\n",
    "\n",
    "#         for batch in train_dataloader_temp:\n",
    "#             inputs, targets, impression_id = batch\n",
    "#             inputs = [inp.to(device) for inp in inputs]\n",
    "#             targets = targets.to(device)\n",
    "#             positive_indices = (targets == 1).nonzero(as_tuple=False)\n",
    "#             targets = positive_indices[:, 1].long()\n",
    "\n",
    "#             optimizer.zero_grad()\n",
    "#             outputs = model(*inputs)\n",
    "#             loss = criterion(outputs, targets)\n",
    "#             loss.backward()\n",
    "#             optimizer.step()\n",
    "\n",
    "#             running_loss += loss.item()\n",
    "#             train_batch_count += 1\n",
    "\n",
    "#         # Validation phase\n",
    "#         model.eval()\n",
    "#         val_loss = 0.0\n",
    "#         val_batch_count = 0\n",
    "        \n",
    "#         with torch.no_grad():\n",
    "#             for batch in val_dataloader_temp:\n",
    "#                 inputs, targets, impression_id = batch\n",
    "#                 inputs = [inp.to(device) for inp in inputs]\n",
    "#                 targets = targets.to(device)\n",
    "#                 positive_indices = (targets == 1).nonzero(as_tuple=False)\n",
    "#                 targets = positive_indices[:, 1].long()\n",
    "#                 outputs = model(*inputs)\n",
    "#                 loss = criterion(outputs, targets)\n",
    "#                 val_loss += loss.item()\n",
    "#                 val_batch_count += 1\n",
    "\n",
    "#         avg_val_loss = val_loss / val_batch_count if val_batch_count > 0 else float('inf')\n",
    "\n",
    "#         # Update best validation loss\n",
    "#         if avg_val_loss < best_val_loss:\n",
    "#             best_val_loss = avg_val_loss\n",
    "\n",
    "#         # Early stopping check\n",
    "#         early_stopping(avg_val_loss)\n",
    "#         if early_stopping.early_stop:\n",
    "#             break\n",
    "\n",
    "#         # Report to Optuna\n",
    "#         trial.report(avg_val_loss, epoch)\n",
    "#         if trial.should_prune():\n",
    "#             raise optuna.TrialPruned()\n",
    "\n",
    "#     return best_val_loss\n",
    "\n",
    "# # Create study and optimize\n",
    "# study = optuna.create_study(direction='minimize')\n",
    "# study.optimize(objective, n_trials=20)  # Increased from 3 to 50 trials\n",
    "\n",
    "# # Print results\n",
    "# print(\"Best trial:\")\n",
    "# trial = study.best_trial\n",
    "# print(\"  Value: \", trial.value)\n",
    "# print(\"  Params: \")\n",
    "# for key, value in trial.params.items():\n",
    "#     print(f\"    {key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Ensure you have defined or imported the EarlyStopping class\n",
    "class EarlyStopping:\n",
    "    def __init__(self, patience=3, min_delta=0.05, verbose=False):\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.verbose = verbose\n",
    "        self.counter = 0\n",
    "        self.best_score = None\n",
    "        self.early_stop = False\n",
    "\n",
    "    def __call__(self, current_score):\n",
    "        if self.best_score is None:\n",
    "            self.best_score = current_score\n",
    "        elif current_score < self.best_score - self.min_delta:\n",
    "            self.best_score = current_score\n",
    "            self.counter = 0\n",
    "        else:\n",
    "            self.counter += 1\n",
    "            if self.verbose:\n",
    "                print(f'EarlyStopping counter: {self.counter} out of {self.patience}')\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "\n",
    "# Define model_checkpoint function if not already defined\n",
    "def model_checkpoint(model, val_loss, epoch, path='model_checkpoint.pth'):\n",
    "    torch.save({\n",
    "        'epoch': epoch,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'val_loss': val_loss,\n",
    "    }, path)\n",
    "    print(f\"Model checkpoint saved at epoch {epoch} with validation loss: {val_loss:.4f}\")\n",
    "\n",
    "# Initialize components\n",
    "NUM_EPOCHS = 30\n",
    "early_stopping = EarlyStopping(patience=4, min_delta=0.025, verbose=True)\n",
    "criterion = nn.CrossEntropyLoss().to(device)  # Use appropriate loss function\n",
    "optimizer = optim.Adam(\n",
    "    model.parameters(),\n",
    "    lr=hparams_nrms.learning_rate,  # Ensure hparams_nrms is defined and has learning_rate\n",
    "    weight_decay=hparams_nrms.weight_decay  # Ensure hparams_nrms has weight_decay\n",
    ")\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer,\n",
    "    mode='min',\n",
    "    factor=0.1,\n",
    "    patience=5,\n",
    "    verbose=True\n",
    ")\n",
    "writer = SummaryWriter()\n",
    "\n",
    "train_losses, val_losses = [], []\n",
    "\n",
    "# Ensure your dataloaders are defined\n",
    "# train_dataloader_temp = ...\n",
    "# val_dataloader_temp = ...\n",
    "\n",
    "epoch_pbar = tqdm(range(1, NUM_EPOCHS + 1), desc=\"Training Progress\", dynamic_ncols=True)\n",
    "for epoch in epoch_pbar:\n",
    "    # Training Phase\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    train_batch_count = 0\n",
    "\n",
    "    for batch in train_dataloader_temp:\n",
    "        inputs, targets, _ = batch  # Assuming the third element is not used\n",
    "        inputs = [inp.to(device) for inp in inputs]\n",
    "        targets = targets.to(device)\n",
    "\n",
    "        # Extract positive indices\n",
    "        positive_indices = (targets == 1).nonzero(as_tuple=False)\n",
    "        if positive_indices.numel() == 0:\n",
    "            raise ValueError(\"No positive samples in the batch\")\n",
    "            continue  # Skip if no positive samples in the batch\n",
    "        targets = positive_indices[:, 1].long()\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(*inputs)  # Shape: (N, C)\n",
    "\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        # torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)  # Gradient clipping\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        train_batch_count += 1\n",
    "\n",
    "    avg_train_loss = running_loss / train_batch_count if train_batch_count > 0 else float('inf')\n",
    "    train_losses.append(avg_train_loss)\n",
    "\n",
    "    # Validation Phase\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    val_batch_count = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in val_dataloader_temp:\n",
    "            inputs, targets, _ = batch\n",
    "            inputs = [inp.to(device) for inp in inputs]\n",
    "            targets = targets.to(device)\n",
    "\n",
    "            positive_indices = (targets == 1).nonzero(as_tuple=False)\n",
    "            if positive_indices.numel() == 0:\n",
    "                continue\n",
    "            targets = positive_indices[:, 1].long()\n",
    "\n",
    "            outputs = model(*inputs)  # Shape: (N, C)\n",
    "            loss = criterion(outputs, targets)\n",
    "\n",
    "            val_loss += loss.item()\n",
    "            val_batch_count += 1\n",
    "\n",
    "    avg_val_loss = val_loss / val_batch_count if val_batch_count > 0 else float('inf')\n",
    "    val_losses.append(avg_val_loss)\n",
    "\n",
    "    # Logging\n",
    "    writer.add_scalar('Loss/Train', avg_train_loss, epoch)\n",
    "    writer.add_scalar('Loss/Validation', avg_val_loss, epoch)\n",
    "\n",
    "    # Update Progress Bar\n",
    "    epoch_pbar.set_postfix({\n",
    "        'train_loss': f'{avg_train_loss:.4f}',\n",
    "        'val_loss': f'{avg_val_loss:.4f}',\n",
    "    })\n",
    "\n",
    "    # Save Checkpoint\n",
    "    if epoch % 10 == 0:\n",
    "        model_checkpoint(model, avg_val_loss, epoch)\n",
    "\n",
    "    # Scheduler Step\n",
    "    scheduler.step(avg_val_loss)\n",
    "\n",
    "    # Early Stopping\n",
    "    early_stopping(avg_val_loss)\n",
    "    if early_stopping.early_stop:\n",
    "        print(\"Early stopping triggered. Stopping training.\")\n",
    "        break  # Exit the training loop\n",
    "\n",
    "writer.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the NRMS model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\gusta\\Documents\\Deeplearning-RecSys-Challenge-2024\\.venv\\Lib\\site-packages\\torch\\optim\\lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n",
      "Training Progress:   0%|          | 0/30 [00:00<?, ?it/s]c:\\Users\\gusta\\Documents\\Deeplearning-RecSys-Challenge-2024\\models_pytorch\\NRMSDocVecModel.py:444: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(enabled=False):\n",
      "Training Progress:  33%|███▎      | 10/30 [04:13<08:24, 25.24s/it, train_loss=2.3016, val_loss=2.9473] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model checkpoint saved at epoch 10 with validation loss: 2.9473\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  47%|████▋     | 14/30 [05:56<06:49, 25.60s/it, train_loss=2.0112, val_loss=2.7154]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EarlyStopping counter: 1 out of 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  53%|█████▎    | 16/30 [06:47<05:59, 25.67s/it, train_loss=1.9174, val_loss=2.5737]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EarlyStopping counter: 1 out of 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  60%|██████    | 18/30 [07:38<05:07, 25.60s/it, train_loss=1.8321, val_loss=2.5500]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EarlyStopping counter: 1 out of 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  67%|██████▋   | 20/30 [08:30<04:16, 25.65s/it, train_loss=1.7703, val_loss=2.4890]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model checkpoint saved at epoch 20 with validation loss: 2.4890\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  70%|███████   | 21/30 [08:56<03:50, 25.67s/it, train_loss=1.7594, val_loss=2.5156]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EarlyStopping counter: 1 out of 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  77%|███████▋  | 23/30 [09:46<02:58, 25.44s/it, train_loss=1.7123, val_loss=2.4568]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EarlyStopping counter: 1 out of 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  83%|████████▎ | 25/30 [10:36<02:06, 25.29s/it, train_loss=1.6794, val_loss=2.4123]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EarlyStopping counter: 1 out of 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  87%|████████▋ | 26/30 [11:01<01:41, 25.27s/it, train_loss=1.6644, val_loss=2.4601]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EarlyStopping counter: 2 out of 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  90%|█████████ | 27/30 [11:27<01:15, 25.20s/it, train_loss=1.6482, val_loss=2.4392]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EarlyStopping counter: 3 out of 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  90%|█████████ | 27/30 [11:52<01:19, 26.37s/it, train_loss=1.6360, val_loss=2.4120]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EarlyStopping counter: 4 out of 4\n",
      "Early stopping triggered. Stopping training.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Ensure you have defined or imported the EarlyStopping class\n",
    "class EarlyStopping:\n",
    "    def __init__(self, patience=3, min_delta=0.05, verbose=False):\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.verbose = verbose\n",
    "        self.counter = 0\n",
    "        self.best_score = None\n",
    "        self.early_stop = False\n",
    "\n",
    "    def __call__(self, current_score):\n",
    "        if self.best_score is None:\n",
    "            self.best_score = current_score\n",
    "        elif current_score < self.best_score - self.min_delta:\n",
    "            self.best_score = current_score\n",
    "            self.counter = 0\n",
    "        else:\n",
    "            self.counter += 1\n",
    "            if self.verbose:\n",
    "                print(f'EarlyStopping counter: {self.counter} out of {self.patience}')\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "\n",
    "# Define model_checkpoint function if not already defined\n",
    "def model_checkpoint(model, val_loss, epoch, path='model_checkpoint.pth'):\n",
    "    torch.save({\n",
    "        'epoch': epoch,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'val_loss': val_loss,\n",
    "    }, path)\n",
    "    print(f\"Model checkpoint saved at epoch {epoch} with validation loss: {val_loss:.4f}\")\n",
    "\n",
    "# Initialize components\n",
    "NUM_EPOCHS = 30\n",
    "early_stopping = EarlyStopping(patience=4, min_delta=0.025, verbose=True)\n",
    "criterion = nn.CrossEntropyLoss().to(device)  # Use appropriate loss function\n",
    "optimizer = optim.Adam(\n",
    "    model.parameters(),\n",
    "    lr=hparams_nrms.learning_rate,  # Ensure hparams_nrms is defined and has learning_rate\n",
    "    weight_decay=hparams_nrms.weight_decay  # Ensure hparams_nrms has weight_decay\n",
    ")\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer,\n",
    "    mode='min',\n",
    "    factor=0.1,\n",
    "    patience=5,\n",
    "    verbose=True\n",
    ")\n",
    "writer = SummaryWriter()\n",
    "\n",
    "train_losses, val_losses = [], []\n",
    "\n",
    "\n",
    "epoch_pbar = tqdm(range(1, NUM_EPOCHS + 1), desc=\"Training Progress\", dynamic_ncols=True)\n",
    "for epoch in epoch_pbar:\n",
    "    # Training Phase\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    train_batch_count = 0\n",
    "\n",
    "    for batch in train_dataloader_temp:\n",
    "        inputs, targets, _ = batch  # Assuming the third element is not used\n",
    "        inputs = [inp.to(device) for inp in inputs]\n",
    "        targets = targets.to(device)\n",
    "\n",
    "        # Extract positive indices\n",
    "        positive_indices = (targets == 1).nonzero(as_tuple=False)\n",
    "        if positive_indices.numel() == 0:\n",
    "            raise ValueError(\"No positive samples in the batch\")\n",
    "            continue  # Skip if no positive samples in the batch\n",
    "        targets = positive_indices[:, 1].long()\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(*inputs)  # Shape: (N, C)\n",
    "\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        # torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)  # Gradient clipping\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        train_batch_count += 1\n",
    "\n",
    "    avg_train_loss = running_loss / train_batch_count if train_batch_count > 0 else float('inf')\n",
    "    train_losses.append(avg_train_loss)\n",
    "\n",
    "    # Validation Phase\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    val_batch_count = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in val_dataloader_temp:\n",
    "            inputs, targets, _ = batch\n",
    "            inputs = [inp.to(device) for inp in inputs]\n",
    "            targets = targets.to(device)\n",
    "\n",
    "            positive_indices = (targets == 1).nonzero(as_tuple=False)\n",
    "            if positive_indices.numel() == 0:\n",
    "                continue\n",
    "            targets = positive_indices[:, 1].long()\n",
    "\n",
    "            outputs = model(*inputs)  # Shape: (N, C)\n",
    "            loss = criterion(outputs, targets)\n",
    "\n",
    "            val_loss += loss.item()\n",
    "            val_batch_count += 1\n",
    "\n",
    "    avg_val_loss = val_loss / val_batch_count if val_batch_count > 0 else float('inf')\n",
    "    val_losses.append(avg_val_loss)\n",
    "\n",
    "    # Logging\n",
    "    writer.add_scalar('Loss/Train', avg_train_loss, epoch)\n",
    "    writer.add_scalar('Loss/Validation', avg_val_loss, epoch)\n",
    "\n",
    "    # Update Progress Bar\n",
    "    epoch_pbar.set_postfix({\n",
    "        'train_loss': f'{avg_train_loss:.4f}',\n",
    "        'val_loss': f'{avg_val_loss:.4f}',\n",
    "    })\n",
    "\n",
    "    # Save Checkpoint\n",
    "    if epoch % 10 == 0:\n",
    "        model_checkpoint(model, avg_val_loss, epoch)\n",
    "\n",
    "    # Scheduler Step\n",
    "    scheduler.step(avg_val_loss)\n",
    "\n",
    "    # Early Stopping\n",
    "    early_stopping(avg_val_loss)\n",
    "    if early_stopping.early_stop:\n",
    "        print(\"Early stopping triggered. Stopping training.\")\n",
    "        break  # Exit the training loop\n",
    "\n",
    "writer.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Summary:\n",
      "Initial Training Loss: 13.6472\n",
      "Final Training Loss: 1.6360\n",
      "Best Training Loss: 1.6360\n",
      "\n",
      "Initial Validation Loss: 12.5222\n",
      "Final Validation Loss: 2.4120\n",
      "Best Validation Loss: 2.4120\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Plot the losses\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(range(1, len(train_losses) + 1), train_losses, label='Training Loss')\n",
    "plt.plot(range(1, len(val_losses) + 1), val_losses, label='Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training and Validation Losses')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.savefig(\"nrms_training_loss.png\")\n",
    "plt.show()\n",
    "\n",
    "# Print final statistics\n",
    "print(\"\\nTraining Summary:\")\n",
    "print(f\"Initial Training Loss: {train_losses[0]:.4f}\")\n",
    "print(f\"Final Training Loss: {train_losses[-1]:.4f}\")\n",
    "print(f\"Best Training Loss: {min(train_losses):.4f}\")\n",
    "print(f\"\\nInitial Validation Loss: {val_losses[0]:.4f}\")\n",
    "print(f\"Final Validation Loss: {val_losses[-1]:.4f}\")\n",
    "print(f\"Best Validation Loss: {min(val_losses):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "\n",
    "# def evaluate_model(model, dataloader, device):\n",
    "#     \"\"\"Evaluate the model and return predictions and labels for metric calculation.\"\"\"\n",
    "#     model.eval()\n",
    "#     all_predictions = []\n",
    "#     all_labels = []\n",
    "\n",
    "#     print(\"\\nEvaluating DataLoader...\")\n",
    "#     total_samples = len(dataloader.dataset)\n",
    "#     print(f\"Total samples in DataLoader: {total_samples}\")\n",
    "\n",
    "#     with torch.no_grad():\n",
    "#         for batch_idx, (inputs, targets, impression_ids) in enumerate(dataloader):\n",
    "#             his_input_title, pred_input_title = inputs\n",
    "\n",
    "#             if batch_idx == 0:  # Debug first batch shapes\n",
    "#                 print(\"\\nFirst batch shapes:\")\n",
    "#                 print(f\"  - his_input_title: {his_input_title.shape}\")\n",
    "#                 print(f\"  - pred_input_title: {pred_input_title.shape}\")\n",
    "#                 print(f\"  - targets: {targets.shape}\")\n",
    "\n",
    "#             # Move data to device\n",
    "#             his_input_title = his_input_title.to(device)\n",
    "#             pred_input_title = pred_input_title.to(device)\n",
    "#             targets = targets.to(device)\n",
    "\n",
    "#             # Get predictions\n",
    "#             predictions = model.predict(his_input_title, pred_input_title)\n",
    "#             predictions = predictions.cpu().numpy()\n",
    "#             targets = targets.cpu().numpy()\n",
    "\n",
    "#             # Process each sample in the batch\n",
    "#             batch_size = predictions.shape[0]\n",
    "#             for sample_idx in range(batch_size):\n",
    "#                 pred = predictions[sample_idx]\n",
    "#                 label = targets[sample_idx]\n",
    "\n",
    "#                 # Create valid_mask where label is not equal to the padding value (-1)\n",
    "#                 valid_mask = (label != -1)\n",
    "#                 sample_preds = pred[valid_mask]\n",
    "#                 sample_labels = label[valid_mask]\n",
    "\n",
    "#                 if len(sample_labels) == 0:\n",
    "#                     continue  # Skip empty samples\n",
    "\n",
    "#                 # Ensure that there is at least one positive and one negative label\n",
    "#                 if len(np.unique(sample_labels)) < 2:\n",
    "#                     continue  # Skip samples with only one class\n",
    "\n",
    "#                 all_predictions.append(sample_preds.tolist())\n",
    "#                 all_labels.append(sample_labels.tolist())\n",
    "\n",
    "#     print(\"\\nEvaluation completed.\")\n",
    "#     print(f\"Total predictions generated: {len(all_predictions)}\")\n",
    "#     print(f\"First few prediction lengths: {[len(x) for x in all_predictions[:15]]}\")\n",
    "#     return all_labels, all_predictions\n",
    "\n",
    "\n",
    "# # Evaluate the model\n",
    "# labels_list, scores_list = evaluate_model(model, val_dataloader_temp, device)\n",
    "\n",
    "# # Validate predictions against the DataFrame\n",
    "# print(\"\\nValidation against DataFrame:\")\n",
    "# if len(scores_list) != len(df_validation):\n",
    "#     print(\"WARNING: Length mismatch!\")\n",
    "#     print(f\"  - Number of predictions: {len(scores_list)}\")\n",
    "#     print(f\"  - Number of rows in DataFrame: {len(df_validation)}\")\n",
    "\n",
    "# # Compute metrics\n",
    "# metrics = MetricEvaluator(\n",
    "#     labels=labels_list,\n",
    "#     predictions=scores_list,\n",
    "#     metric_functions=[\n",
    "#         AucScore(),\n",
    "#         MrrScore(),\n",
    "#         NdcgScore(k=5),\n",
    "#         NdcgScore(k=10)\n",
    "#     ],\n",
    "# )\n",
    "# results = metrics.evaluate()\n",
    "# print(\"\\nMetrics:\", results.evaluations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FRACTION: 0.1, HISTORY_SIZE: 20\n",
      "Hyperparameters:\n",
      "title_size: 300\n",
      "embedding_dim: 32\n",
      "word_emb_dim: 8\n",
      "vocab_size: 10000\n",
      "head_num: 32\n",
      "head_dim: 192\n",
      "attention_hidden_dim: 192\n",
      "hidden_dim: 4\n",
      "optimizer: adam\n",
      "loss: cross_entropy_loss\n",
      "dropout: 0.262803257540282\n",
      "learning_rate: 1.0454050068420554e-05\n",
      "weight_decay: 0.00016061208774452676\n",
      "news_output_dim: 192\n",
      "units_per_layer: [262, 84]\n",
      "use_category: True\n",
      "use_topic: True\n",
      "use_numeric: False\n",
      "use_session_discount: True\n",
      "use_publication_discount: True\n",
      "doc_out_dim: 128\n",
      "cat_out_dim: 128\n",
      "top_out_dim: 128\n",
      "numeric_proj_dim: 16\n"
     ]
    }
   ],
   "source": [
    "print(f\"FRACTION: {FRACTION}, HISTORY_SIZE: {HISTORY_SIZE}\")\n",
    "\n",
    "# Filter out special Python attributes and print parameters\n",
    "params = {k: v for k, v in hparams_nrms.__dict__.items() if not k.startswith('__')}\n",
    "print(\"Hyperparameters:\")\n",
    "for key, value in params.items():\n",
    "    print(f\"{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_length_of_labels(df: pl.DataFrame, impression_id: int) -> int:\n",
    "    # Filter for matching impression_id\n",
    "    filtered = df.filter(pl.col('impression_id') == impression_id)\n",
    "    \n",
    "    if filtered.height == 0:\n",
    "        raise ValueError(f\"No row found for impression_id {impression_id}\")\n",
    "    \n",
    "    # Get labels from first row\n",
    "    labels = filtered.select('labels').row(0)[0]\n",
    "    \n",
    "    return len(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get_length_of_labels(df_validation, 349992000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label 0: [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "Label 1: [0, 0, 0, 1, 0, 0, 0, 0, 0, 0]\n",
      "Label 2: [0, 0, 0, 0, 1, 0]\n",
      "Label 3: [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "Label 4: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "# Get first 5 labels\n",
    "first_5_labels = df_validation.select('labels').head(5)\n",
    "\n",
    "# Print each label with index\n",
    "for i, row in enumerate(first_5_labels.iter_rows()):\n",
    "    print(f\"Label {i}: {row[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipped 0 samples with only one class\n",
      "Remaining valid samples: 24332\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "# Initialize lists\n",
    "all_predictions = []\n",
    "all_labels = []\n",
    "skipped_samples = 0\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for batch in val_dataloader_temp:\n",
    "        inputs, targets, impression_ids = batch\n",
    "        \n",
    "        # Move to device\n",
    "        inputs = [inp.to(device) for inp in inputs]\n",
    "        \n",
    "        # Get predictions\n",
    "        predictions = model.predict(*inputs)\n",
    "        \n",
    "        # Convert to probabilities if needed\n",
    "        if not torch.is_floating_point(predictions):\n",
    "            predictions = torch.sigmoid(predictions)\n",
    "        \n",
    "        # Convert to lists while preserving structure\n",
    "        batch_preds = predictions.cpu().numpy().tolist()\n",
    "        batch_labels = targets.cpu().numpy().tolist()\n",
    "        impression_ids = impression_ids.cpu().numpy().tolist()\n",
    "        \n",
    "        batch_preds_without_padding = []\n",
    "        batch_labels_without_padding = []\n",
    "        \n",
    "        for pred_sample, label_sample, impression_id_sample in zip(batch_preds, batch_labels, impression_ids):\n",
    "            # Remove padding\n",
    "            actual_length = get_length_of_labels(df_validation, impression_id_sample)\n",
    "            pred_sample = pred_sample[:actual_length]\n",
    "            \n",
    "            # Check if sample has both classes before adding\n",
    "            if 1 in label_sample[:actual_length] and 0 in label_sample[:actual_length]:\n",
    "                batch_preds_without_padding.append(pred_sample)\n",
    "                batch_labels_without_padding.append(label_sample[:actual_length])\n",
    "            else:\n",
    "                skipped_samples += 1\n",
    "        \n",
    "        # Add batch predictions and labels\n",
    "        all_predictions.extend(batch_preds_without_padding)\n",
    "        all_labels.extend(batch_labels_without_padding)\n",
    "\n",
    "print(f\"Skipped {skipped_samples} samples with only one class\")\n",
    "print(f\"Remaining valid samples: {len(all_predictions)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Debug: Print label distribution for first 5 samples\n",
    "# for i, (preds, labels) in enumerate(zip(all_predictions[:5], all_labels[:5])):\n",
    "#     print(f\"\\nSample {i}:\")\n",
    "#     print(f\"Labels length:      {len(labels)}\")\n",
    "#     print(f\"Predictions length: {len(preds)}\")\n",
    "#     print(f\"Num positives: {sum(labels)}\")\n",
    "#     print(f\"Num negatives: {len(labels) - sum(labels)}\")\n",
    "#     print(f\"Label distribution: {labels}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(type(all_predictions))\n",
    "# print(type(all_labels))\n",
    "\n",
    "# print(f\"Number of predictions: {len(all_predictions)}\")\n",
    "# print(f\"example prediction: {all_predictions[0:2]}\")\n",
    "# print(f\"Number of labels: {len(all_labels)}\")\n",
    "# print(f\"example label: {all_labels[0:2]}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.1722\n"
     ]
    }
   ],
   "source": [
    "# Initialize counters\n",
    "correct_predictions = 0\n",
    "total_samples = len(all_predictions)\n",
    "\n",
    "# Iterate over samples\n",
    "for idx, _ in enumerate(all_predictions):\n",
    "    # print(f\"Sample {idx}: Prediction: {all_predictions[idx]}\")\n",
    "    # print(f\"Sample {idx}: Label:      {all_labels[idx]}\")\n",
    "    \n",
    "    # Extract index of maximum value in predictions and labels\n",
    "    pred_max_index = np.argmax(all_predictions[idx])\n",
    "    label_max_index = np.argmax(all_labels[idx])\n",
    "    \n",
    "    # Compare indices to determine if the prediction was correct\n",
    "    if pred_max_index == label_max_index:\n",
    "        # print(f\"Sample {idx}: Prediction was correct.\")\n",
    "        correct_predictions += 1\n",
    "  #  else:\n",
    "        # print(f\"Sample {idx}: Prediction was wrong.\")\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = correct_predictions / total_samples\n",
    "print(f\"Accuracy: {accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Mean AUC: 0.5970\n",
      "Number of valid AUC calculations: 24332\n"
     ]
    }
   ],
   "source": [
    "from evaluation import AucScore\n",
    "\n",
    "# auc_score = AucScore()\n",
    "\n",
    "# auc_score.calculate(all_predictions, all_labels)\n",
    "# print(f\"AUC: {auc_score.score}\")\n",
    "# # Calculate AUC per sample\n",
    "aucs = []\n",
    "for preds, labels in zip(all_predictions, all_labels):\n",
    "    try:\n",
    "        # Only calculate if we have both positive and negative samples\n",
    "        if sum(labels) > 0 and sum(labels) < len(labels):\n",
    "            auc = roc_auc_score(labels, preds)\n",
    "            aucs.append(auc)\n",
    "    except ValueError:\n",
    "        raise ValueError(\"Only one class present in labels. Cannot calculate AUC.\")\n",
    "\n",
    "print(f\"\\nMean AUC: {np.mean(aucs):.4f}\")\n",
    "print(f\"Number of valid AUC calculations: {len(aucs)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Output Prediction File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "https://github.com/recommenders-team/recommenders/blob/main/examples/00_quick_start/nrms_MIND.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
