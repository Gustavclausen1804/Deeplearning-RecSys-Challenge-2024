{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Global settings and imports"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\DTU - Master\\1 semester\\Deep learning (02456)\\Deeplearning-RecSys-Challenge-2024\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python version: 3.11.5\n",
      "PyTorch version: 2.5.1+cpu\n",
      "CUDA available: False\n"
     ]
    }
   ],
   "source": [
    "#%load_ext autoreload\n",
    "#%autoreload 2\n",
    "\n",
    "import sys\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import polars as pl\n",
    "import matplotlib.pyplot as plt\n",
    "#import optuna\n",
    "from tqdm import tqdm\n",
    "import torch.optim as optim\n",
    "\n",
    "from utils._constants import (\n",
    "    DEFAULT_HISTORY_ARTICLE_ID_COL,\n",
    "    DEFAULT_CLICKED_ARTICLES_COL,\n",
    "    DEFAULT_INVIEW_ARTICLES_COL,\n",
    "    DEFAULT_IMPRESSION_ID_COL,\n",
    "    DEFAULT_SUBTITLE_COL,\n",
    "    DEFAULT_LABELS_COL,\n",
    "    DEFAULT_TITLE_COL,\n",
    "    DEFAULT_USER_COL,\n",
    "    DEFAULT_ARTICLE_PUBLISHED_TIMESTAMP_COL\n",
    ")\n",
    "\n",
    "from utils._behaviors import (\n",
    "    create_binary_labels_column,\n",
    "    sampling_strategy_wu2019,\n",
    "    add_known_user_column,\n",
    "    add_prediction_scores,\n",
    "    truncate_history,\n",
    ")\n",
    "from evaluation import MetricEvaluator, AucScore, NdcgScore, MrrScore\n",
    "from utils._articles import convert_text2encoding_with_transformers\n",
    "from utils._polars import concat_str_columns, slice_join_dataframes\n",
    "from utils._articles import create_article_id_to_value_mapping\n",
    "from utils._nlp import get_transformers_word_embeddings, generate_embeddings_with_transformers\n",
    "from utils._python import write_submission_file, rank_predictions_by_score\n",
    "from models_pytorch.model_config import hparams_nrms\n",
    "\n",
    "from models_pytorch.nrms import NRMSModel\n",
    "from models_pytorch.NRMSDocVecModel import NRMSDocVecModel\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from models_pytorch.dataloader import NRMSDataSet\n",
    "\n",
    "# Check Python version\n",
    "print(f\"Python version: {sys.version.split()[0]}\")\n",
    "\n",
    "# Check PyTorch version\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "\n",
    "# Check GPU availability\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"Current GPU device: {torch.cuda.get_device_name()}\")\n",
    "    print(f\"Number of GPUs available: {torch.cuda.device_count()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Looking at behaviours and history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download and load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ebnerd_demo\n"
     ]
    }
   ],
   "source": [
    "PATH = Path(\"./ebnerd_demo\")  # Base path for your data directory\n",
    "print(PATH)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (2, 6)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>user_id</th><th>article_id_fixed</th><th>article_ids_inview</th><th>article_ids_clicked</th><th>impression_id</th><th>labels</th></tr><tr><td>u32</td><td>list[i32]</td><td>list[i64]</td><td>list[i64]</td><td>u32</td><td>list[i8]</td></tr></thead><tbody><tr><td>645703</td><td>[9767955, 9768722, … 9719416]</td><td>[9774648, 9746360, … 9738729]</td><td>[9774555]</td><td>486612408</td><td>[0, 0, … 0]</td></tr><tr><td>1193431</td><td>[9766805, 9766042, … 9770333]</td><td>[9772925, 9695098, … 9772923]</td><td>[9772923]</td><td>155019967</td><td>[0, 0, … 1]</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (2, 6)\n",
       "┌─────────┬───────────────────┬───────────────────┬──────────────────┬───────────────┬─────────────┐\n",
       "│ user_id ┆ article_id_fixed  ┆ article_ids_invie ┆ article_ids_clic ┆ impression_id ┆ labels      │\n",
       "│ ---     ┆ ---               ┆ w                 ┆ ked              ┆ ---           ┆ ---         │\n",
       "│ u32     ┆ list[i32]         ┆ ---               ┆ ---              ┆ u32           ┆ list[i8]    │\n",
       "│         ┆                   ┆ list[i64]         ┆ list[i64]        ┆               ┆             │\n",
       "╞═════════╪═══════════════════╪═══════════════════╪══════════════════╪═══════════════╪═════════════╡\n",
       "│ 645703  ┆ [9767955,         ┆ [9774648,         ┆ [9774555]        ┆ 486612408     ┆ [0, 0, … 0] │\n",
       "│         ┆ 9768722, …        ┆ 9746360, …        ┆                  ┆               ┆             │\n",
       "│         ┆ 9719416]          ┆ 9738729]          ┆                  ┆               ┆             │\n",
       "│ 1193431 ┆ [9766805,         ┆ [9772925,         ┆ [9772923]        ┆ 155019967     ┆ [0, 0, … 1] │\n",
       "│         ┆ 9766042, …        ┆ 9695098, …        ┆                  ┆               ┆             │\n",
       "│         ┆ 9770333]          ┆ 9772923]          ┆                  ┆               ┆             │\n",
       "└─────────┴───────────────────┴───────────────────┴──────────────────┴───────────────┴─────────────┘"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def ebnerd_from_path(path: Path, history_size: int = 30) -> pl.DataFrame:\n",
    "    \"\"\"\n",
    "    Load ebnerd - function\n",
    "    \"\"\"\n",
    "    df_history = (\n",
    "        pl.scan_parquet(path.joinpath(\"history.parquet\"))\n",
    "        .select(DEFAULT_USER_COL, DEFAULT_HISTORY_ARTICLE_ID_COL)\n",
    "        .pipe(\n",
    "            truncate_history,\n",
    "            column=DEFAULT_HISTORY_ARTICLE_ID_COL,\n",
    "            history_size=history_size,\n",
    "            padding_value=0,\n",
    "            enable_warning=False,\n",
    "        )\n",
    "    )\n",
    "    df_behaviors = (\n",
    "        pl.scan_parquet(path.joinpath(\"behaviors.parquet\"))\n",
    "        .collect()\n",
    "        .pipe(\n",
    "            slice_join_dataframes,\n",
    "            df2=df_history.collect(),\n",
    "            on=DEFAULT_USER_COL,\n",
    "            how=\"left\",\n",
    "        )\n",
    "    )\n",
    "    return df_behaviors\n",
    "\n",
    "COLUMNS = [\n",
    "    DEFAULT_USER_COL,\n",
    "    DEFAULT_HISTORY_ARTICLE_ID_COL,\n",
    "    DEFAULT_INVIEW_ARTICLES_COL,\n",
    "    DEFAULT_CLICKED_ARTICLES_COL,\n",
    "    DEFAULT_IMPRESSION_ID_COL,\n",
    "]\n",
    "HISTORY_SIZE = 20 # TODO: History size. \n",
    "FRACTION = 0.001\n",
    "\n",
    "df_train = (\n",
    "    ebnerd_from_path(PATH.joinpath(\"train\"), history_size=HISTORY_SIZE)\n",
    "    .select(COLUMNS)\n",
    "    .pipe(\n",
    "        sampling_strategy_wu2019,\n",
    "        npratio=4,\n",
    "        shuffle=True,\n",
    "        with_replacement=True,\n",
    "        seed=123,\n",
    "    )\n",
    "    .pipe(create_binary_labels_column)\n",
    "    .sample(fraction=FRACTION)\n",
    ")\n",
    "# =>\n",
    "df_validation = (\n",
    "    ebnerd_from_path(PATH.joinpath(\"validation\"), history_size=HISTORY_SIZE)\n",
    "    .select(COLUMNS)\n",
    "    .pipe(create_binary_labels_column)\n",
    "    .sample(fraction=FRACTION)\n",
    ")\n",
    "df_train.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "vscode": {
     "languageId": "ruby"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average length of article_ids_inview in df_train: 5.0\n",
      "Average length of article_ids_inview in df_validation: 14.04\n"
     ]
    }
   ],
   "source": [
    "def calculate_average_length(df, column):\n",
    "    total_length = sum(len(row) for row in df[column])\n",
    "    average_length = total_length / len(df)\n",
    "    return average_length\n",
    "\n",
    "# Calculate average length for df_train\n",
    "average_length_inview_train = calculate_average_length(df_train, DEFAULT_INVIEW_ARTICLES_COL)\n",
    "print(f\"Average length of article_ids_inview in df_train: {average_length_inview_train}\")\n",
    "\n",
    "# Calculate average length for df_validation\n",
    "average_length_inview_validation = calculate_average_length(df_validation, DEFAULT_INVIEW_ARTICLES_COL)\n",
    "print(f\"Average length of article_ids_inview in df_validation: {average_length_inview_validation}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Longest inview article length in df_train: 5\n",
      "Longest inview article length in df_validation: 33\n",
      "Longest history length in df_train: 20\n",
      "Longest history length in df_validation: 20\n"
     ]
    }
   ],
   "source": [
    "# Function to find the maximum length of arrays in a column\n",
    "def find_max_length(df, column):\n",
    "    max_length = 0\n",
    "    for row in df[column]:\n",
    "        max_length = max(max_length, len(row))\n",
    "    return max_length\n",
    "\n",
    "# Find the longest inview article length in df_train\n",
    "max_inview_length_train = find_max_length(df_train, DEFAULT_INVIEW_ARTICLES_COL)\n",
    "\n",
    "# Find the longest inview article length in df_validation\n",
    "max_inview_length_validation = find_max_length(df_validation, DEFAULT_INVIEW_ARTICLES_COL)\n",
    "\n",
    "print(f\"Longest inview article length in df_train: {max_inview_length_train}\")\n",
    "print(f\"Longest inview article length in df_validation: {max_inview_length_validation}\")\n",
    "\n",
    "max_history_length_train = find_max_length(df_train, DEFAULT_HISTORY_ARTICLE_ID_COL)\n",
    "max_history_length_validation = find_max_length(df_validation, DEFAULT_HISTORY_ARTICLE_ID_COL)\n",
    "\n",
    "print(f\"Longest history length in df_train: {max_history_length_train}\")\n",
    "print(f\"Longest history length in df_validation: {max_history_length_validation}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "vscode": {
     "languageId": "ruby"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows with exactly one clicked article in df_train: 24\n",
      "Number of rows with exactly one clicked article in df_validation: 25\n"
     ]
    }
   ],
   "source": [
    "# Function to filter rows with exactly one clicked article\n",
    "def filter_rows_with_one_clicked_article(df, clicked_articles_col):\n",
    "    # Manually filter rows where the array has exactly one element\n",
    "    filtered_rows = []\n",
    "    for row in df.iter_rows(named=True):\n",
    "        if len(row[clicked_articles_col]) == 1:\n",
    "            filtered_rows.append(row)\n",
    "    return pl.DataFrame(filtered_rows)\n",
    "\n",
    "\n",
    "# Filter rows in df_train and df_validation\n",
    "df_train = filter_rows_with_one_clicked_article(df_train, DEFAULT_CLICKED_ARTICLES_COL)\n",
    "df_validation = filter_rows_with_one_clicked_article(df_validation, DEFAULT_CLICKED_ARTICLES_COL)\n",
    "\n",
    "# Print the results\n",
    "print(f\"Number of rows with exactly one clicked article in df_train: {df_train.shape[0]}\")\n",
    "print(f\"Number of rows with exactly one clicked article in df_validation: {df_validation.shape[0]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "vscode": {
     "languageId": "ruby"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (5, 6)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>user_id</th><th>article_id_fixed</th><th>article_ids_inview</th><th>article_ids_clicked</th><th>impression_id</th><th>labels</th></tr><tr><td>i64</td><td>list[i64]</td><td>list[i64]</td><td>list[i64]</td><td>i64</td><td>list[i64]</td></tr></thead><tbody><tr><td>1901483</td><td>[9778500, 9778381, … 9779417]</td><td>[9785020, 9784097, … 9785665]</td><td>[9785020]</td><td>182849320</td><td>[1, 0, … 0]</td></tr><tr><td>139350</td><td>[9779748, 9777769, … 9777910]</td><td>[9789037, 9788841, … 9788921]</td><td>[9782845]</td><td>571321560</td><td>[0, 0, … 0]</td></tr><tr><td>1952887</td><td>[9778922, 9779289, … 9779205]</td><td>[9080070, 9790811, … 9790987]</td><td>[9790784]</td><td>206470750</td><td>[0, 0, … 0]</td></tr><tr><td>751626</td><td>[9749668, 9751064, … 9773887]</td><td>[9786139, 8560195, … 9786364]</td><td>[9782517]</td><td>488114430</td><td>[0, 0, … 0]</td></tr><tr><td>337540</td><td>[9775846, 9778074, … 9780039]</td><td>[9784097, 9781785, … 9784951]</td><td>[9784875]</td><td>473299626</td><td>[0, 0, … 0]</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (5, 6)\n",
       "┌─────────┬───────────────────┬───────────────────┬──────────────────┬───────────────┬─────────────┐\n",
       "│ user_id ┆ article_id_fixed  ┆ article_ids_invie ┆ article_ids_clic ┆ impression_id ┆ labels      │\n",
       "│ ---     ┆ ---               ┆ w                 ┆ ked              ┆ ---           ┆ ---         │\n",
       "│ i64     ┆ list[i64]         ┆ ---               ┆ ---              ┆ i64           ┆ list[i64]   │\n",
       "│         ┆                   ┆ list[i64]         ┆ list[i64]        ┆               ┆             │\n",
       "╞═════════╪═══════════════════╪═══════════════════╪══════════════════╪═══════════════╪═════════════╡\n",
       "│ 1901483 ┆ [9778500,         ┆ [9785020,         ┆ [9785020]        ┆ 182849320     ┆ [1, 0, … 0] │\n",
       "│         ┆ 9778381, …        ┆ 9784097, …        ┆                  ┆               ┆             │\n",
       "│         ┆ 9779417]          ┆ 9785665]          ┆                  ┆               ┆             │\n",
       "│ 139350  ┆ [9779748,         ┆ [9789037,         ┆ [9782845]        ┆ 571321560     ┆ [0, 0, … 0] │\n",
       "│         ┆ 9777769, …        ┆ 9788841, …        ┆                  ┆               ┆             │\n",
       "│         ┆ 9777910]          ┆ 9788921]          ┆                  ┆               ┆             │\n",
       "│ 1952887 ┆ [9778922,         ┆ [9080070,         ┆ [9790784]        ┆ 206470750     ┆ [0, 0, … 0] │\n",
       "│         ┆ 9779289, …        ┆ 9790811, …        ┆                  ┆               ┆             │\n",
       "│         ┆ 9779205]          ┆ 9790987]          ┆                  ┆               ┆             │\n",
       "│ 751626  ┆ [9749668,         ┆ [9786139,         ┆ [9782517]        ┆ 488114430     ┆ [0, 0, … 0] │\n",
       "│         ┆ 9751064, …        ┆ 8560195, …        ┆                  ┆               ┆             │\n",
       "│         ┆ 9773887]          ┆ 9786364]          ┆                  ┆               ┆             │\n",
       "│ 337540  ┆ [9775846,         ┆ [9784097,         ┆ [9784875]        ┆ 473299626     ┆ [0, 0, … 0] │\n",
       "│         ┆ 9778074, …        ┆ 9781785, …        ┆                  ┆               ┆             │\n",
       "│         ┆ 9780039]          ┆ 9784951]          ┆                  ┆               ┆             │\n",
       "└─────────┴───────────────────┴───────────────────┴──────────────────┴───────────────┴─────────────┘"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_validation.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "vscode": {
     "languageId": "ruby"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of users in df_train: 23\n"
     ]
    }
   ],
   "source": [
    "print(f\"Number of users in df_train: {df_train['user_id'].n_unique()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (10, 21)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>article_id</th><th>title</th><th>subtitle</th><th>last_modified_time</th><th>premium</th><th>body</th><th>published_time</th><th>image_ids</th><th>article_type</th><th>url</th><th>ner_clusters</th><th>entity_groups</th><th>topics</th><th>category</th><th>subcategory</th><th>category_str</th><th>total_inviews</th><th>total_pageviews</th><th>total_read_time</th><th>sentiment_score</th><th>sentiment_label</th></tr><tr><td>i32</td><td>str</td><td>str</td><td>datetime[μs]</td><td>bool</td><td>str</td><td>datetime[μs]</td><td>list[i64]</td><td>str</td><td>str</td><td>list[str]</td><td>list[str]</td><td>list[str]</td><td>i16</td><td>list[i16]</td><td>str</td><td>i32</td><td>i32</td><td>f32</td><td>f32</td><td>str</td></tr></thead><tbody><tr><td>3037230</td><td>&quot;Ishockey-spiller: Jeg troede j…</td><td>&quot;ISHOCKEY: Ishockey-spilleren S…</td><td>2023-06-29 06:20:57</td><td>false</td><td>&quot;Ambitionerne om at komme til U…</td><td>2003-08-28 08:55:00</td><td>null</td><td>&quot;article_default&quot;</td><td>&quot;https://ekstrabladet.dk/sport/…</td><td>[]</td><td>[]</td><td>[&quot;Kriminalitet&quot;, &quot;Kendt&quot;, … &quot;Mindre ulykke&quot;]</td><td>142</td><td>[327, 334]</td><td>&quot;sport&quot;</td><td>null</td><td>null</td><td>null</td><td>0.9752</td><td>&quot;Negative&quot;</td></tr><tr><td>3044020</td><td>&quot;Prins Harry tvunget til dna-te…</td><td>&quot;Hoffet tvang Prins Harry til a…</td><td>2023-06-29 06:21:16</td><td>false</td><td>&quot;Den britiske tabloidavis The S…</td><td>2005-06-29 08:47:00</td><td>[3097307, 3097197, 3104927]</td><td>&quot;article_default&quot;</td><td>&quot;https://ekstrabladet.dk/underh…</td><td>[&quot;Harry&quot;, &quot;James Hewitt&quot;]</td><td>[&quot;PER&quot;, &quot;PER&quot;]</td><td>[&quot;Kriminalitet&quot;, &quot;Kendt&quot;, … &quot;Personfarlig kriminalitet&quot;]</td><td>414</td><td>[432]</td><td>&quot;underholdning&quot;</td><td>null</td><td>null</td><td>null</td><td>0.7084</td><td>&quot;Negative&quot;</td></tr><tr><td>3057622</td><td>&quot;Rådden kørsel på blå plader&quot;</td><td>&quot;Kan ikke straffes: Udenlandske…</td><td>2023-06-29 06:21:24</td><td>false</td><td>&quot;Slingrende spritkørsel. Grove …</td><td>2005-10-10 07:20:00</td><td>[3047102]</td><td>&quot;article_default&quot;</td><td>&quot;https://ekstrabladet.dk/nyhede…</td><td>[]</td><td>[]</td><td>[&quot;Kriminalitet&quot;, &quot;Transportmiddel&quot;, &quot;Bil&quot;]</td><td>118</td><td>[133]</td><td>&quot;nyheder&quot;</td><td>null</td><td>null</td><td>null</td><td>0.9236</td><td>&quot;Negative&quot;</td></tr><tr><td>3073151</td><td>&quot;Mærsk-arvinger i livsfare&quot;</td><td>&quot;FANGET I FLODBØLGEN: Skibsrede…</td><td>2023-06-29 06:21:38</td><td>false</td><td>&quot;To oldebørn af skibsreder Mærs…</td><td>2005-01-04 06:59:00</td><td>[3067474, 3067478, 3153705]</td><td>&quot;article_default&quot;</td><td>&quot;https://ekstrabladet.dk/nyhede…</td><td>[]</td><td>[]</td><td>[&quot;Erhverv&quot;, &quot;Privat virksomhed&quot;, … &quot;Rejse&quot;]</td><td>118</td><td>[133]</td><td>&quot;nyheder&quot;</td><td>null</td><td>null</td><td>null</td><td>0.9945</td><td>&quot;Negative&quot;</td></tr><tr><td>3193383</td><td>&quot;Skød svigersøn gennem babydyne&quot;</td><td>&quot;44-årig kvinde tiltalt for dra…</td><td>2023-06-29 06:22:57</td><td>false</td><td>&quot;En 44-årig mormor blev i dag f…</td><td>2003-09-15 15:30:00</td><td>null</td><td>&quot;article_default&quot;</td><td>&quot;https://ekstrabladet.dk/krimi/…</td><td>[]</td><td>[]</td><td>[&quot;Kriminalitet&quot;, &quot;Personfarlig kriminalitet&quot;]</td><td>140</td><td>[]</td><td>&quot;krimi&quot;</td><td>null</td><td>null</td><td>null</td><td>0.9966</td><td>&quot;Negative&quot;</td></tr><tr><td>3196611</td><td>&quot;Zoo-tårnet 100 år&quot;</td><td>&quot;I mange år var det god latin a…</td><td>2023-06-29 06:23:02</td><td>false</td><td>&quot;I mange år var det god latin a…</td><td>2005-06-10 05:40:00</td><td>[3067931, 3035588]</td><td>&quot;article_default&quot;</td><td>&quot;https://ekstrabladet.dk/ferie/…</td><td>[]</td><td>[]</td><td>[&quot;Kultur&quot;, &quot;Museum og seværdighed&quot;]</td><td>539</td><td>[]</td><td>&quot;ferie&quot;</td><td>null</td><td>null</td><td>null</td><td>0.6275</td><td>&quot;Neutral&quot;</td></tr><tr><td>3200325</td><td>&quot;Tævet ihjel på tre kvarter&quot;</td><td>&quot;Sadomasochistisk sex-guru: - H…</td><td>2023-06-29 06:23:13</td><td>false</td><td>&quot;.\n",
       "Skolepige vil giftes med gur…</td><td>2002-06-25 05:10:00</td><td>[3200179, 3186817]</td><td>&quot;article_default&quot;</td><td>&quot;https://ekstrabladet.dk/krimi/…</td><td>[]</td><td>[]</td><td>[&quot;Kriminalitet&quot;, &quot;Livsstil&quot;, … &quot;Samfund&quot;]</td><td>140</td><td>[]</td><td>&quot;krimi&quot;</td><td>null</td><td>null</td><td>null</td><td>0.9913</td><td>&quot;Negative&quot;</td></tr><tr><td>3200913</td><td>&quot;Denne kæp kan fælde voldtægtsm…</td><td>&quot;Nye spor i den bestialske vold…</td><td>2023-06-29 06:23:15</td><td>false</td><td>&quot;Den usædvanlig grove voldtægt,…</td><td>2003-09-11 08:55:00</td><td>null</td><td>&quot;article_default&quot;</td><td>&quot;https://ekstrabladet.dk/krimi/…</td><td>[]</td><td>[]</td><td>[&quot;Kriminalitet&quot;, &quot;Personfarlig kriminalitet&quot;]</td><td>140</td><td>[]</td><td>&quot;krimi&quot;</td><td>null</td><td>null</td><td>null</td><td>0.9839</td><td>&quot;Negative&quot;</td></tr><tr><td>3209311</td><td>&quot;Morder truer med nyt drab&quot;</td><td>&quot;En morder er blevet varetægtsf…</td><td>2023-06-29 06:23:35</td><td>false</td><td>&quot;En morder er i retten i Koldin…</td><td>2003-03-20 12:50:00</td><td>null</td><td>&quot;article_default&quot;</td><td>&quot;https://ekstrabladet.dk/krimi/…</td><td>[&quot;Torben Pedersen&quot;]</td><td>[&quot;PER&quot;]</td><td>[&quot;Kriminalitet&quot;, &quot;Personfarlig kriminalitet&quot;]</td><td>140</td><td>[]</td><td>&quot;krimi&quot;</td><td>null</td><td>null</td><td>null</td><td>0.9975</td><td>&quot;Negative&quot;</td></tr><tr><td>3209357</td><td>&quot;Pædofil må stadig undervise bø…</td><td>&quot;Lærer havde 700 børnepornobill…</td><td>2023-06-29 06:23:35</td><td>false</td><td>&quot;En 56-årig sønderjysk lærer må…</td><td>2005-02-26 04:45:00</td><td>[3069815]</td><td>&quot;article_default&quot;</td><td>&quot;https://ekstrabladet.dk/krimi/…</td><td>[]</td><td>[]</td><td>[&quot;Kriminalitet&quot;, &quot;Personfarlig kriminalitet&quot;, … &quot;Uddannelse&quot;]</td><td>140</td><td>[]</td><td>&quot;krimi&quot;</td><td>null</td><td>null</td><td>null</td><td>0.7929</td><td>&quot;Negative&quot;</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (10, 21)\n",
       "┌───────────┬───────────┬───────────┬───────────┬───┬───────────┬───────────┬───────────┬──────────┐\n",
       "│ article_i ┆ title     ┆ subtitle  ┆ last_modi ┆ … ┆ total_pag ┆ total_rea ┆ sentiment ┆ sentimen │\n",
       "│ d         ┆ ---       ┆ ---       ┆ fied_time ┆   ┆ eviews    ┆ d_time    ┆ _score    ┆ t_label  │\n",
       "│ ---       ┆ str       ┆ str       ┆ ---       ┆   ┆ ---       ┆ ---       ┆ ---       ┆ ---      │\n",
       "│ i32       ┆           ┆           ┆ datetime[ ┆   ┆ i32       ┆ f32       ┆ f32       ┆ str      │\n",
       "│           ┆           ┆           ┆ μs]       ┆   ┆           ┆           ┆           ┆          │\n",
       "╞═══════════╪═══════════╪═══════════╪═══════════╪═══╪═══════════╪═══════════╪═══════════╪══════════╡\n",
       "│ 3037230   ┆ Ishockey- ┆ ISHOCKEY: ┆ 2023-06-2 ┆ … ┆ null      ┆ null      ┆ 0.9752    ┆ Negative │\n",
       "│           ┆ spiller:  ┆ Ishockey- ┆ 9         ┆   ┆           ┆           ┆           ┆          │\n",
       "│           ┆ Jeg       ┆ spilleren ┆ 06:20:57  ┆   ┆           ┆           ┆           ┆          │\n",
       "│           ┆ troede j… ┆ S…        ┆           ┆   ┆           ┆           ┆           ┆          │\n",
       "│ 3044020   ┆ Prins     ┆ Hoffet    ┆ 2023-06-2 ┆ … ┆ null      ┆ null      ┆ 0.7084    ┆ Negative │\n",
       "│           ┆ Harry     ┆ tvang     ┆ 9         ┆   ┆           ┆           ┆           ┆          │\n",
       "│           ┆ tvunget   ┆ Prins     ┆ 06:21:16  ┆   ┆           ┆           ┆           ┆          │\n",
       "│           ┆ til       ┆ Harry til ┆           ┆   ┆           ┆           ┆           ┆          │\n",
       "│           ┆ dna-te…   ┆ a…        ┆           ┆   ┆           ┆           ┆           ┆          │\n",
       "│ 3057622   ┆ Rådden    ┆ Kan ikke  ┆ 2023-06-2 ┆ … ┆ null      ┆ null      ┆ 0.9236    ┆ Negative │\n",
       "│           ┆ kørsel på ┆ straffes: ┆ 9         ┆   ┆           ┆           ┆           ┆          │\n",
       "│           ┆ blå       ┆ Udenlands ┆ 06:21:24  ┆   ┆           ┆           ┆           ┆          │\n",
       "│           ┆ plader    ┆ ke…       ┆           ┆   ┆           ┆           ┆           ┆          │\n",
       "│ 3073151   ┆ Mærsk-arv ┆ FANGET I  ┆ 2023-06-2 ┆ … ┆ null      ┆ null      ┆ 0.9945    ┆ Negative │\n",
       "│           ┆ inger i   ┆ FLODBØLGE ┆ 9         ┆   ┆           ┆           ┆           ┆          │\n",
       "│           ┆ livsfare  ┆ N: Skibsr ┆ 06:21:38  ┆   ┆           ┆           ┆           ┆          │\n",
       "│           ┆           ┆ ede…      ┆           ┆   ┆           ┆           ┆           ┆          │\n",
       "│ 3193383   ┆ Skød      ┆ 44-årig   ┆ 2023-06-2 ┆ … ┆ null      ┆ null      ┆ 0.9966    ┆ Negative │\n",
       "│           ┆ svigersøn ┆ kvinde    ┆ 9         ┆   ┆           ┆           ┆           ┆          │\n",
       "│           ┆ gennem    ┆ tiltalt   ┆ 06:22:57  ┆   ┆           ┆           ┆           ┆          │\n",
       "│           ┆ babydyne  ┆ for dra…  ┆           ┆   ┆           ┆           ┆           ┆          │\n",
       "│ 3196611   ┆ Zoo-tårne ┆ I mange   ┆ 2023-06-2 ┆ … ┆ null      ┆ null      ┆ 0.6275    ┆ Neutral  │\n",
       "│           ┆ t 100 år  ┆ år var    ┆ 9         ┆   ┆           ┆           ┆           ┆          │\n",
       "│           ┆           ┆ det god   ┆ 06:23:02  ┆   ┆           ┆           ┆           ┆          │\n",
       "│           ┆           ┆ latin a…  ┆           ┆   ┆           ┆           ┆           ┆          │\n",
       "│ 3200325   ┆ Tævet     ┆ Sadomasoc ┆ 2023-06-2 ┆ … ┆ null      ┆ null      ┆ 0.9913    ┆ Negative │\n",
       "│           ┆ ihjel på  ┆ histisk   ┆ 9         ┆   ┆           ┆           ┆           ┆          │\n",
       "│           ┆ tre       ┆ sex-guru: ┆ 06:23:13  ┆   ┆           ┆           ┆           ┆          │\n",
       "│           ┆ kvarter   ┆ - H…      ┆           ┆   ┆           ┆           ┆           ┆          │\n",
       "│ 3200913   ┆ Denne kæp ┆ Nye spor  ┆ 2023-06-2 ┆ … ┆ null      ┆ null      ┆ 0.9839    ┆ Negative │\n",
       "│           ┆ kan fælde ┆ i den bes ┆ 9         ┆   ┆           ┆           ┆           ┆          │\n",
       "│           ┆ voldtægts ┆ tialske   ┆ 06:23:15  ┆   ┆           ┆           ┆           ┆          │\n",
       "│           ┆ m…        ┆ vold…     ┆           ┆   ┆           ┆           ┆           ┆          │\n",
       "│ 3209311   ┆ Morder    ┆ En morder ┆ 2023-06-2 ┆ … ┆ null      ┆ null      ┆ 0.9975    ┆ Negative │\n",
       "│           ┆ truer med ┆ er blevet ┆ 9         ┆   ┆           ┆           ┆           ┆          │\n",
       "│           ┆ nyt drab  ┆ varetægts ┆ 06:23:35  ┆   ┆           ┆           ┆           ┆          │\n",
       "│           ┆           ┆ f…        ┆           ┆   ┆           ┆           ┆           ┆          │\n",
       "│ 3209357   ┆ Pædofil   ┆ Lærer     ┆ 2023-06-2 ┆ … ┆ null      ┆ null      ┆ 0.7929    ┆ Negative │\n",
       "│           ┆ må stadig ┆ havde 700 ┆ 9         ┆   ┆           ┆           ┆           ┆          │\n",
       "│           ┆ undervise ┆ børneporn ┆ 06:23:35  ┆   ┆           ┆           ┆           ┆          │\n",
       "│           ┆ bø…       ┆ obill…    ┆           ┆   ┆           ┆           ┆           ┆          │\n",
       "└───────────┴───────────┴───────────┴───────────┴───┴───────────┴───────────┴───────────┴──────────┘"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_articles = pl.read_parquet(PATH.joinpath(\"articles.parquet\"))\n",
    "df_articles.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n",
      "DataFrame after tokenization:\n"
     ]
    }
   ],
   "source": [
    "TRANSFORMER_MODEL_NAME = \"FacebookAI/xlm-roberta-base\"\n",
    "TEXT_COLUMNS_TO_USE = [DEFAULT_SUBTITLE_COL, DEFAULT_TITLE_COL]\n",
    "MAX_TITLE_LENGTH = 30\n",
    "from transformers import ConvBertTokenizer, ConvBertModel\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(device)\n",
    "\n",
    "# LOAD HUGGINGFACE and move to device immediately:\n",
    "# transformer_model = AutoModel.from_pretrained(TRANSFORMER_MODEL_NAME).to(device)\n",
    "# transformer_tokenizer = AutoTokenizer.from_pretrained(TRANSFORMER_MODEL_NAME)\n",
    "\n",
    "transformer_tokenizer = ConvBertTokenizer.from_pretrained(\"sarnikowski/convbert-small-da-cased\")\n",
    "transformer_model = ConvBertModel.from_pretrained(\"sarnikowski/convbert-small-da-cased\")\n",
    "\n",
    "# We'll init the word embeddings using the\n",
    "word2vec_embedding = get_transformers_word_embeddings(transformer_model)\n",
    "\n",
    "# Concatenate text columns\n",
    "df_articles, cat_cal = concat_str_columns(df_articles, columns=TEXT_COLUMNS_TO_USE)\n",
    "\n",
    "# Get tokenized version\n",
    "df_articles, token_col_title = convert_text2encoding_with_transformers(\n",
    "    df_articles, transformer_tokenizer, cat_cal, max_length=MAX_TITLE_LENGTH\n",
    ")\n",
    "\n",
    "print(\"DataFrame after tokenization:\")\n",
    "# print(df_articles[token_col_title][0].shape)\n",
    "article_mapping = create_article_id_to_value_mapping(df=df_articles, value_col=token_col_title)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# print(len(df_articles[\"subtitle-title_encode_FacebookAI/xlm-roberta-base\"][0]))\n",
    "# print(len(df_articles[\"subtitle-title_encode_FacebookAI/xlm-roberta-base\"][1]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embedding tokenized article title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from tqdm import tqdm\n",
    "# from utils._python import batch_items_generator\n",
    "\n",
    "\n",
    "# BATCH_SIZE = 32\n",
    "# n_batches = int(np.ceil(df_articles.height / BATCH_SIZE))\n",
    "\n",
    "# chunked_text_list = batch_items_generator(df_articles[DEFAULT_TITLE_COL].to_list(), BATCH_SIZE)\n",
    "# embeddings = (\n",
    "#     generate_embeddings_with_transformers(\n",
    "#         model=transformer_model,\n",
    "#         tokenizer=transformer_tokenizer,\n",
    "#         text_list=text_list,\n",
    "#         batch_size=BATCH_SIZE,\n",
    "#         disable_tqdm=True,\n",
    "#     )\n",
    "#     for text_list in tqdm(\n",
    "#         chunked_text_list, desc=\"Encoding\", total=n_batches, unit=\"text\"\n",
    "#     )\n",
    "# )\n",
    "# embeddings = torch.vstack(list(embeddings))\n",
    "# # print(embeddings.shape)\n",
    "# # embedded_title = f\"{DEFAULT_TITLE_COL}_embedded\"\n",
    "\n",
    "# # df_articles = df_articles.with_columns(pl.Series(embedded_title, embeddings.to(\"cpu\").numpy()))\n",
    "\n",
    "# # article_mapping = create_article_id_to_value_mapping(\n",
    "# #     df=df_articles, value_col=embedded_title\n",
    "# # )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(embeddings[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reduce dimensionality of embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.decomposition import PCA\n",
    "# from sklearn.manifold import TSNE\n",
    "# from sklearn.metrics import explained_variance_score\n",
    "# import numpy as np\n",
    "\n",
    "# def reduce_and_analyze_dimensionality(embeddings_array, target_dims=[24, 32, 64, 128, 256]):\n",
    "#     \"\"\"\n",
    "#     Reduce dimensionality using different methods and analyze information retention\n",
    "#     \"\"\"\n",
    "#     results = {}\n",
    "    \n",
    "#     # PCA Analysis for different dimensions\n",
    "#     for dim in target_dims:\n",
    "#         # PCA\n",
    "#         pca = PCA(n_components=dim)\n",
    "#         reduced_data_pca = pca.fit_transform(embeddings_array)\n",
    "        \n",
    "#         # Calculate explained variance ratio\n",
    "#         explained_var = np.sum(pca.explained_variance_ratio_) * 100\n",
    "        \n",
    "#         results[dim] = {\n",
    "#             'method': 'PCA',\n",
    "#             'explained_variance_ratio': explained_var,\n",
    "#             'reduced_data': reduced_data_pca\n",
    "#         }\n",
    "        \n",
    "#         print(f\"\\nDimensionality Reduction to {dim} dimensions:\")\n",
    "#         print(f\"Explained variance ratio (PCA): {explained_var:.2f}%\")\n",
    "#         print(f\"Shape after reduction: {reduced_data_pca.shape}\")\n",
    "\n",
    "#     return results\n",
    "\n",
    "# # Convert embeddings to numpy array if it's not already\n",
    "# embeddings_numpy = embeddings.cpu().numpy()\n",
    "\n",
    "# # Analyze different dimensionality reductions\n",
    "# reduction_results = reduce_and_analyze_dimensionality(embeddings_numpy)\n",
    "\n",
    "# # Choose the dimension that provides good balance \n",
    "# # between compression and information retention\n",
    "# chosen_dim = hparams_nrms.__dict__['title_size']  # Adjust based on analysis results\n",
    "# pca = PCA(n_components=chosen_dim)\n",
    "# reduced_embeddings = pca.fit_transform(embeddings_numpy)\n",
    "\n",
    "# # Update the dataframe with reduced embeddings\n",
    "# embedded_title = f\"{DEFAULT_TITLE_COL}_embedded_reduced\"\n",
    "# df_articles = df_articles.with_columns(pl.Series(embedded_title, reduced_embeddings))\n",
    "\n",
    "# # Create new article mapping with reduced embeddings\n",
    "# article_mapping = create_article_id_to_value_mapping(\n",
    "#     df=df_articles, value_col=embedded_title\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Init dataloaders\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting preprocessing...\n",
      "Preprocessing data...\n",
      "(24, 6)\n",
      "Data preprocessing completed in 0.01 seconds.\n",
      "Starting preprocessing...\n",
      "Preprocessing data...\n",
      "(25, 6)\n",
      "Data preprocessing completed in 0.02 seconds.\n"
     ]
    }
   ],
   "source": [
    "train_dataset = NRMSDataSet(\n",
    "    behaviors=df_train,\n",
    "    article_dict=article_mapping,\n",
    "    unknown_representation=\"zeros\",\n",
    "    history_column=DEFAULT_HISTORY_ARTICLE_ID_COL,\n",
    "    eval_mode=False,\n",
    ")\n",
    "val_dataset = NRMSDataSet(\n",
    "    behaviors=df_validation,\n",
    "    article_dict=article_mapping,\n",
    "    unknown_representation=\"zeros\",\n",
    "    history_column=DEFAULT_HISTORY_ARTICLE_ID_COL,\n",
    "    eval_mode=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample 0:\n",
      "his_input_title shape: torch.Size([20, 30])\n",
      "pred_input_title shape: torch.Size([5, 30]) 474249.0\n",
      "Targets shape: torch.Size([5]) , torch.float32 1.0\n",
      "impression id: 486612408\n",
      "Sample 1:\n",
      "his_input_title shape: torch.Size([20, 30])\n",
      "pred_input_title shape: torch.Size([5, 30]) 523534.0\n",
      "Targets shape: torch.Size([5]) , torch.float32 1.0\n",
      "impression id: 155019967\n",
      "Sample 2:\n",
      "his_input_title shape: torch.Size([20, 30])\n",
      "pred_input_title shape: torch.Size([5, 30]) 746970.0\n",
      "Targets shape: torch.Size([5]) , torch.float32 1.0\n",
      "impression id: 297828934\n",
      "Sample 3:\n",
      "his_input_title shape: torch.Size([20, 30])\n",
      "pred_input_title shape: torch.Size([5, 30]) 780423.0\n",
      "Targets shape: torch.Size([5]) , torch.float32 1.0\n",
      "impression id: 184682178\n",
      "Sample 4:\n",
      "his_input_title shape: torch.Size([20, 30])\n",
      "pred_input_title shape: torch.Size([5, 30]) 553661.0\n",
      "Targets shape: torch.Size([5]) , torch.float32 1.0\n",
      "impression id: 491029215\n"
     ]
    }
   ],
   "source": [
    "for idx in range(5):\n",
    "    sample = train_dataset[idx]\n",
    "    print(f\"Sample {idx}:\")\n",
    "    print(f\"his_input_title shape: {sample[0][0].shape}\")\n",
    "    print(f\"pred_input_title shape: {sample[0][1].shape} {sample[0][1].sum()}\")\n",
    "    print(f\"Targets shape: {sample[1].shape} , {sample[1].dtype} {sample[1].sum()}\")\n",
    "    print(f\"impression id: {sample[2]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "\n",
    "def collate_fn_with_global_padding(batch, max_len_pred, apply_padding_to_targets : bool = True):\n",
    "    try:\n",
    "        his_input_titles = [item[0][0] for item in batch]  # History inputs\n",
    "        pred_input_titles = [item[0][1] for item in batch]  # Prediction inputs\n",
    "        batch_ys = [item[1] for item in batch]  # Targets\n",
    "        impression_id = torch.tensor([item[2] for item in batch], dtype=torch.int64)  # Impression ID\n",
    "        \n",
    "\n",
    "        # Pad sequences to the global maximum length\n",
    "        his_input_titles_padded = pad_sequence(his_input_titles, batch_first=True, padding_value=0)\n",
    "\n",
    "        # Pad prediction inputs and adjust to the global maximum length\n",
    "        pred_input_titles_padded = pad_sequence(pred_input_titles, batch_first=True, padding_value=0)\n",
    "        if pred_input_titles_padded.size(1) < max_len_pred:\n",
    "            # Add padding if sequence length is shorter than max_len_pred\n",
    "            pad_size = max_len_pred - pred_input_titles_padded.size(1)\n",
    "            pred_input_titles_padded = torch.nn.functional.pad(\n",
    "                pred_input_titles_padded, (0, 0, 0, pad_size), value=0\n",
    "            )\n",
    "        elif pred_input_titles_padded.size(1) > max_len_pred:\n",
    "            # Trim if sequence length exceeds max_len_pred\n",
    "            pred_input_titles_padded = pred_input_titles_padded[:, :max_len_pred, :]\n",
    "\n",
    "        # Pad targets to the global maximum length\n",
    "        if apply_padding_to_targets:\n",
    "            batch_ys_padded = pad_sequence(batch_ys, batch_first=True, padding_value=-1)\n",
    "            if batch_ys_padded.size(1) < max_len_pred:\n",
    "                pad_size = max_len_pred - batch_ys_padded.size(1)\n",
    "                batch_ys_padded = torch.nn.functional.pad(batch_ys_padded, (0, pad_size), value=-1)\n",
    "            elif batch_ys_padded.size(1) > max_len_pred:\n",
    "                batch_ys_padded = batch_ys_padded[:, :max_len_pred]\n",
    "\n",
    "            return (his_input_titles_padded, pred_input_titles_padded), batch_ys_padded, impression_id\n",
    "        else:\n",
    "            return (his_input_titles_padded, pred_input_titles_padded), batch_ys, impression_id\n",
    "    except Exception as e:\n",
    "        print(f\"Error in collate_fn: {e}\")\n",
    "        raise\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wrap the dataset with DataLoader\n",
    "train_dataloader_temp = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=64,    # Set your desired batch size\n",
    "    shuffle=True,\n",
    "    num_workers=0,\n",
    "    collate_fn=lambda batch: collate_fn_with_global_padding(batch, max_inview_length_validation)\n",
    ")\n",
    "\n",
    "val_dataloader_temp = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=128,    # Set your desired batch size\n",
    "    shuffle=False,\n",
    "    num_workers=0,\n",
    "    collate_fn=lambda batch: collate_fn_with_global_padding(batch, max_inview_length_validation)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([25, 33, 30])\n",
      "torch.Size([25, 33])\n",
      "tensor([182849320, 571321560, 206470750, 488114430, 473299626, 225448361,\n",
      "        398159868, 171284778, 490813872, 531715016, 351152973,  87157375,\n",
      "        226614052, 340044447, 169735445, 311360947, 174929855, 288156053,\n",
      "        243342974, 196100740, 330496456, 437928095, 321537402, 223718445,\n",
      "         70859308])\n",
      "Batch loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "for batch in val_dataloader_temp:\n",
    "    (his_input_titles_padded, pred_input_titles_padded), batch_ys_padded, impression_id = batch\n",
    "    print(pred_input_titles_padded.shape)  # Look at one padded sequence\n",
    "    print(batch_ys_padded.shape)  # Look at one padded sequence\n",
    "    print(impression_id)\n",
    "\n",
    "    print(\"Batch loaded successfully!\")\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # THIS CODE SHOULD ONLY RUN WHEN GENERATING THE DATA FOR THE FIRST TIME\n",
    "# import torch\n",
    "# from tqdm import tqdm\n",
    "\n",
    "# # Function to preprocess and save data\n",
    "# def preprocess_and_save(dataloader, filepath, device=\"cuda\"):\n",
    "#     all_inputs_his = []\n",
    "#     all_inputs_pred = []\n",
    "#     all_targets = []\n",
    "#     all_impression_ids = []\n",
    "\n",
    "#     # Iterate over DataLoader and collect data\n",
    "#     for (his_inputs, pred_inputs), targets, impressionID in tqdm(dataloader, desc=\"Processing Data\"):\n",
    "#         all_inputs_his.append(his_inputs)\n",
    "#         all_inputs_pred.append(pred_inputs)\n",
    "#         all_targets.append(targets)\n",
    "#         all_impression_ids.append(impressionID)\n",
    "\n",
    "#     # Concatenate all batches into a single tensor\n",
    "#     all_inputs_his = torch.cat(all_inputs_his).to(device)\n",
    "#     all_inputs_pred = torch.cat(all_inputs_pred).to(device)\n",
    "#     all_targets = torch.cat(all_targets).to(device)\n",
    "#     all_impression_ids = torch.cat(all_impression_ids).to(device)\n",
    "\n",
    "#     # Save the preprocessed data as a tuple\n",
    "#     torch.save((all_inputs_his, all_inputs_pred, all_targets, all_impression_ids), filepath)\n",
    "#     print(f\"Data saved to {filepath}\")\n",
    "\n",
    "# # Save train and validation data\n",
    "# preprocess_and_save(val_dataloader_temp, \"val_data_small_dataset_with_impression_ids.pt\", device=\"cuda\")\n",
    "\n",
    "# preprocess_and_save(train_dataloader_temp, \"train_data_small_dataset_with_impression_ids.pt\", device=\"cuda\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def load_preprocessed_data(filepath, device=\"cuda\"):\n",
    "#     # Load the data from the .pt file\n",
    "#     data = torch.load(filepath)\n",
    "\n",
    "#     # Unpack the data\n",
    "#     his_inputs, pred_inputs, targets, impression_ids = data\n",
    "\n",
    "#     # Move the data to the specified device\n",
    "#     his_inputs = his_inputs.to(device, non_blocking=True)\n",
    "#     pred_inputs = pred_inputs.to(device, non_blocking=True)\n",
    "#     targets = targets.to(device, non_blocking=True)\n",
    "#     impression_ids = impression_ids.to(device, non_blocking=True)\n",
    "\n",
    "#     return his_inputs, pred_inputs, targets, impression_ids\n",
    "\n",
    "# # Example: Load train and validation data\n",
    "# device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "# train_his_inputs, train_pred_inputs, train_targets, impression_ids = load_preprocessed_data(\"train_data_small_dataset_with_impression_ids.pt\", device)\n",
    "# val_his_inputs, val_pred_inputs, val_targets, impression_ids = load_preprocessed_data(\"val_data_small_dataset_with_impression_ids.pt\", device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def create_batches(inputs, targets, impression_ids, batch_size):\n",
    "#     his_inputs, pred_inputs = inputs\n",
    "#     for i in range(0, his_inputs.size(0), batch_size):\n",
    "#         his_batch = his_inputs[i:i+batch_size]\n",
    "#         pred_batch = pred_inputs[i:i+batch_size]\n",
    "#         target_batch = targets[i:i+batch_size]\n",
    "#         impression_id_batch = impression_ids[i:i+batch_size]\n",
    "#         yield (his_batch, pred_batch), target_batch, impression_id_batch\n",
    "\n",
    "# # Set the batch size\n",
    "# batch_size = 64\n",
    "\n",
    "# # Example: Create batches for train and validation data\n",
    "# #train_batches = create_batches((train_his_inputs, train_pred_inputs), train_targets, batch_size)\n",
    "# #val_batches = create_batches((val_his_inputs, val_pred_inputs), val_targets, batch_size)\n",
    "\n",
    "# train_batches = list(create_batches((train_his_inputs, train_pred_inputs), train_targets, impression_ids, batch_size))\n",
    "# val_batches = list(create_batches((val_his_inputs, val_pred_inputs), val_targets, impression_ids, batch_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for (his_batch, pred_batch), target_batch, impression_ids_batch in train_batches:\n",
    "#     print(f\"his_batch device: {his_batch.device}\")\n",
    "#     print(f\"pred_batch device: {pred_batch.device}\")\n",
    "#     print(f\"target_batch device: {target_batch.device}\")\n",
    "#     print(f\"impression_ids_batch device: {impression_ids_batch.device}\")\n",
    "#     break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "device = \"cpu\"\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create hyper-parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'__module__': 'models_pytorch.model_config', '__annotations__': {'title_size': <class 'int'>, 'embedding_dim': <class 'int'>, 'word_emb_dim': <class 'int'>, 'vocab_size': <class 'int'>, 'head_num': <class 'int'>, 'head_dim': <class 'int'>, 'attention_hidden_dim': <class 'int'>, 'optimizer': <class 'str'>, 'loss': <class 'str'>, 'dropout': <class 'float'>, 'learning_rate': <class 'float'>, 'weight_decay': <class 'float'>, 'units_per_layer': list[int]}, 'title_size': 30, 'embedding_dim': 32, 'word_emb_dim': 8, 'vocab_size': 10000, 'head_num': 32, 'head_dim': 128, 'attention_hidden_dim': 200, 'hidden_dim': 4, 'optimizer': 'adam', 'loss': 'cross_entropy_loss', 'dropout': 0.2, 'learning_rate': 0.0001, 'weight_decay': 0.001, 'news_output_dim': 128, 'units_per_layer': [128, 128], '__dict__': <attribute '__dict__' of 'hparams_nrms' objects>, '__weakref__': <attribute '__weakref__' of 'hparams_nrms' objects>, '__doc__': None}\n"
     ]
    }
   ],
   "source": [
    "# see the model parameters: \n",
    "hparams_nrms.attention_hidden_dim = 200\n",
    "hparams_nrms.head_num = 32\n",
    "hparams_nrms.head_dim = 128\n",
    "print(hparams_nrms.__dict__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the NRMS model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "Model device: cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\DTU - Master\\1 semester\\Deep learning (02456)\\Deeplearning-RecSys-Challenge-2024\\models_pytorch\\nrms.py:164: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  self.scaler = amp.GradScaler()\n",
      "c:\\DTU - Master\\1 semester\\Deep learning (02456)\\Deeplearning-RecSys-Challenge-2024\\.venv\\Lib\\site-packages\\torch\\amp\\grad_scaler.py:132: UserWarning: torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "# Define paths\n",
    "MODEL_NAME = \"NRMS\"\n",
    "LOG_DIR = os.path.join(\"downloads\", \"runs\", MODEL_NAME)\n",
    "MODEL_WEIGHTS = os.path.join(\"downloads\", \"data\", \"state_dict\", MODEL_NAME, \"weights.pth\")\n",
    "\n",
    "# Create directories if they don't exist\n",
    "os.makedirs(LOG_DIR, exist_ok=True)\n",
    "os.makedirs(os.path.dirname(MODEL_WEIGHTS), exist_ok=True)\n",
    "\n",
    "# Define ModelCheckpoint class\n",
    "class ModelCheckpoint:\n",
    "    \"\"\"Saves the model after every epoch if it has the best performance so far.\"\"\"\n",
    "    def __init__(self, filepath, verbose=False, save_best_only=True):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            filepath (str): Path to save the model checkpoint.\n",
    "            verbose (bool): If True, prints a message when the model is saved.\n",
    "            save_best_only (bool): If True, saves only when the model is better than before.\n",
    "        \"\"\"\n",
    "        self.filepath = filepath\n",
    "        self.verbose = verbose\n",
    "        self.save_best_only = save_best_only\n",
    "        self.best_loss = None\n",
    "\n",
    "    def __call__(self, model, val_loss):\n",
    "        if self.best_loss is None:\n",
    "            self.best_loss = val_loss\n",
    "            self.save_checkpoint(model)\n",
    "        elif val_loss < self.best_loss:\n",
    "            self.best_loss = val_loss\n",
    "            self.save_checkpoint(model)\n",
    "\n",
    "    def save_checkpoint(self, model):\n",
    "        torch.save(model.state_dict(), self.filepath)\n",
    "        if self.verbose:\n",
    "            print(f\"Model saved to {self.filepath}\")\n",
    "\n",
    "# Define EarlyStopping class\n",
    "class EarlyStopping:\n",
    "    \"\"\"Early stops the training if validation loss doesn't improve by a given percentage over a patience period.\"\"\"\n",
    "    def __init__(self, patience=3, min_delta=0.05, verbose=False):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            patience (int): Number of epochs to wait after last time validation loss improved by min_delta.\n",
    "            min_delta (float): Minimum percentage improvement required to reset patience.\n",
    "            verbose (bool): If True, prints a message when early stopping is triggered.\n",
    "        \"\"\"\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta  # Minimum percentage improvement\n",
    "        self.verbose = verbose\n",
    "        self.counter = 0\n",
    "        self.best_loss = None\n",
    "        self.early_stop = False\n",
    "\n",
    "    def __call__(self, val_loss):\n",
    "        if self.best_loss is None:\n",
    "            # Initialize best_loss with the first validation loss\n",
    "            self.best_loss = val_loss\n",
    "            self.counter = 0\n",
    "        elif val_loss < self.best_loss * (1 - self.min_delta):\n",
    "            # Significant improvement found\n",
    "            self.best_loss = val_loss\n",
    "            self.counter = 0\n",
    "            if self.verbose:\n",
    "                print(f\"Validation loss improved by at least {self.min_delta*100:.1f}%\")\n",
    "        else:\n",
    "            # No significant improvement\n",
    "            self.counter += 1\n",
    "            if self.verbose:\n",
    "                print(f\"No significant improvement in validation loss. Counter: {self.counter}/{self.patience}\")\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "\n",
    "# Initialize TensorBoard SummaryWriter\n",
    "writer = SummaryWriter(log_dir=LOG_DIR)\n",
    "\n",
    "# Initialize callbacks\n",
    "model_checkpoint = ModelCheckpoint(filepath=MODEL_WEIGHTS, verbose=True, save_best_only=True)\n",
    "# Initialize EarlyStopping\n",
    "early_stopping = EarlyStopping(patience=3, min_delta=0.05, verbose=True)\n",
    "\n",
    "# Initialize your model\n",
    "# Ensure that NRMSModel is a PyTorch nn.Module\n",
    "\n",
    "# CUDA checks\n",
    "#print(f\"CUDA Available: {torch.cuda.is_available()}\")\n",
    "#print(f\"Current Device: {torch.cuda.current_device()}\")\n",
    "#print(f\"Device Name: {torch.cuda.get_device_name()}\")\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    \n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Initialize model\n",
    "model = NRMSModel(\n",
    "    hparams=hparams_nrms.__dict__,\n",
    "    word2vec_embedding=word2vec_embedding,\n",
    "    vocab_size=30000,\n",
    "    word_emb_dim=8,\n",
    "    device=device,\n",
    "    feed_forward_layers_after_3rd_layer=True,\n",
    ")\n",
    "\n",
    "# model = NRMSDocVecModel(hparams=hparams_nrms.__dict__,\n",
    "#                         device=device)\n",
    "\n",
    "# model = NRMSModel(hparams=hparams_nrms.__dict__,\n",
    "#                   word2vec_embedding=word2vec_embedding,\n",
    "#                         device=device)\n",
    "print(f\"Model device: {next(model.parameters()).device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def print_prediction_details(outputs, targets, k=5):\n",
    "#     \"\"\"Print detailed prediction information for the first k samples\"\"\"\n",
    "#     # Get predicted class (highest score)\n",
    "#     _, predicted = torch.max(outputs.data, 1)\n",
    "    \n",
    "#     # Calculate accuracy for this batch\n",
    "#     correct = (predicted == targets).sum().item()\n",
    "#     total = targets.size(0)\n",
    "#     accuracy = 100 * correct / total\n",
    "    \n",
    "#     # Print details for k samples\n",
    "#     for i in range(min(k, len(targets))):\n",
    "#         print(f\"\\nSample {i}:\")\n",
    "#         print(f\"Predicted probabilities: {torch.softmax(outputs[i], dim=0)}\")\n",
    "#         print(f\"Predicted class: {predicted[i]}, True class: {targets[i]}\")\n",
    "#         print(f\"Correct: {predicted[i] == targets[i]}\")\n",
    "    \n",
    "#     return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NRMSModel(\n",
      "  (newsencoder): NewsEncoder(\n",
      "    (embedding): Embedding(28995, 128)\n",
      "    (self_attention): SelfAttention(\n",
      "      (multihead_attention): MultiheadAttention(\n",
      "        (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
      "      )\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (attention_layer): AttLayer2(\n",
      "      (V_w): Linear(in_features=128, out_features=200, bias=True)\n",
      "      (q_w): Linear(in_features=200, out_features=1, bias=False)\n",
      "    )\n",
      "    (feedforward): Sequential(\n",
      "      (0): Linear(in_features=128, out_features=128, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (3): Dropout(p=0.2, inplace=False)\n",
      "      (4): Linear(in_features=128, out_features=128, bias=True)\n",
      "      (5): ReLU()\n",
      "      (6): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (7): Dropout(p=0.2, inplace=False)\n",
      "      (8): Linear(in_features=128, out_features=128, bias=True)\n",
      "      (9): ReLU()\n",
      "    )\n",
      "  )\n",
      "  (userencoder): UserEncoder(\n",
      "    (titleencoder): NewsEncoder(\n",
      "      (embedding): Embedding(28995, 128)\n",
      "      (self_attention): SelfAttention(\n",
      "        (multihead_attention): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
      "        )\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (attention_layer): AttLayer2(\n",
      "        (V_w): Linear(in_features=128, out_features=200, bias=True)\n",
      "        (q_w): Linear(in_features=200, out_features=1, bias=False)\n",
      "      )\n",
      "      (feedforward): Sequential(\n",
      "        (0): Linear(in_features=128, out_features=128, bias=True)\n",
      "        (1): ReLU()\n",
      "        (2): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (3): Dropout(p=0.2, inplace=False)\n",
      "        (4): Linear(in_features=128, out_features=128, bias=True)\n",
      "        (5): ReLU()\n",
      "        (6): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (7): Dropout(p=0.2, inplace=False)\n",
      "        (8): Linear(in_features=128, out_features=128, bias=True)\n",
      "        (9): ReLU()\n",
      "      )\n",
      "    )\n",
      "    (self_attention): SelfAttention(\n",
      "      (multihead_attention): MultiheadAttention(\n",
      "        (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
      "      )\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (attention_layer): AttLayer2(\n",
      "      (V_w): Linear(in_features=128, out_features=200, bias=True)\n",
      "      (q_w): Linear(in_features=200, out_features=1, bias=False)\n",
      "    )\n",
      "    (user_projection): Linear(in_features=128, out_features=128, bias=True)\n",
      "  )\n",
      ")\n",
      "Layer: newsencoder.embedding.weight | Size: torch.Size([28995, 128])\n",
      "Layer: newsencoder.self_attention.multihead_attention.in_proj_weight | Size: torch.Size([384, 128])\n",
      "Layer: newsencoder.self_attention.multihead_attention.in_proj_bias | Size: torch.Size([384])\n",
      "Layer: newsencoder.self_attention.multihead_attention.out_proj.weight | Size: torch.Size([128, 128])\n",
      "Layer: newsencoder.self_attention.multihead_attention.out_proj.bias | Size: torch.Size([128])\n",
      "Layer: newsencoder.attention_layer.V_w.weight | Size: torch.Size([200, 128])\n",
      "Layer: newsencoder.attention_layer.V_w.bias | Size: torch.Size([200])\n",
      "Layer: newsencoder.attention_layer.q_w.weight | Size: torch.Size([1, 200])\n",
      "Layer: newsencoder.feedforward.0.weight | Size: torch.Size([128, 128])\n",
      "Layer: newsencoder.feedforward.0.bias | Size: torch.Size([128])\n",
      "Layer: newsencoder.feedforward.2.weight | Size: torch.Size([128])\n",
      "Layer: newsencoder.feedforward.2.bias | Size: torch.Size([128])\n",
      "Layer: newsencoder.feedforward.4.weight | Size: torch.Size([128, 128])\n",
      "Layer: newsencoder.feedforward.4.bias | Size: torch.Size([128])\n",
      "Layer: newsencoder.feedforward.6.weight | Size: torch.Size([128])\n",
      "Layer: newsencoder.feedforward.6.bias | Size: torch.Size([128])\n",
      "Layer: newsencoder.feedforward.8.weight | Size: torch.Size([128, 128])\n",
      "Layer: newsencoder.feedforward.8.bias | Size: torch.Size([128])\n",
      "Layer: userencoder.self_attention.multihead_attention.in_proj_weight | Size: torch.Size([384, 128])\n",
      "Layer: userencoder.self_attention.multihead_attention.in_proj_bias | Size: torch.Size([384])\n",
      "Layer: userencoder.self_attention.multihead_attention.out_proj.weight | Size: torch.Size([128, 128])\n",
      "Layer: userencoder.self_attention.multihead_attention.out_proj.bias | Size: torch.Size([128])\n",
      "Layer: userencoder.attention_layer.V_w.weight | Size: torch.Size([200, 128])\n",
      "Layer: userencoder.attention_layer.V_w.bias | Size: torch.Size([200])\n",
      "Layer: userencoder.attention_layer.q_w.weight | Size: torch.Size([1, 200])\n",
      "Layer: userencoder.user_projection.weight | Size: torch.Size([128, 128])\n",
      "Layer: userencoder.user_projection.bias | Size: torch.Size([128])\n",
      "\n",
      "Total parameters: 3,962,016\n"
     ]
    }
   ],
   "source": [
    "# 1. Print model architecture\n",
    "print(model)\n",
    "\n",
    "# 2. Print specific layer sizes\n",
    "for name, param in model.named_parameters():\n",
    "    print(f\"Layer: {name} | Size: {param.size()}\")\n",
    "\n",
    "# 3. Get total number of parameters\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"\\nTotal parameters: {total_params:,}\")\n",
    "\n",
    "# 4. Print layer by layer with shapes\n",
    "def print_model_structure(model):\n",
    "    print(\"\\nDetailed Model Structure:\")\n",
    "    for name, module in model.named_children():\n",
    "        print(f\"\\nLayer: {name}\")\n",
    "        print(f\"Type: {type(module).__name__}\")\n",
    "        if hasattr(module, 'weight'):\n",
    "            print(f\"Shape: {module.weight.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for name, param in model.named_parameters():\n",
    "#     print(f\"Layer: {name} | Size: {param.size()} | Parameters: {param.numel()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MAX_GRAD_NORM = np.sqrt(sum(p.numel() for p in model.parameters()))\n",
    "# print(f\"Max grad norm: {MAX_GRAD_NORM}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###\n",
    "Added gradient clipping to avoid exploiding gradients. The paramter MAX_GRAD_NORM is set to 5.0 as this is a common value used in transformers. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# # Variables to track counts\n",
    "# total_inputs = 0\n",
    "# total_targets = 0\n",
    "\n",
    "# # Iterate over the DataLoader\n",
    "# for batch_idx, (inputs, targets, impression_ids) in enumerate(train_dataloader_temp):\n",
    "#     # Move data to GPU\n",
    "#     inputs = [inp.to(device) for inp in inputs]\n",
    "#     targets = targets.to(device)\n",
    "#     impression_ids = impression_ids.to(device)\n",
    "    \n",
    "#     # Print information for the first few batches to avoid delays\n",
    "#     if batch_idx < 5:  # Adjust the number of batches to print as needed\n",
    "#         print(f\"Batch {batch_idx + 1} (on {device}):\")\n",
    "#         print(f\"  - Number of inputs: {len(inputs[0])}\")  # History input\n",
    "#         print(f\"  - Number of targets: {len(targets)}\")   # Target labels\n",
    "#         print(f\"  - Impression IDs: {len(impression_ids)}\")\n",
    "    \n",
    "#     # Update total counts\n",
    "#     total_inputs += len(inputs[0])\n",
    "#     total_targets += len(targets)\n",
    "\n",
    "# # Final counts after iteration\n",
    "# print(f\"\\nTotal number of inputs in train_dataloader_temp: {total_inputs}\")\n",
    "# print(f\"Total number of targets in train_dataloader_temp: {total_targets}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperoptimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NUM_EPOCHS = 30\n",
    "\n",
    "# def objective(trial):\n",
    "#     # Define hyperparameter search space\n",
    "#     hparams = {\n",
    "#         'title_size': 768,\n",
    "#         'history_size': trial.suggest_int('history_size', 5, 20),\n",
    "#         'head_num': trial.suggest_int('head_num', 2, 8),\n",
    "#         'head_dim': trial.suggest_int('head_dim', 4, 16),\n",
    "#         'attention_hidden_dim': trial.suggest_int('attention_hidden_dim', 32, 128),\n",
    "#         'dropout': trial.suggest_float('dropout', 0.1, 0.5),\n",
    "#         'learning_rate': trial.suggest_float('learning_rate', 1e-5, 1e-3, log=True),\n",
    "#         'news_output_dim': trial.suggest_int('news_output_dim', 32, 128),\n",
    "#         'units_per_layer': [trial.suggest_int(f'unit_layer_{i}', 32, 128) for i in range(3)]\n",
    "#     }\n",
    "\n",
    "#     # Initialize model and training components\n",
    "#     model = NRMSDocVecModel(hparams=hparams, device=device)\n",
    "#     criterion = model.get_loss().to(device)\n",
    "#     optimizer = optim.Adam(model.parameters(), \n",
    "#                           lr=hparams['learning_rate'], \n",
    "#                           weight_decay=WEIGHT_DECAY)\n",
    "    \n",
    "#     # Initialize EarlyStopping\n",
    "#     early_stopping = EarlyStopping(patience=3, min_delta=0.05, verbose=True)\n",
    "#     best_val_loss = float('inf')\n",
    "\n",
    "#     for epoch in range(NUM_EPOCHS):\n",
    "#         # Training phase\n",
    "#         model.train()\n",
    "#         train_loss = train_one_epoch(model, train_dataloader_temp, optimizer, criterion)\n",
    "        \n",
    "#         # Validation phase\n",
    "#         val_loss = validate(model, val_dataloader_temp, criterion)\n",
    "        \n",
    "#         # Update best validation loss\n",
    "#         if val_loss < best_val_loss:\n",
    "#             best_val_loss = val_loss\n",
    "\n",
    "#         # Early stopping check\n",
    "#         early_stopping(val_loss)\n",
    "#         if early_stopping.early_stop:\n",
    "#             print(f\"Early stopping triggered at epoch {epoch}\")\n",
    "#             break\n",
    "\n",
    "#         # Report to Optuna\n",
    "#         trial.report(val_loss, epoch)\n",
    "#         if trial.should_prune():\n",
    "#             raise optuna.TrialPruned()\n",
    "\n",
    "#     return best_val_loss\n",
    "\n",
    "# def train_one_epoch(model, dataloader, optimizer, criterion):\n",
    "#     running_loss = 0.0\n",
    "#     batch_count = 0\n",
    "    \n",
    "#     for inputs, targets, impression_ids in dataloader:\n",
    "#         inputs = [inp.to(device) for inp in inputs]\n",
    "#         targets = targets.to(device)\n",
    "#         positive_indices = (targets == 1).nonzero(as_tuple=False)\n",
    "#         targets = positive_indices[:, 1].long()\n",
    "        \n",
    "#         optimizer.zero_grad()\n",
    "#         outputs = model(*inputs)\n",
    "#         loss = criterion(outputs, targets)\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "        \n",
    "#         running_loss += loss.item()\n",
    "#         batch_count += 1\n",
    "    \n",
    "#     return running_loss / batch_count\n",
    "\n",
    "# def validate(model, dataloader, criterion):\n",
    "#     model.eval()\n",
    "#     val_loss = 0.0\n",
    "#     batch_count = 0\n",
    "    \n",
    "#     with torch.no_grad():\n",
    "#         for inputs, targets, impression_ids in dataloader:\n",
    "#             inputs = [inp.to(device) for inp in inputs]\n",
    "#             targets = targets.to(device)\n",
    "#             positive_indices = (targets == 1).nonzero(as_tuple=False)\n",
    "#             targets = positive_indices[:, 1].long()\n",
    "#             outputs = model(*inputs)\n",
    "#             loss = criterion(outputs, targets)\n",
    "#             val_loss += loss.item()\n",
    "#             batch_count += 1\n",
    "    \n",
    "#     return val_loss / batch_count\n",
    "\n",
    "# # Create study and optimize\n",
    "# study = optuna.create_study(direction='minimize')\n",
    "# study.optimize(objective, n_trials=3)  # Run 50 trials\n",
    "\n",
    "# # Print results\n",
    "# print(\"Best trial:\")\n",
    "# trial = study.best_trial\n",
    "\n",
    "# print(\"  Value: \", trial.value)\n",
    "# print(\"  Params: \")\n",
    "# for key, value in trial.params.items():\n",
    "#     print(f\"    {key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the NRMS model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:   0%|          | 0/15 [00:00<?, ?it/s]c:\\DTU - Master\\1 semester\\Deep learning (02456)\\Deeplearning-RecSys-Challenge-2024\\models_pytorch\\nrms.py:181: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast():\n",
      "c:\\DTU - Master\\1 semester\\Deep learning (02456)\\Deeplearning-RecSys-Challenge-2024\\.venv\\Lib\\site-packages\\torch\\amp\\autocast_mode.py:266: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
      "  warnings.warn(\n",
      "Training Progress:  67%|██████▋   | 10/15 [00:16<00:07,  1.52s/it, train_loss=1.6653, val_loss=3.4960]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to downloads\\data\\state_dict\\NRMS\\weights.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress: 100%|██████████| 15/15 [00:25<00:00,  1.72s/it, train_loss=1.3331, val_loss=3.4938]\n"
     ]
    }
   ],
   "source": [
    "NUM_EPOCHS = 15\n",
    "\n",
    "# Define loss function and optimizer\n",
    "criterion = model.get_loss().to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=hparams_nrms.__dict__['learning_rate'], weight_decay=hparams_nrms.__dict__['weight_decay'])\n",
    "\n",
    "# Training parameters\n",
    "train_losses, val_losses = [], []\n",
    "\n",
    "# Epoch progress bar\n",
    "epoch_pbar = tqdm(range(1, NUM_EPOCHS + 1), desc=\"Training Progress\", dynamic_ncols=True)\n",
    "for epoch in epoch_pbar:\n",
    "    # Training phase\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    train_batch_count = 0\n",
    "\n",
    "    for batch_idx, (inputs, targets, impression_ids) in enumerate(train_dataloader_temp):\n",
    "        # Prepare data\n",
    "        inputs = [inp.to(device) for inp in inputs]\n",
    "        targets = targets.to(device)\n",
    "\n",
    "        # Get positive labels\n",
    "        positive_indices = (targets == 1).nonzero(as_tuple=False)\n",
    "        targets = positive_indices[:, 1].long()\n",
    "\n",
    "        # Forward and backward passes\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(*inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Update running statistics\n",
    "        running_loss += loss.item()\n",
    "        train_batch_count += 1\n",
    "\n",
    "    # Compute average training loss\n",
    "    avg_train_loss = running_loss / train_batch_count if train_batch_count > 0 else float('inf')\n",
    "    train_losses.append(avg_train_loss)\n",
    "\n",
    "    # Validation phase\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    val_batch_count = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, targets, impression_ids in val_dataloader_temp:\n",
    "            inputs = [inp.to(device) for inp in inputs]\n",
    "            targets = targets.to(device)\n",
    "            positive_indices = (targets == 1).nonzero(as_tuple=False)\n",
    "            targets = positive_indices[:, 1].long()\n",
    "            outputs = model(*inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "\n",
    "            val_loss += loss.item()\n",
    "            val_batch_count += 1\n",
    "\n",
    "    avg_val_loss = val_loss / val_batch_count if val_batch_count > 0 else float('inf')\n",
    "    val_losses.append(avg_val_loss)\n",
    "\n",
    "    # Update tensorboard\n",
    "    writer.add_scalar('Loss/Train', avg_train_loss, epoch)\n",
    "    writer.add_scalar('Loss/Validation', avg_val_loss, epoch)\n",
    "\n",
    "    # Update epoch progress bar with metrics\n",
    "    epoch_pbar.set_postfix({\n",
    "        'train_loss': f'{avg_train_loss:.4f}',\n",
    "        'val_loss': f'{avg_val_loss:.4f}',\n",
    "    })\n",
    "\n",
    "    # Save checkpoint\n",
    "    if epoch % 10 == 0:\n",
    "        model_checkpoint(model, avg_val_loss)\n",
    "\n",
    "    # Check early stopping condition\n",
    "    # early_stopping(avg_val_loss)\n",
    "    # if early_stopping.early_stop:\n",
    "    #     print(\"Early stopping triggered. Stopping training.\")\n",
    "    #     break  # Exit the training loop\n",
    "\n",
    "writer.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Summary:\n",
      "Initial Training Loss: 4.1298\n",
      "Final Training Loss: 1.3331\n",
      "Best Training Loss: 1.3331\n",
      "\n",
      "Initial Validation Loss: 3.4965\n",
      "Final Validation Loss: 3.4938\n",
      "Best Validation Loss: 3.4938\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Plot the losses\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(range(1, len(train_losses) + 1), train_losses, label='Training Loss')\n",
    "plt.plot(range(1, len(val_losses) + 1), val_losses, label='Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training and Validation Losses')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.savefig(\"nrms_training_loss.png\")\n",
    "plt.show()\n",
    "\n",
    "# Print final statistics\n",
    "print(\"\\nTraining Summary:\")\n",
    "print(f\"Initial Training Loss: {train_losses[0]:.4f}\")\n",
    "print(f\"Final Training Loss: {train_losses[-1]:.4f}\")\n",
    "print(f\"Best Training Loss: {min(train_losses):.4f}\")\n",
    "print(f\"\\nInitial Validation Loss: {val_losses[0]:.4f}\")\n",
    "print(f\"Final Validation Loss: {val_losses[-1]:.4f}\")\n",
    "print(f\"Best Validation Loss: {min(val_losses):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating DataLoader...\n",
      "Total samples in DataLoader: 25\n",
      "\n",
      "First batch shapes:\n",
      "  - his_input_title: torch.Size([25, 20, 30])\n",
      "  - pred_input_title: torch.Size([25, 33, 30])\n",
      "  - targets: torch.Size([25, 33])\n",
      "\n",
      "Evaluation completed.\n",
      "Total predictions generated: 25\n",
      "First few prediction lengths: [11, 20, 16, 21, 32, 22, 5, 17, 14, 33, 14, 7, 11, 7, 6]\n",
      "\n",
      "Validation against DataFrame:\n",
      "\n",
      "Metrics: {'auc': 0.5434400919220155, 'mrr': 0.31457795827361046, 'ndcg@5': 0.33135084179973306, 'ndcg@10': 0.39376128964325985}\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "def evaluate_model(model, dataloader, device):\n",
    "    \"\"\"Evaluate the model and return predictions and labels for metric calculation.\"\"\"\n",
    "    model.eval()\n",
    "    all_predictions = []\n",
    "    all_labels = []\n",
    "\n",
    "    print(\"\\nEvaluating DataLoader...\")\n",
    "    total_samples = len(dataloader.dataset)\n",
    "    print(f\"Total samples in DataLoader: {total_samples}\")\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (inputs, targets, impression_ids) in enumerate(dataloader):\n",
    "            his_input_title, pred_input_title = inputs\n",
    "\n",
    "            if batch_idx == 0:  # Debug first batch shapes\n",
    "                print(\"\\nFirst batch shapes:\")\n",
    "                print(f\"  - his_input_title: {his_input_title.shape}\")\n",
    "                print(f\"  - pred_input_title: {pred_input_title.shape}\")\n",
    "                print(f\"  - targets: {targets.shape}\")\n",
    "\n",
    "            # Move data to device\n",
    "            his_input_title = his_input_title.to(device)\n",
    "            pred_input_title = pred_input_title.to(device)\n",
    "            targets = targets.to(device)\n",
    "\n",
    "            # Get predictions\n",
    "            predictions = model.predict(his_input_title, pred_input_title)\n",
    "            predictions = predictions.cpu().numpy()\n",
    "            targets = targets.cpu().numpy()\n",
    "\n",
    "            # Process each sample in the batch\n",
    "            batch_size = predictions.shape[0]\n",
    "            for sample_idx in range(batch_size):\n",
    "                pred = predictions[sample_idx]\n",
    "                label = targets[sample_idx]\n",
    "\n",
    "                # Create valid_mask where label is not equal to the padding value (-1)\n",
    "                valid_mask = (label != -1)\n",
    "                sample_preds = pred[valid_mask]\n",
    "                sample_labels = label[valid_mask]\n",
    "\n",
    "                if len(sample_labels) == 0:\n",
    "                    continue  # Skip empty samples\n",
    "\n",
    "                # Ensure that there is at least one positive and one negative label\n",
    "                if len(np.unique(sample_labels)) < 2:\n",
    "                    continue  # Skip samples with only one class\n",
    "\n",
    "                all_predictions.append(sample_preds.tolist())\n",
    "                all_labels.append(sample_labels.tolist())\n",
    "\n",
    "    print(\"\\nEvaluation completed.\")\n",
    "    print(f\"Total predictions generated: {len(all_predictions)}\")\n",
    "    print(f\"First few prediction lengths: {[len(x) for x in all_predictions[:15]]}\")\n",
    "    return all_labels, all_predictions\n",
    "\n",
    "\n",
    "# Evaluate the model\n",
    "labels_list, scores_list = evaluate_model(model, val_dataloader_temp, device)\n",
    "\n",
    "# Validate predictions against the DataFrame\n",
    "print(\"\\nValidation against DataFrame:\")\n",
    "if len(scores_list) != len(df_validation):\n",
    "    print(\"WARNING: Length mismatch!\")\n",
    "    print(f\"  - Number of predictions: {len(scores_list)}\")\n",
    "    print(f\"  - Number of rows in DataFrame: {len(df_validation)}\")\n",
    "\n",
    "# Compute metrics\n",
    "metrics = MetricEvaluator(\n",
    "    labels=labels_list,\n",
    "    predictions=scores_list,\n",
    "    metric_functions=[\n",
    "        AucScore(),\n",
    "        MrrScore(),\n",
    "        NdcgScore(k=5),\n",
    "        NdcgScore(k=10)\n",
    "    ],\n",
    ")\n",
    "results = metrics.evaluate()\n",
    "print(\"\\nMetrics:\", results.evaluations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FRACTION: 0.001, HISTORY_SIZE: 20\n",
      "Hyperparameters:\n",
      "title_size: 30\n",
      "embedding_dim: 32\n",
      "word_emb_dim: 8\n",
      "vocab_size: 10000\n",
      "head_num: 32\n",
      "head_dim: 128\n",
      "attention_hidden_dim: 200\n",
      "hidden_dim: 4\n",
      "optimizer: adam\n",
      "loss: cross_entropy_loss\n",
      "dropout: 0.2\n",
      "learning_rate: 0.0001\n",
      "weight_decay: 0.001\n",
      "news_output_dim: 128\n",
      "units_per_layer: [128, 128]\n"
     ]
    }
   ],
   "source": [
    "print(f\"FRACTION: {FRACTION}, HISTORY_SIZE: {HISTORY_SIZE}\")\n",
    "\n",
    "# Filter out special Python attributes and print parameters\n",
    "params = {k: v for k, v in hparams_nrms.__dict__.items() if not k.startswith('__')}\n",
    "print(\"Hyperparameters:\")\n",
    "for key, value in params.items():\n",
    "    print(f\"{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_length_of_labels(df: pl.DataFrame, impression_id: int) -> int:\n",
    "    # Filter for matching impression_id\n",
    "    filtered = df.filter(pl.col('impression_id') == impression_id)\n",
    "    \n",
    "    if filtered.height == 0:\n",
    "        raise ValueError(f\"No row found for impression_id {impression_id}\")\n",
    "    \n",
    "    # Get labels from first row\n",
    "    labels = filtered.select('article_ids_inview').row(0)[0]\n",
    "    \n",
    "    return len(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get_length_of_labels(df_validation, 349992000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label 0: [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "Label 1: [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "Label 2: [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]\n",
      "Label 3: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "Label 4: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "# Get first 5 labels\n",
    "first_5_labels = df_validation.select('labels').head(5)\n",
    "\n",
    "# Print each label with index\n",
    "for i, row in enumerate(first_5_labels.iter_rows()):\n",
    "    print(f\"Label {i}: {row[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\DTU - Master\\1 semester\\Deep learning (02456)\\Deeplearning-RecSys-Challenge-2024\\models_pytorch\\nrms.py:181: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast():\n",
      "c:\\DTU - Master\\1 semester\\Deep learning (02456)\\Deeplearning-RecSys-Challenge-2024\\.venv\\Lib\\site-packages\\torch\\amp\\autocast_mode.py:266: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipped 0 samples with only one class\n",
      "Remaining valid samples: 25\n",
      "[[0.5077387094497681, 0.5077624320983887, 0.507216215133667, 0.5074234008789062, 0.5073673129081726, 0.5074703097343445, 0.5070481300354004, 0.5081822872161865, 0.5072020888328552, 0.5075074434280396, 0.5074614882469177], [0.5075490474700928, 0.5072618722915649, 0.5077022314071655, 0.5076622366905212, 0.5074983835220337, 0.5064917802810669, 0.5074813365936279, 0.507813036441803, 0.5075894594192505, 0.5077290534973145, 0.5074641108512878, 0.5079082250595093, 0.5076126456260681, 0.5076327323913574, 0.5074700713157654, 0.5075199007987976, 0.5076563358306885, 0.5075355768203735, 0.5075979232788086, 0.5072925686836243], [0.5075262188911438, 0.5074038505554199, 0.5074012279510498, 0.5073606371879578, 0.5075982213020325, 0.5074918270111084, 0.507299542427063, 0.5077663064002991, 0.5071713924407959, 0.5075210332870483, 0.5071666240692139, 0.5076150298118591, 0.5077347159385681, 0.50725919008255, 0.5072194337844849, 0.5074200630187988], [0.5075860619544983, 0.507655918598175, 0.5076131224632263, 0.5072910785675049, 0.5075529217720032, 0.5071480870246887, 0.5074687600135803, 0.5074561238288879, 0.5077610611915588, 0.5077407956123352, 0.507483959197998, 0.5071026086807251, 0.5072764754295349, 0.5075562596321106, 0.5075172781944275, 0.5075936317443848, 0.5077235102653503, 0.5074589848518372, 0.5077061057090759, 0.5072422623634338, 0.5072848200798035], [0.5076504945755005, 0.5074898600578308, 0.5072645545005798, 0.507578432559967, 0.5073481798171997, 0.5075207352638245, 0.5079382658004761, 0.5076577067375183, 0.5075055360794067, 0.5068885087966919, 0.5071308612823486, 0.5075082182884216, 0.5071361064910889, 0.5069466829299927, 0.5073822140693665, 0.5074261426925659, 0.507447361946106, 0.5078099370002747, 0.5072149634361267, 0.5073149800300598, 0.5075330138206482, 0.5074156522750854, 0.5074375867843628, 0.506627082824707, 0.5075808763504028, 0.5074443817138672, 0.5071967840194702, 0.5072104930877686, 0.507550835609436, 0.5075475573539734, 0.5076207518577576, 0.5072429180145264], [0.5074572563171387, 0.5073480010032654, 0.5073866844177246, 0.5076147317886353, 0.5078449249267578, 0.5077832937240601, 0.5073583722114563, 0.5074458122253418, 0.5077379941940308, 0.5076435208320618, 0.5076640844345093, 0.5069726705551147, 0.507554829120636, 0.5074859261512756, 0.5073173642158508, 0.5076510906219482, 0.5077487826347351, 0.507527232170105, 0.5072442889213562, 0.5066509246826172, 0.5076082348823547, 0.5075172185897827], [0.5075699687004089, 0.50722736120224, 0.5070780515670776, 0.5072939395904541, 0.5072716474533081], [0.5075138211250305, 0.5075471997261047, 0.5075086951255798, 0.5072280764579773, 0.5075126886367798, 0.5075359344482422, 0.5076586008071899, 0.507887601852417, 0.5078778862953186, 0.5073423385620117, 0.5076423287391663, 0.5077846050262451, 0.5080968737602234, 0.507321834564209, 0.5076673626899719, 0.5072562098503113, 0.5075334906578064], [0.5075639486312866, 0.5073623061180115, 0.507419764995575, 0.5074236989021301, 0.5077177882194519, 0.5075374841690063, 0.5073646306991577, 0.5075026154518127, 0.5070861577987671, 0.5066461563110352, 0.507454514503479, 0.5073959231376648, 0.5073155760765076, 0.507341742515564], [0.5071800351142883, 0.5076994895935059, 0.5075154304504395, 0.5074135661125183, 0.5077728629112244, 0.5065350532531738, 0.5075945258140564, 0.5074527263641357, 0.5075058341026306, 0.5078573226928711, 0.5077930688858032, 0.5067301988601685, 0.5077545642852783, 0.507684051990509, 0.5070502161979675, 0.5073386430740356, 0.5072040557861328, 0.5072578191757202, 0.5073817372322083, 0.5076074004173279, 0.5078648924827576, 0.5072256922721863, 0.5075621604919434, 0.5076476335525513, 0.5074548721313477, 0.5073511004447937, 0.5074597001075745, 0.507249653339386, 0.5072625279426575, 0.5077710747718811, 0.5075168013572693, 0.5077359676361084, 0.5075716376304626], [0.5076857805252075, 0.507584810256958, 0.5074564218521118, 0.5072709321975708, 0.5073503851890564, 0.5074887275695801, 0.5077742338180542, 0.5075505971908569, 0.5075050592422485, 0.5073100924491882, 0.5074838399887085, 0.5071357488632202, 0.5076541304588318, 0.5072838068008423], [0.5074976682662964, 0.5075705647468567, 0.5074695944786072, 0.5073136687278748, 0.5077146291732788, 0.5071372985839844, 0.5075315833091736], [0.5066572427749634, 0.5065003037452698, 0.5065656304359436, 0.5065773129463196, 0.5066552758216858, 0.5067200660705566, 0.5066493153572083, 0.506515383720398, 0.5073204636573792, 0.5067389607429504, 0.5065255165100098], [0.5074123740196228, 0.5075995922088623, 0.5076411366462708, 0.5074691772460938, 0.5078468322753906, 0.5072759389877319, 0.5073934197425842], [0.5071815252304077, 0.5073132514953613, 0.5070490837097168, 0.5071709752082825, 0.5073176026344299, 0.5071598887443542], [0.5078272819519043, 0.5073562860488892, 0.5075634121894836, 0.5074222683906555, 0.5076543092727661, 0.5077111124992371, 0.5079382061958313, 0.5077475905418396, 0.5071954131126404, 0.5074265599250793, 0.5077070593833923], [0.5074717998504639, 0.5073679089546204, 0.5079624056816101, 0.5072888135910034, 0.5074679255485535, 0.5076693296432495, 0.5078578591346741, 0.5075049996376038, 0.5075486302375793, 0.5075982213020325, 0.5076206922531128, 0.5074328780174255, 0.5078222155570984, 0.5078972578048706, 0.5076677203178406, 0.507099986076355, 0.5068219304084778, 0.5078344345092773], [0.5071489810943604, 0.507332980632782, 0.5075023174285889, 0.5074597597122192, 0.5076597929000854, 0.507611095905304], [0.5074058175086975, 0.5075289607048035, 0.5067101120948792, 0.5078262090682983, 0.5074574947357178, 0.507635235786438, 0.5071723461151123, 0.5077288150787354, 0.5074180960655212, 0.507378101348877, 0.5079813599586487, 0.5074289441108704, 0.507605791091919, 0.5077251195907593, 0.5074864029884338, 0.507358729839325, 0.5072175860404968, 0.5067148208618164, 0.5075786709785461], [0.507108747959137, 0.5077444314956665, 0.5072519183158875, 0.5076084136962891, 0.5074809789657593, 0.5073630213737488, 0.5076606869697571, 0.5075110793113708, 0.5076320171356201, 0.5076208114624023, 0.5072702765464783, 0.5069188475608826, 0.5075769424438477, 0.5078656077384949, 0.5075525045394897, 0.5075448751449585, 0.5072603821754456, 0.5072497725486755, 0.507643461227417, 0.5075144171714783], [0.507824718952179, 0.506889283657074, 0.507707953453064, 0.5072132349014282, 0.5078758001327515, 0.5073931217193604, 0.5078638792037964], [0.507599413394928, 0.5076900124549866, 0.5078376531600952, 0.5074154734611511, 0.5073989033699036, 0.5079002380371094, 0.5072985887527466, 0.5075216889381409], [0.5074487924575806, 0.5077728629112244, 0.5079002380371094, 0.5074443221092224, 0.5077409148216248, 0.5067446827888489, 0.5078063607215881, 0.5077750086784363, 0.5072841644287109, 0.5073561668395996], [0.5075391530990601, 0.5071819424629211, 0.5075746178627014, 0.5078619718551636, 0.5076949596405029], [0.5066641569137573, 0.5075781941413879, 0.50776207447052, 0.5073660612106323, 0.5076613426208496, 0.5069574117660522, 0.5074570178985596, 0.5077788233757019, 0.5077504515647888, 0.5077521204948425, 0.5076579451560974]]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "# Initialize lists\n",
    "all_predictions = []\n",
    "all_labels = []\n",
    "skipped_samples = 0\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for (his_input_title, pred_input_title), targets, impression_ids in val_dataloader_temp:\n",
    "        # Move to device\n",
    "        his_input_title = his_input_title.to(device)\n",
    "        pred_input_title = pred_input_title.to(device)\n",
    "        \n",
    "        # Get predictions\n",
    "        predictions = model.predict(his_input_title, pred_input_title)\n",
    "        \n",
    "        \n",
    "        # Convert to probabilities if needed\n",
    "        if not torch.is_floating_point(predictions):\n",
    "            predictions = torch.sigmoid(predictions)\n",
    "        \n",
    "        # Convert to lists while preserving structure\n",
    "        batch_preds = predictions.cpu().numpy().tolist()\n",
    "        batch_labels = targets.cpu().numpy().tolist()\n",
    "        impression_ids = impression_ids.cpu().numpy().tolist()\n",
    "        \n",
    "        batch_preds_without_padding = []\n",
    "        batch_labels_without_padding = []\n",
    "        \n",
    "        for pred_sample, label_sample, impression_id_sample in zip(batch_preds, batch_labels, impression_ids):\n",
    "            # Remove padding\n",
    "            actual_length = get_length_of_labels(df_validation, impression_id_sample)\n",
    "            pred_sample = pred_sample[:actual_length]\n",
    "            \n",
    "            # Check if sample has both classes before adding\n",
    "            if 1 in label_sample[:actual_length] and 0 in label_sample[:actual_length]:\n",
    "                batch_preds_without_padding.append(pred_sample)\n",
    "                batch_labels_without_padding.append(label_sample[:actual_length])\n",
    "            else:\n",
    "                skipped_samples += 1\n",
    "        \n",
    "        # Add batch predictions and labels\n",
    "        all_predictions.extend(batch_preds_without_padding)\n",
    "        all_labels.extend(batch_labels_without_padding)\n",
    "print(f\"Skipped {skipped_samples} samples with only one class\")\n",
    "print(f\"Remaining valid samples: {len(all_predictions)}\")\n",
    "print(all_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Debug: Print label distribution for first 5 samples\n",
    "# for i, (preds, labels) in enumerate(zip(all_predictions[:5], all_labels[:5])):\n",
    "#     print(f\"\\nSample {i}:\")\n",
    "#     print(f\"Labels length:      {len(labels)}\")\n",
    "#     print(f\"Predictions length: {len(preds)}\")\n",
    "#     print(f\"Num positives: {sum(labels)}\")\n",
    "#     print(f\"Num negatives: {len(labels) - sum(labels)}\")\n",
    "#     print(f\"Label distribution: {labels}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(type(all_predictions))\n",
    "# print(type(all_labels))\n",
    "\n",
    "# print(f\"Number of predictions: {len(all_predictions)}\")\n",
    "# print(f\"example prediction: {all_predictions[0:2]}\")\n",
    "# print(f\"Number of labels: {len(all_labels)}\")\n",
    "# print(f\"example label: {all_labels[0:2]}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.1600\n"
     ]
    }
   ],
   "source": [
    "# Initialize counters\n",
    "correct_predictions = 0\n",
    "total_samples = len(all_predictions)\n",
    "\n",
    "# Iterate over samples\n",
    "for idx, _ in enumerate(all_predictions):\n",
    "    # print(f\"Sample {idx}: Prediction: {all_predictions[idx]}\")\n",
    "    # print(f\"Sample {idx}: Label:      {all_labels[idx]}\")\n",
    "    \n",
    "    # Extract index of maximum value in predictions and labels\n",
    "    pred_max_index = np.argmax(all_predictions[idx])\n",
    "    label_max_index = np.argmax(all_labels[idx])\n",
    "    \n",
    "    # Compare indices to determine if the prediction was correct\n",
    "    if pred_max_index == label_max_index:\n",
    "        # print(f\"Sample {idx}: Prediction was correct.\")\n",
    "        correct_predictions += 1\n",
    "  #  else:\n",
    "        # print(f\"Sample {idx}: Prediction was wrong.\")\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = correct_predictions / total_samples\n",
    "print(f\"Accuracy: {accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Mean AUC: 0.5254\n",
      "Number of valid AUC calculations: 25\n"
     ]
    }
   ],
   "source": [
    "from evaluation import AucScore\n",
    "\n",
    "# auc_score = AucScore()\n",
    "\n",
    "# auc_score.calculate(all_predictions, all_labels)\n",
    "# print(f\"AUC: {auc_score.score}\")\n",
    "# # Calculate AUC per sample\n",
    "aucs = []\n",
    "for preds, labels in zip(all_predictions, all_labels):\n",
    "    try:\n",
    "        # Only calculate if we have both positive and negative samples\n",
    "        if sum(labels) > 0 and sum(labels) < len(labels):\n",
    "            auc = roc_auc_score(labels, preds)\n",
    "            aucs.append(auc)\n",
    "    except ValueError:\n",
    "        raise ValueError(\"Only one class present in labels. Cannot calculate AUC.\")\n",
    "\n",
    "print(f\"\\nMean AUC: {np.mean(aucs):.4f}\")\n",
    "print(f\"Number of valid AUC calculations: {len(aucs)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Output Prediction File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASPLIT = \"\"\n",
    "DUMP_DIR = Path(\"ebnerd_predictions\")\n",
    "PATH =  Path(\"./ebnerd_testset\")\n",
    "COLUMNS = [\n",
    "    DEFAULT_USER_COL,\n",
    "    DEFAULT_HISTORY_ARTICLE_ID_COL,\n",
    "    DEFAULT_INVIEW_ARTICLES_COL,\n",
    "    DEFAULT_IMPRESSION_ID_COL\n",
    "]\n",
    "\n",
    "df_test = (\n",
    "    ebnerd_from_path(\n",
    "        PATH.joinpath(DATASPLIT, \"test\"),\n",
    "        history_size=HISTORY_SIZE\n",
    "    )\n",
    "    .select(COLUMNS)\n",
    "    .sample(fraction=0.001)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting preprocessing...\n",
      "Preprocessing data...\n",
      "(13536, 5)\n",
      "Data preprocessing completed in 5.50 seconds.\n"
     ]
    }
   ],
   "source": [
    "test_dataset = NRMSDataSet(\n",
    "    behaviors=df_test,\n",
    "    article_dict=article_mapping,\n",
    "    unknown_representation=\"zeros\",\n",
    "    history_column=DEFAULT_HISTORY_ARTICLE_ID_COL,\n",
    "    eval_mode=True,\n",
    "    test = True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "\n",
    "def collate_fn_with_global_padding_test(batch, max_len_pred, apply_padding_to_targets : bool = True):\n",
    "    try:\n",
    "        his_input_titles = [item[0][0] for item in batch]  # History inputs\n",
    "        pred_input_titles = [item[0][1] for item in batch]  # Prediction inputs\n",
    "        #batch_ys = [item[1] for item in batch]  # Targets\n",
    "        impression_id = torch.tensor([item[1] for item in batch], dtype=torch.int64)  # Impression ID\n",
    "        \n",
    "\n",
    "        # Pad sequences to the global maximum length\n",
    "        his_input_titles_padded = pad_sequence(his_input_titles, batch_first=True, padding_value=0)\n",
    "\n",
    "        # Pad prediction inputs and adjust to the global maximum length\n",
    "        pred_input_titles_padded = pad_sequence(pred_input_titles, batch_first=True, padding_value=0)\n",
    "        if pred_input_titles_padded.size(1) < max_len_pred:\n",
    "            # Add padding if sequence length is shorter than max_len_pred\n",
    "            pad_size = max_len_pred - pred_input_titles_padded.size(1)\n",
    "            pred_input_titles_padded = torch.nn.functional.pad(\n",
    "                pred_input_titles_padded, (0, 0, 0, pad_size), value=0\n",
    "            )\n",
    "        elif pred_input_titles_padded.size(1) > max_len_pred:\n",
    "            # Trim if sequence length exceeds max_len_pred\n",
    "            pred_input_titles_padded = pred_input_titles_padded[:, :max_len_pred, :]\n",
    "\n",
    "    \n",
    "       \n",
    "        return (his_input_titles_padded, pred_input_titles_padded), impression_id\n",
    "    except Exception as e:\n",
    "        print(f\"Error in collate_fn: {e}\")\n",
    "        raise\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "test_dataloader_temp = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=16,    # Set your desired batch size\n",
    "    shuffle=False,\n",
    "    num_workers=0,\n",
    "    collate_fn=lambda batch: collate_fn_with_global_padding_test(batch, max_inview_length_validation)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "view size is not compatible with input tensor's size and stride (at least one dimension spans across two contiguous subspaces). Use .reshape(...) instead.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[77], line 14\u001b[0m\n\u001b[0;32m     11\u001b[0m pred_input_title \u001b[38;5;241m=\u001b[39m pred_input_title\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# Get predictions\u001b[39;00m\n\u001b[1;32m---> 14\u001b[0m predictions \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhis_input_title\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpred_input_title\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;66;03m# Convert to probabilities if needed\u001b[39;00m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mis_floating_point(predictions):\n",
      "File \u001b[1;32mc:\\DTU - Master\\1 semester\\Deep learning (02456)\\Deeplearning-RecSys-Challenge-2024\\.venv\\Lib\\site-packages\\torch\\utils\\_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    113\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[0;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    115\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[1;32m--> 116\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\DTU - Master\\1 semester\\Deep learning (02456)\\Deeplearning-RecSys-Challenge-2024\\models_pytorch\\nrms.py:199\u001b[0m, in \u001b[0;36mNRMSModel.predict\u001b[1;34m(self, his_input_title, pred_input_title)\u001b[0m\n\u001b[0;32m    196\u001b[0m \u001b[38;5;129m@torch\u001b[39m\u001b[38;5;241m.\u001b[39mno_grad()\n\u001b[0;32m    197\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpredict\u001b[39m(\u001b[38;5;28mself\u001b[39m, his_input_title, pred_input_title):\n\u001b[0;32m    198\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m--> 199\u001b[0m         scores \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhis_input_title\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpred_input_title\u001b[49m\u001b[43m)\u001b[49m        \n\u001b[0;32m    200\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39msigmoid(scores)\n",
      "File \u001b[1;32mc:\\DTU - Master\\1 semester\\Deep learning (02456)\\Deeplearning-RecSys-Challenge-2024\\models_pytorch\\nrms.py:186\u001b[0m, in \u001b[0;36mNRMSModel.forward\u001b[1;34m(self, his_input_title, pred_input_title)\u001b[0m\n\u001b[0;32m    183\u001b[0m pred_input_title \u001b[38;5;241m=\u001b[39m pred_input_title\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m    185\u001b[0m user_present \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muserencoder(his_input_title) \u001b[38;5;66;03m# browsed news\u001b[39;00m\n\u001b[1;32m--> 186\u001b[0m news_present \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_batch_encode_news\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpred_input_title\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# candidate news\u001b[39;00m\n\u001b[0;32m    188\u001b[0m \u001b[38;5;66;03m# Compute scores using batch matrix multiplication\u001b[39;00m\n\u001b[0;32m    189\u001b[0m scores \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mbmm(news_present, user_present\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m))\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[1;32mc:\\DTU - Master\\1 semester\\Deep learning (02456)\\Deeplearning-RecSys-Challenge-2024\\models_pytorch\\nrms.py:176\u001b[0m, in \u001b[0;36mNRMSModel._batch_encode_news\u001b[1;34m(self, titles)\u001b[0m\n\u001b[0;32m    174\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Batch process all titles at once\"\"\"\u001b[39;00m\n\u001b[0;32m    175\u001b[0m batch_size, num_titles, title_size \u001b[38;5;241m=\u001b[39m titles\u001b[38;5;241m.\u001b[39msize()\n\u001b[1;32m--> 176\u001b[0m titles_flat \u001b[38;5;241m=\u001b[39m \u001b[43mtitles\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mview\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtitle_size\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    177\u001b[0m encoded_flat \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnewsencoder(titles_flat)\n\u001b[0;32m    178\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m encoded_flat\u001b[38;5;241m.\u001b[39mview(batch_size, num_titles, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: view size is not compatible with input tensor's size and stride (at least one dimension spans across two contiguous subspaces). Use .reshape(...) instead."
     ]
    }
   ],
   "source": [
    "\n",
    "# Initialize lists\n",
    "all_predictions = []\n",
    "all_labels = []\n",
    "skipped_samples = 0\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for (his_input_title, pred_input_title), impression_ids in test_dataloader_temp:\n",
    "        # Move to device\n",
    "        his_input_title = his_input_title.to(device)\n",
    "        pred_input_title = pred_input_title.to(device)\n",
    "        \n",
    "        # Get predictions\n",
    "        predictions = model.predict(his_input_title, pred_input_title)\n",
    "        \n",
    "        \n",
    "        # Convert to probabilities if needed\n",
    "        if not torch.is_floating_point(predictions):\n",
    "            predictions = torch.sigmoid(predictions)\n",
    "        \n",
    "        # Convert to lists while preserving structure\n",
    "        batch_preds = predictions.cpu().numpy().tolist()\n",
    "        impression_ids = impression_ids.cpu().numpy().tolist()\n",
    "        \n",
    "        batch_preds_without_padding = []\n",
    "        batch_labels_without_padding = []\n",
    "        \n",
    "        for pred_sample, impression_id_sample in zip(batch_preds, impression_ids):\n",
    "            # Remove padding\n",
    "            actual_length = get_length_of_labels(df_test, impression_id_sample)\n",
    "            pred_sample = pred_sample[:actual_length]\n",
    "    \n",
    "            # Check if sample has both classes before adding\n",
    "            batch_preds_without_padding.append(pred_sample)\n",
    "        # Add batch predictions and labels\n",
    "        all_predictions.extend(batch_preds_without_padding)\n",
    "print(f\"Skipped {skipped_samples} samples with only one class\")\n",
    "print(f\"Remaining valid samples: {len(all_predictions)}\")\n",
    "print(all_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "#result = [[float_num] for inner_list in all_predictions for float_num in inner_list]\n",
    "\n",
    "#print(df_test.head())\n",
    "df_test = df_test.with_columns(\n",
    "    pl.Series(\"scores\", all_predictions)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rasm8\\AppData\\Local\\Temp\\ipykernel_20372\\721020885.py:1: MapWithoutReturnDtypeWarning: Calling `map_elements` without specifying `return_dtype` can lead to unpredictable results. Specify `return_dtype` to silence this warning.\n",
      "  df_test = df_test.with_columns(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (2, 8)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>user_id</th><th>article_id_fixed</th><th>article_ids_inview</th><th>article_ids_clicked</th><th>impression_id</th><th>labels</th><th>scores</th><th>ranked_scores</th></tr><tr><td>u32</td><td>list[i32]</td><td>list[i32]</td><td>list[i32]</td><td>u32</td><td>list[i8]</td><td>list[f64]</td><td>list[i64]</td></tr></thead><tbody><tr><td>918965</td><td>[9749628, 9774527, … 9777026]</td><td>[9782205, 9782180, … 9142564]</td><td>[9782256]</td><td>423783217</td><td>[0, 0, … 0]</td><td>[0.507333, 0.506769, … 0.507146]</td><td>[2, 7, … 5]</td></tr><tr><td>847729</td><td>[9772038, 9772168, … 9778627]</td><td>[9789711, 9789423, … 9789743]</td><td>[9789623]</td><td>273622419</td><td>[0, 0, … 0]</td><td>[0.506611, 0.507256, … 0.507002]</td><td>[14, 1, … 6]</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (2, 8)\n",
       "┌─────────┬────────────┬────────────┬────────────┬────────────┬────────────┬───────────┬───────────┐\n",
       "│ user_id ┆ article_id ┆ article_id ┆ article_id ┆ impression ┆ labels     ┆ scores    ┆ ranked_sc │\n",
       "│ ---     ┆ _fixed     ┆ s_inview   ┆ s_clicked  ┆ _id        ┆ ---        ┆ ---       ┆ ores      │\n",
       "│ u32     ┆ ---        ┆ ---        ┆ ---        ┆ ---        ┆ list[i8]   ┆ list[f64] ┆ ---       │\n",
       "│         ┆ list[i32]  ┆ list[i32]  ┆ list[i32]  ┆ u32        ┆            ┆           ┆ list[i64] │\n",
       "╞═════════╪════════════╪════════════╪════════════╪════════════╪════════════╪═══════════╪═══════════╡\n",
       "│ 918965  ┆ [9749628,  ┆ [9782205,  ┆ [9782256]  ┆ 423783217  ┆ [0, 0, …   ┆ [0.507333 ┆ [2, 7, …  │\n",
       "│         ┆ 9774527, … ┆ 9782180, … ┆            ┆            ┆ 0]         ┆ ,         ┆ 5]        │\n",
       "│         ┆ 9777026]   ┆ 9142564]   ┆            ┆            ┆            ┆ 0.506769, ┆           │\n",
       "│         ┆            ┆            ┆            ┆            ┆            ┆ …         ┆           │\n",
       "│         ┆            ┆            ┆            ┆            ┆            ┆ 0.50714…  ┆           │\n",
       "│ 847729  ┆ [9772038,  ┆ [9789711,  ┆ [9789623]  ┆ 273622419  ┆ [0, 0, …   ┆ [0.506611 ┆ [14, 1, … │\n",
       "│         ┆ 9772168, … ┆ 9789423, … ┆            ┆            ┆ 0]         ┆ ,         ┆ 6]        │\n",
       "│         ┆ 9778627]   ┆ 9789743]   ┆            ┆            ┆            ┆ 0.507256, ┆           │\n",
       "│         ┆            ┆            ┆            ┆            ┆            ┆ …         ┆           │\n",
       "│         ┆            ┆            ┆            ┆            ┆            ┆ 0.50700…  ┆           │\n",
       "└─────────┴────────────┴────────────┴────────────┴────────────┴────────────┴───────────┴───────────┘"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test = df_test.with_columns(\n",
    "    pl.col(\"scores\")\n",
    "    .map_elements(lambda x: list(rank_predictions_by_score(x)))\n",
    "    .alias(\"ranked_scores\")\n",
    ")\n",
    "df_test.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25it [00:00, 25315.69it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Zipping ebnerd_predictions\\predictions.txt to ebnerd_predictions\\_predictions-NRMS.zip\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "write_submission_file(\n",
    "    impression_ids=df_test[DEFAULT_IMPRESSION_ID_COL],\n",
    "    prediction_scores=df_test[\"ranked_scores\"],\n",
    "    path=DUMP_DIR.joinpath(\"predictions.txt\"),\n",
    "    filename_zip=f\"{DATASPLIT}_predictions-{MODEL_NAME}.zip\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "https://github.com/recommenders-team/recommenders/blob/main/examples/00_quick_start/nrms_MIND.ipynb"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
