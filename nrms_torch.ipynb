{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Global settings and imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/gustavsiphone/Documents/GitHub/Deeplearning-RecSys-Challenge-2024/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python version: 3.12.5\n",
      "PyTorch version: 2.5.1\n",
      "CUDA available: False\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import sys\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import polars as pl\n",
    "import matplotlib.pyplot as plt\n",
    "#import optuna\n",
    "from tqdm import tqdm\n",
    "import torch.optim as optim\n",
    "\n",
    "from utils._constants import (\n",
    "    DEFAULT_HISTORY_ARTICLE_ID_COL,\n",
    "    DEFAULT_CLICKED_ARTICLES_COL,\n",
    "    DEFAULT_INVIEW_ARTICLES_COL,\n",
    "    DEFAULT_IMPRESSION_ID_COL,\n",
    "    DEFAULT_SUBTITLE_COL,\n",
    "    DEFAULT_LABELS_COL,\n",
    "    DEFAULT_TITLE_COL,\n",
    "    DEFAULT_USER_COL,\n",
    "    DEFAULT_ARTICLE_PUBLISHED_TIMESTAMP_COL\n",
    ")\n",
    "\n",
    "from utils._behaviors import (\n",
    "    create_binary_labels_column,\n",
    "    sampling_strategy_wu2019,\n",
    "    add_known_user_column,\n",
    "    add_prediction_scores,\n",
    "    truncate_history,\n",
    ")\n",
    "from evaluation import MetricEvaluator, AucScore, NdcgScore, MrrScore\n",
    "from utils._articles import convert_text2encoding_with_transformers\n",
    "from utils._polars import concat_str_columns, slice_join_dataframes\n",
    "from utils._articles import create_article_id_to_value_mapping\n",
    "from utils._nlp import get_transformers_word_embeddings, generate_embeddings_with_transformers\n",
    "from utils._python import write_submission_file, rank_predictions_by_score\n",
    "from models_pytorch.model_config import hparams_nrms\n",
    "\n",
    "from models_pytorch.nrms import NRMSModel\n",
    "from models_pytorch.NRMSDocVecModel import NRMSDocVecModel\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from models_pytorch.dataloader import NRMSDataSet\n",
    "\n",
    "# Check Python version\n",
    "print(f\"Python version: {sys.version.split()[0]}\")\n",
    "\n",
    "# Check PyTorch version\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "\n",
    "# Check GPU availability\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"Current GPU device: {torch.cuda.get_device_name()}\")\n",
    "    print(f\"Number of GPUs available: {torch.cuda.device_count()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 42\n",
    "batch_size = 64\n",
    "\n",
    "# Options: demo, small, large\n",
    "MIND_type = 'demo'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Looking at behaviours and history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download and load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ebnerd_small\n"
     ]
    }
   ],
   "source": [
    "PATH = Path(\"./ebnerd_small\")  # Base path for your data directory\n",
    "print(PATH)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (2, 6)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>user_id</th><th>article_id_fixed</th><th>article_ids_inview</th><th>article_ids_clicked</th><th>impression_id</th><th>labels</th></tr><tr><td>u32</td><td>list[i32]</td><td>list[i64]</td><td>list[i64]</td><td>u32</td><td>list[i8]</td></tr></thead><tbody><tr><td>1192302</td><td>[0, 0, … 9764781]</td><td>[9494434, 9755800, … 9527358]</td><td>[9773275]</td><td>85979274</td><td>[0, 0, … 0]</td></tr><tr><td>1792806</td><td>[9766468, 9763307, … 9770638]</td><td>[9778422, 9778413, … 9775703]</td><td>[9778413]</td><td>174370599</td><td>[0, 1, … 0]</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (2, 6)\n",
       "┌─────────┬───────────────────┬───────────────────┬──────────────────┬───────────────┬─────────────┐\n",
       "│ user_id ┆ article_id_fixed  ┆ article_ids_invie ┆ article_ids_clic ┆ impression_id ┆ labels      │\n",
       "│ ---     ┆ ---               ┆ w                 ┆ ked              ┆ ---           ┆ ---         │\n",
       "│ u32     ┆ list[i32]         ┆ ---               ┆ ---              ┆ u32           ┆ list[i8]    │\n",
       "│         ┆                   ┆ list[i64]         ┆ list[i64]        ┆               ┆             │\n",
       "╞═════════╪═══════════════════╪═══════════════════╪══════════════════╪═══════════════╪═════════════╡\n",
       "│ 1192302 ┆ [0, 0, … 9764781] ┆ [9494434,         ┆ [9773275]        ┆ 85979274      ┆ [0, 0, … 0] │\n",
       "│         ┆                   ┆ 9755800, …        ┆                  ┆               ┆             │\n",
       "│         ┆                   ┆ 9527358]          ┆                  ┆               ┆             │\n",
       "│ 1792806 ┆ [9766468,         ┆ [9778422,         ┆ [9778413]        ┆ 174370599     ┆ [0, 1, … 0] │\n",
       "│         ┆ 9763307, …        ┆ 9778413, …        ┆                  ┆               ┆             │\n",
       "│         ┆ 9770638]          ┆ 9775703]          ┆                  ┆               ┆             │\n",
       "└─────────┴───────────────────┴───────────────────┴──────────────────┴───────────────┴─────────────┘"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def ebnerd_from_path(path: Path, history_size: int = 30) -> pl.DataFrame:\n",
    "    \"\"\"\n",
    "    Load ebnerd - function\n",
    "    \"\"\"\n",
    "    df_history = (\n",
    "        pl.scan_parquet(path.joinpath(\"history.parquet\"))\n",
    "        .select(DEFAULT_USER_COL, DEFAULT_HISTORY_ARTICLE_ID_COL)\n",
    "        .pipe(\n",
    "            truncate_history,\n",
    "            column=DEFAULT_HISTORY_ARTICLE_ID_COL,\n",
    "            history_size=history_size,\n",
    "            padding_value=0,\n",
    "            enable_warning=False,\n",
    "        )\n",
    "    )\n",
    "    df_behaviors = (\n",
    "        pl.scan_parquet(path.joinpath(\"behaviors.parquet\"))\n",
    "        .collect()\n",
    "        .pipe(\n",
    "            slice_join_dataframes,\n",
    "            df2=df_history.collect(),\n",
    "            on=DEFAULT_USER_COL,\n",
    "            how=\"left\",\n",
    "        )\n",
    "    )\n",
    "    return df_behaviors\n",
    "\n",
    "COLUMNS = [\n",
    "    DEFAULT_USER_COL,\n",
    "    DEFAULT_HISTORY_ARTICLE_ID_COL,\n",
    "    DEFAULT_INVIEW_ARTICLES_COL,\n",
    "    DEFAULT_CLICKED_ARTICLES_COL,\n",
    "    DEFAULT_IMPRESSION_ID_COL,\n",
    "]\n",
    "HISTORY_SIZE = 40 # TODO: History size. \n",
    "FRACTION = 0.2\n",
    "\n",
    "df_train = (\n",
    "    ebnerd_from_path(PATH.joinpath(\"train\"), history_size=HISTORY_SIZE)\n",
    "    .select(COLUMNS)\n",
    "    .pipe(\n",
    "        sampling_strategy_wu2019,\n",
    "        npratio=4,\n",
    "        shuffle=True,\n",
    "        with_replacement=True,\n",
    "        seed=123,\n",
    "    )\n",
    "    .pipe(create_binary_labels_column)\n",
    "    .sample(fraction=FRACTION)\n",
    ")\n",
    "# =>\n",
    "df_validation = (\n",
    "    ebnerd_from_path(PATH.joinpath(\"validation\"), history_size=HISTORY_SIZE)\n",
    "    .select(COLUMNS)\n",
    "    .pipe(create_binary_labels_column)\n",
    "    .sample(fraction=FRACTION)\n",
    ")\n",
    "df_train.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "vscode": {
     "languageId": "ruby"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average length of article_ids_inview in df_train: 5.0\n",
      "Average length of article_ids_inview in df_validation: 12.014653886243332\n"
     ]
    }
   ],
   "source": [
    "def calculate_average_length(df, column):\n",
    "    total_length = sum(len(row) for row in df[column])\n",
    "    average_length = total_length / len(df)\n",
    "    return average_length\n",
    "\n",
    "# Calculate average length for df_train\n",
    "average_length_inview_train = calculate_average_length(df_train, DEFAULT_INVIEW_ARTICLES_COL)\n",
    "print(f\"Average length of article_ids_inview in df_train: {average_length_inview_train}\")\n",
    "\n",
    "# Calculate average length for df_validation\n",
    "average_length_inview_validation = calculate_average_length(df_validation, DEFAULT_INVIEW_ARTICLES_COL)\n",
    "print(f\"Average length of article_ids_inview in df_validation: {average_length_inview_validation}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Longest inview article length in df_train: 5\n",
      "Longest inview article length in df_validation: 91\n",
      "Longest history length in df_train: 40\n",
      "Longest history length in df_validation: 40\n"
     ]
    }
   ],
   "source": [
    "# Function to find the maximum length of arrays in a column\n",
    "def find_max_length(df, column):\n",
    "    max_length = 0\n",
    "    for row in df[column]:\n",
    "        max_length = max(max_length, len(row))\n",
    "    return max_length\n",
    "\n",
    "# Find the longest inview article length in df_train\n",
    "max_inview_length_train = find_max_length(df_train, DEFAULT_INVIEW_ARTICLES_COL)\n",
    "\n",
    "# Find the longest inview article length in df_validation\n",
    "max_inview_length_validation = find_max_length(df_validation, DEFAULT_INVIEW_ARTICLES_COL)\n",
    "\n",
    "print(f\"Longest inview article length in df_train: {max_inview_length_train}\")\n",
    "print(f\"Longest inview article length in df_validation: {max_inview_length_validation}\")\n",
    "\n",
    "max_history_length_train = find_max_length(df_train, DEFAULT_HISTORY_ARTICLE_ID_COL)\n",
    "max_history_length_validation = find_max_length(df_validation, DEFAULT_HISTORY_ARTICLE_ID_COL)\n",
    "\n",
    "print(f\"Longest history length in df_train: {max_history_length_train}\")\n",
    "print(f\"Longest history length in df_validation: {max_history_length_validation}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "vscode": {
     "languageId": "ruby"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows with exactly one clicked article in df_train: 46855\n",
      "Number of rows with exactly one clicked article in df_validation: 48660\n"
     ]
    }
   ],
   "source": [
    "# Function to filter rows with exactly one clicked article\n",
    "def filter_rows_with_one_clicked_article(df, clicked_articles_col):\n",
    "    # Manually filter rows where the array has exactly one element\n",
    "    filtered_rows = []\n",
    "    for row in df.iter_rows(named=True):\n",
    "        if len(row[clicked_articles_col]) == 1:\n",
    "            filtered_rows.append(row)\n",
    "    return pl.DataFrame(filtered_rows)\n",
    "\n",
    "\n",
    "# Filter rows in df_train and df_validation\n",
    "df_train = filter_rows_with_one_clicked_article(df_train, DEFAULT_CLICKED_ARTICLES_COL)\n",
    "df_validation = filter_rows_with_one_clicked_article(df_validation, DEFAULT_CLICKED_ARTICLES_COL)\n",
    "\n",
    "# Print the results\n",
    "print(f\"Number of rows with exactly one clicked article in df_train: {df_train.shape[0]}\")\n",
    "print(f\"Number of rows with exactly one clicked article in df_validation: {df_validation.shape[0]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "vscode": {
     "languageId": "ruby"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (5, 6)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>user_id</th><th>article_id_fixed</th><th>article_ids_inview</th><th>article_ids_clicked</th><th>impression_id</th><th>labels</th></tr><tr><td>i64</td><td>list[i64]</td><td>list[i64]</td><td>list[i64]</td><td>i64</td><td>list[i64]</td></tr></thead><tbody><tr><td>2585704</td><td>[9772088, 9767909, … 9778302]</td><td>[9783993, 9784856, … 9784879]</td><td>[9783993]</td><td>476486371</td><td>[1, 0, … 0]</td></tr><tr><td>1929780</td><td>[9774142, 9773486, … 9777307]</td><td>[9769155, 9784044, … 9784406]</td><td>[9784662]</td><td>486250838</td><td>[0, 0, … 0]</td></tr><tr><td>643200</td><td>[9771995, 9769605, … 9778971]</td><td>[8392487, 9788400, … 9789001]</td><td>[9782845]</td><td>548034706</td><td>[0, 0, … 0]</td></tr><tr><td>2004965</td><td>[9737062, 9764822, … 9776322]</td><td>[9277339, 9782996, … 9759345]</td><td>[9781991]</td><td>429985421</td><td>[0, 0, … 0]</td></tr><tr><td>1429215</td><td>[9777005, 9778168, … 9778902]</td><td>[9779370, 9784642, … 9784444]</td><td>[9778787]</td><td>270600342</td><td>[0, 0, … 0]</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (5, 6)\n",
       "┌─────────┬───────────────────┬───────────────────┬──────────────────┬───────────────┬─────────────┐\n",
       "│ user_id ┆ article_id_fixed  ┆ article_ids_invie ┆ article_ids_clic ┆ impression_id ┆ labels      │\n",
       "│ ---     ┆ ---               ┆ w                 ┆ ked              ┆ ---           ┆ ---         │\n",
       "│ i64     ┆ list[i64]         ┆ ---               ┆ ---              ┆ i64           ┆ list[i64]   │\n",
       "│         ┆                   ┆ list[i64]         ┆ list[i64]        ┆               ┆             │\n",
       "╞═════════╪═══════════════════╪═══════════════════╪══════════════════╪═══════════════╪═════════════╡\n",
       "│ 2585704 ┆ [9772088,         ┆ [9783993,         ┆ [9783993]        ┆ 476486371     ┆ [1, 0, … 0] │\n",
       "│         ┆ 9767909, …        ┆ 9784856, …        ┆                  ┆               ┆             │\n",
       "│         ┆ 9778302]          ┆ 9784879]          ┆                  ┆               ┆             │\n",
       "│ 1929780 ┆ [9774142,         ┆ [9769155,         ┆ [9784662]        ┆ 486250838     ┆ [0, 0, … 0] │\n",
       "│         ┆ 9773486, …        ┆ 9784044, …        ┆                  ┆               ┆             │\n",
       "│         ┆ 9777307]          ┆ 9784406]          ┆                  ┆               ┆             │\n",
       "│ 643200  ┆ [9771995,         ┆ [8392487,         ┆ [9782845]        ┆ 548034706     ┆ [0, 0, … 0] │\n",
       "│         ┆ 9769605, …        ┆ 9788400, …        ┆                  ┆               ┆             │\n",
       "│         ┆ 9778971]          ┆ 9789001]          ┆                  ┆               ┆             │\n",
       "│ 2004965 ┆ [9737062,         ┆ [9277339,         ┆ [9781991]        ┆ 429985421     ┆ [0, 0, … 0] │\n",
       "│         ┆ 9764822, …        ┆ 9782996, …        ┆                  ┆               ┆             │\n",
       "│         ┆ 9776322]          ┆ 9759345]          ┆                  ┆               ┆             │\n",
       "│ 1429215 ┆ [9777005,         ┆ [9779370,         ┆ [9778787]        ┆ 270600342     ┆ [0, 0, … 0] │\n",
       "│         ┆ 9778168, …        ┆ 9784642, …        ┆                  ┆               ┆             │\n",
       "│         ┆ 9778902]          ┆ 9784444]          ┆                  ┆               ┆             │\n",
       "└─────────┴───────────────────┴───────────────────┴──────────────────┴───────────────┴─────────────┘"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_validation.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "vscode": {
     "languageId": "ruby"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of users in df_train: 11273\n"
     ]
    }
   ],
   "source": [
    "print(f\"Number of users in df_train: {df_train['user_id'].n_unique()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (10, 21)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>article_id</th><th>title</th><th>subtitle</th><th>last_modified_time</th><th>premium</th><th>body</th><th>published_time</th><th>image_ids</th><th>article_type</th><th>url</th><th>ner_clusters</th><th>entity_groups</th><th>topics</th><th>category</th><th>subcategory</th><th>category_str</th><th>total_inviews</th><th>total_pageviews</th><th>total_read_time</th><th>sentiment_score</th><th>sentiment_label</th></tr><tr><td>i32</td><td>str</td><td>str</td><td>datetime[μs]</td><td>bool</td><td>str</td><td>datetime[μs]</td><td>list[i64]</td><td>str</td><td>str</td><td>list[str]</td><td>list[str]</td><td>list[str]</td><td>i16</td><td>list[i16]</td><td>str</td><td>i32</td><td>i32</td><td>f32</td><td>f32</td><td>str</td></tr></thead><tbody><tr><td>3001353</td><td>&quot;Natascha var ikke den første&quot;</td><td>&quot;Politiet frygter nu, at Natasc…</td><td>2023-06-29 06:20:33</td><td>false</td><td>&quot;Sagen om den østriske Natascha…</td><td>2006-08-31 08:06:45</td><td>[3150850]</td><td>&quot;article_default&quot;</td><td>&quot;https://ekstrabladet.dk/krimi/…</td><td>[]</td><td>[]</td><td>[&quot;Kriminalitet&quot;, &quot;Personfarlig kriminalitet&quot;]</td><td>140</td><td>[]</td><td>&quot;krimi&quot;</td><td>null</td><td>null</td><td>null</td><td>0.9955</td><td>&quot;Negative&quot;</td></tr><tr><td>3003065</td><td>&quot;Kun Star Wars tjente mere&quot;</td><td>&quot;Biografgængerne strømmer ind f…</td><td>2023-06-29 06:20:35</td><td>false</td><td>&quot;Vatikanet har opfordret til at…</td><td>2006-05-21 16:57:00</td><td>[3006712]</td><td>&quot;article_default&quot;</td><td>&quot;https://ekstrabladet.dk/underh…</td><td>[]</td><td>[]</td><td>[&quot;Underholdning&quot;, &quot;Film og tv&quot;, &quot;Økonomi&quot;]</td><td>414</td><td>[433, 434]</td><td>&quot;underholdning&quot;</td><td>null</td><td>null</td><td>null</td><td>0.846</td><td>&quot;Positive&quot;</td></tr><tr><td>3012771</td><td>&quot;Morten Bruun fyret i Sønderjys…</td><td>&quot;FODBOLD: Morten Bruun fyret me…</td><td>2023-06-29 06:20:39</td><td>false</td><td>&quot;Kemien mellem spillerne i Supe…</td><td>2006-05-01 14:28:40</td><td>[3177953]</td><td>&quot;article_default&quot;</td><td>&quot;https://ekstrabladet.dk/sport/…</td><td>[]</td><td>[]</td><td>[&quot;Erhverv&quot;, &quot;Kendt&quot;, … &quot;Ansættelsesforhold&quot;]</td><td>142</td><td>[196, 199]</td><td>&quot;sport&quot;</td><td>null</td><td>null</td><td>null</td><td>0.8241</td><td>&quot;Negative&quot;</td></tr><tr><td>3023463</td><td>&quot;Luderne flytter på landet&quot;</td><td>&quot;I landets tyndest befolkede om…</td><td>2023-06-29 06:20:43</td><td>false</td><td>&quot;Det frække erhverv rykker på l…</td><td>2007-03-24 08:27:59</td><td>[3184029]</td><td>&quot;article_default&quot;</td><td>&quot;https://ekstrabladet.dk/nyhede…</td><td>[]</td><td>[]</td><td>[&quot;Livsstil&quot;, &quot;Erotik&quot;]</td><td>118</td><td>[133]</td><td>&quot;nyheder&quot;</td><td>null</td><td>null</td><td>null</td><td>0.7053</td><td>&quot;Neutral&quot;</td></tr><tr><td>3032577</td><td>&quot;Cybersex: Hvornår er man utro?&quot;</td><td>&quot;En flirtende sms til den flott…</td><td>2023-06-29 06:20:46</td><td>false</td><td>&quot;De fleste af os mener, at et t…</td><td>2007-01-18 10:30:37</td><td>[3030463]</td><td>&quot;article_default&quot;</td><td>&quot;https://ekstrabladet.dk/sex_og…</td><td>[]</td><td>[]</td><td>[&quot;Livsstil&quot;, &quot;Partnerskab&quot;]</td><td>565</td><td>[]</td><td>&quot;sex_og_samliv&quot;</td><td>null</td><td>null</td><td>null</td><td>0.9307</td><td>&quot;Neutral&quot;</td></tr><tr><td>3033563</td><td>&quot;Kniven for struben-vært får se…</td><td>&quot;I aftenens udgave af &#x27;Med kniv…</td><td>2023-06-29 06:20:47</td><td>false</td><td>&quot;Når man ser fjerde program i T…</td><td>2007-03-27 10:22:08</td><td>[3005524, 3005525]</td><td>&quot;article_default&quot;</td><td>&quot;https://ekstrabladet.dk/underh…</td><td>[]</td><td>[]</td><td>[&quot;Livsstil&quot;, &quot;Underholdning&quot;, … &quot;Mad og drikke&quot;]</td><td>414</td><td>[433, 436]</td><td>&quot;underholdning&quot;</td><td>null</td><td>null</td><td>null</td><td>0.9371</td><td>&quot;Neutral&quot;</td></tr><tr><td>3034608</td><td>&quot;Willy Strube har begået selvmo…</td><td>&quot;Den tidligere SiD-chef tog sit…</td><td>2023-06-29 06:20:49</td><td>false</td><td>&quot;Den tidligere formand for Indu…</td><td>2001-10-19 12:30:00</td><td>[3204848]</td><td>&quot;article_default&quot;</td><td>&quot;https://ekstrabladet.dk/nyhede…</td><td>[&quot;Willy Strube&quot;, &quot;Willy Strube&quot;, &quot;Willy Strube&quot;]</td><td>[&quot;PER&quot;, &quot;PER&quot;, &quot;PER&quot;]</td><td>[&quot;Kriminalitet&quot;, &quot;Erhverv&quot;, … &quot;Offentlig instans&quot;]</td><td>118</td><td>[130]</td><td>&quot;nyheder&quot;</td><td>null</td><td>null</td><td>null</td><td>0.9971</td><td>&quot;Negative&quot;</td></tr><tr><td>3034849</td><td>&quot;Venner for livet&quot;</td><td>&quot;VK-REGERINGEN&quot;</td><td>2023-06-29 06:20:50</td><td>false</td><td>&quot;VK-REGERINGEN\n",
       "håndplukkede Bjø…</td><td>2003-01-09 06:00:00</td><td>null</td><td>&quot;article_default&quot;</td><td>&quot;https://ekstrabladet.dk/incomi…</td><td>[]</td><td>[]</td><td>[&quot;Kendt&quot;, &quot;Politik&quot;, &quot;National politik&quot;]</td><td>2</td><td>[]</td><td>&quot;incoming&quot;</td><td>null</td><td>null</td><td>null</td><td>0.8454</td><td>&quot;Neutral&quot;</td></tr><tr><td>3035648</td><td>&quot;Dronning af escort-branchen&quot;</td><td>&quot;Trine Michelsen hjælper københ…</td><td>2023-06-29 06:20:52</td><td>false</td><td>&quot;En af escortbranchens største …</td><td>2003-06-17 07:10:00</td><td>[3082573]</td><td>&quot;article_default&quot;</td><td>&quot;https://ekstrabladet.dk/krimi/…</td><td>[]</td><td>[]</td><td>[&quot;Erhverv&quot;, &quot;Livsstil&quot;, … &quot;Erotik&quot;]</td><td>140</td><td>[]</td><td>&quot;krimi&quot;</td><td>null</td><td>null</td><td>null</td><td>0.8814</td><td>&quot;Neutral&quot;</td></tr><tr><td>3036444</td><td>&quot;Mia kendte sandsynligvis sin m…</td><td>&quot;Hun var ikke den type, der søg…</td><td>2023-06-29 06:20:54</td><td>false</td><td>&quot;Den 12-årige Mia Teglgaard Spr…</td><td>2003-07-13 19:50:00</td><td>null</td><td>&quot;article_default&quot;</td><td>&quot;https://ekstrabladet.dk/krimi/…</td><td>[&quot;Mia Teglgaard Sprotte&quot;, &quot;Erik Andersen&quot;, … &quot;Mia Teglgaard Sprotte&quot;]</td><td>[&quot;PER&quot;, &quot;PER&quot;, … &quot;PER&quot;]</td><td>[&quot;Kriminalitet&quot;, &quot;Personfarlig kriminalitet&quot;]</td><td>140</td><td>[]</td><td>&quot;krimi&quot;</td><td>null</td><td>null</td><td>null</td><td>0.9752</td><td>&quot;Negative&quot;</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (10, 21)\n",
       "┌───────────┬───────────┬───────────┬───────────┬───┬───────────┬───────────┬───────────┬──────────┐\n",
       "│ article_i ┆ title     ┆ subtitle  ┆ last_modi ┆ … ┆ total_pag ┆ total_rea ┆ sentiment ┆ sentimen │\n",
       "│ d         ┆ ---       ┆ ---       ┆ fied_time ┆   ┆ eviews    ┆ d_time    ┆ _score    ┆ t_label  │\n",
       "│ ---       ┆ str       ┆ str       ┆ ---       ┆   ┆ ---       ┆ ---       ┆ ---       ┆ ---      │\n",
       "│ i32       ┆           ┆           ┆ datetime[ ┆   ┆ i32       ┆ f32       ┆ f32       ┆ str      │\n",
       "│           ┆           ┆           ┆ μs]       ┆   ┆           ┆           ┆           ┆          │\n",
       "╞═══════════╪═══════════╪═══════════╪═══════════╪═══╪═══════════╪═══════════╪═══════════╪══════════╡\n",
       "│ 3001353   ┆ Natascha  ┆ Politiet  ┆ 2023-06-2 ┆ … ┆ null      ┆ null      ┆ 0.9955    ┆ Negative │\n",
       "│           ┆ var ikke  ┆ frygter   ┆ 9         ┆   ┆           ┆           ┆           ┆          │\n",
       "│           ┆ den       ┆ nu, at    ┆ 06:20:33  ┆   ┆           ┆           ┆           ┆          │\n",
       "│           ┆ første    ┆ Natasc…   ┆           ┆   ┆           ┆           ┆           ┆          │\n",
       "│ 3003065   ┆ Kun Star  ┆ Biografgæ ┆ 2023-06-2 ┆ … ┆ null      ┆ null      ┆ 0.846     ┆ Positive │\n",
       "│           ┆ Wars      ┆ ngerne    ┆ 9         ┆   ┆           ┆           ┆           ┆          │\n",
       "│           ┆ tjente    ┆ strømmer  ┆ 06:20:35  ┆   ┆           ┆           ┆           ┆          │\n",
       "│           ┆ mere      ┆ ind f…    ┆           ┆   ┆           ┆           ┆           ┆          │\n",
       "│ 3012771   ┆ Morten    ┆ FODBOLD:  ┆ 2023-06-2 ┆ … ┆ null      ┆ null      ┆ 0.8241    ┆ Negative │\n",
       "│           ┆ Bruun     ┆ Morten    ┆ 9         ┆   ┆           ┆           ┆           ┆          │\n",
       "│           ┆ fyret i   ┆ Bruun     ┆ 06:20:39  ┆   ┆           ┆           ┆           ┆          │\n",
       "│           ┆ Sønderjys ┆ fyret me… ┆           ┆   ┆           ┆           ┆           ┆          │\n",
       "│           ┆ …         ┆           ┆           ┆   ┆           ┆           ┆           ┆          │\n",
       "│ 3023463   ┆ Luderne   ┆ I landets ┆ 2023-06-2 ┆ … ┆ null      ┆ null      ┆ 0.7053    ┆ Neutral  │\n",
       "│           ┆ flytter   ┆ tyndest   ┆ 9         ┆   ┆           ┆           ┆           ┆          │\n",
       "│           ┆ på landet ┆ befolkede ┆ 06:20:43  ┆   ┆           ┆           ┆           ┆          │\n",
       "│           ┆           ┆ om…       ┆           ┆   ┆           ┆           ┆           ┆          │\n",
       "│ 3032577   ┆ Cybersex: ┆ En        ┆ 2023-06-2 ┆ … ┆ null      ┆ null      ┆ 0.9307    ┆ Neutral  │\n",
       "│           ┆ Hvornår   ┆ flirtende ┆ 9         ┆   ┆           ┆           ┆           ┆          │\n",
       "│           ┆ er man    ┆ sms til   ┆ 06:20:46  ┆   ┆           ┆           ┆           ┆          │\n",
       "│           ┆ utro?     ┆ den       ┆           ┆   ┆           ┆           ┆           ┆          │\n",
       "│           ┆           ┆ flott…    ┆           ┆   ┆           ┆           ┆           ┆          │\n",
       "│ 3033563   ┆ Kniven    ┆ I         ┆ 2023-06-2 ┆ … ┆ null      ┆ null      ┆ 0.9371    ┆ Neutral  │\n",
       "│           ┆ for strub ┆ aftenens  ┆ 9         ┆   ┆           ┆           ┆           ┆          │\n",
       "│           ┆ en-vært   ┆ udgave af ┆ 06:20:47  ┆   ┆           ┆           ┆           ┆          │\n",
       "│           ┆ får se…   ┆ 'Med      ┆           ┆   ┆           ┆           ┆           ┆          │\n",
       "│           ┆           ┆ kniv…     ┆           ┆   ┆           ┆           ┆           ┆          │\n",
       "│ 3034608   ┆ Willy     ┆ Den       ┆ 2023-06-2 ┆ … ┆ null      ┆ null      ┆ 0.9971    ┆ Negative │\n",
       "│           ┆ Strube    ┆ tidligere ┆ 9         ┆   ┆           ┆           ┆           ┆          │\n",
       "│           ┆ har       ┆ SiD-chef  ┆ 06:20:49  ┆   ┆           ┆           ┆           ┆          │\n",
       "│           ┆ begået    ┆ tog sit…  ┆           ┆   ┆           ┆           ┆           ┆          │\n",
       "│           ┆ selvmo…   ┆           ┆           ┆   ┆           ┆           ┆           ┆          │\n",
       "│ 3034849   ┆ Venner    ┆ VK-REGERI ┆ 2023-06-2 ┆ … ┆ null      ┆ null      ┆ 0.8454    ┆ Neutral  │\n",
       "│           ┆ for livet ┆ NGEN      ┆ 9         ┆   ┆           ┆           ┆           ┆          │\n",
       "│           ┆           ┆           ┆ 06:20:50  ┆   ┆           ┆           ┆           ┆          │\n",
       "│ 3035648   ┆ Dronning  ┆ Trine     ┆ 2023-06-2 ┆ … ┆ null      ┆ null      ┆ 0.8814    ┆ Neutral  │\n",
       "│           ┆ af escort ┆ Michelsen ┆ 9         ┆   ┆           ┆           ┆           ┆          │\n",
       "│           ┆ -branchen ┆ hjælper   ┆ 06:20:52  ┆   ┆           ┆           ┆           ┆          │\n",
       "│           ┆           ┆ københ…   ┆           ┆   ┆           ┆           ┆           ┆          │\n",
       "│ 3036444   ┆ Mia       ┆ Hun var   ┆ 2023-06-2 ┆ … ┆ null      ┆ null      ┆ 0.9752    ┆ Negative │\n",
       "│           ┆ kendte    ┆ ikke den  ┆ 9         ┆   ┆           ┆           ┆           ┆          │\n",
       "│           ┆ sandsynli ┆ type, der ┆ 06:20:54  ┆   ┆           ┆           ┆           ┆          │\n",
       "│           ┆ gvis sin  ┆ søg…      ┆           ┆   ┆           ┆           ┆           ┆          │\n",
       "│           ┆ m…        ┆           ┆           ┆   ┆           ┆           ┆           ┆          │\n",
       "└───────────┴───────────┴───────────┴───────────┴───┴───────────┴───────────┴───────────┴──────────┘"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_articles = pl.read_parquet(PATH.joinpath(\"articles.parquet\"))\n",
    "df_articles.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n",
      "DataFrame after tokenization:\n",
      "shape: (20_738, 21)\n",
      "┌───────────┬───────────┬───────────┬───────────┬───┬───────────┬───────────┬───────────┬──────────┐\n",
      "│ article_i ┆ title     ┆ subtitle  ┆ last_modi ┆ … ┆ total_pag ┆ total_rea ┆ sentiment ┆ sentimen │\n",
      "│ d         ┆ ---       ┆ ---       ┆ fied_time ┆   ┆ eviews    ┆ d_time    ┆ _score    ┆ t_label  │\n",
      "│ ---       ┆ str       ┆ str       ┆ ---       ┆   ┆ ---       ┆ ---       ┆ ---       ┆ ---      │\n",
      "│ i32       ┆           ┆           ┆ datetime[ ┆   ┆ i32       ┆ f32       ┆ f32       ┆ str      │\n",
      "│           ┆           ┆           ┆ μs]       ┆   ┆           ┆           ┆           ┆          │\n",
      "╞═══════════╪═══════════╪═══════════╪═══════════╪═══╪═══════════╪═══════════╪═══════════╪══════════╡\n",
      "│ 3001353   ┆ Natascha  ┆ Politiet  ┆ 2023-06-2 ┆ … ┆ null      ┆ null      ┆ 0.9955    ┆ Negative │\n",
      "│           ┆ var ikke  ┆ frygter   ┆ 9         ┆   ┆           ┆           ┆           ┆          │\n",
      "│           ┆ den       ┆ nu, at    ┆ 06:20:33  ┆   ┆           ┆           ┆           ┆          │\n",
      "│           ┆ første    ┆ Natasc…   ┆           ┆   ┆           ┆           ┆           ┆          │\n",
      "│ 3003065   ┆ Kun Star  ┆ Biografgæ ┆ 2023-06-2 ┆ … ┆ null      ┆ null      ┆ 0.846     ┆ Positive │\n",
      "│           ┆ Wars      ┆ ngerne    ┆ 9         ┆   ┆           ┆           ┆           ┆          │\n",
      "│           ┆ tjente    ┆ strømmer  ┆ 06:20:35  ┆   ┆           ┆           ┆           ┆          │\n",
      "│           ┆ mere      ┆ ind f…    ┆           ┆   ┆           ┆           ┆           ┆          │\n",
      "│ 3012771   ┆ Morten    ┆ FODBOLD:  ┆ 2023-06-2 ┆ … ┆ null      ┆ null      ┆ 0.8241    ┆ Negative │\n",
      "│           ┆ Bruun     ┆ Morten    ┆ 9         ┆   ┆           ┆           ┆           ┆          │\n",
      "│           ┆ fyret i   ┆ Bruun     ┆ 06:20:39  ┆   ┆           ┆           ┆           ┆          │\n",
      "│           ┆ Sønderjys ┆ fyret me… ┆           ┆   ┆           ┆           ┆           ┆          │\n",
      "│           ┆ …         ┆           ┆           ┆   ┆           ┆           ┆           ┆          │\n",
      "│ 3023463   ┆ Luderne   ┆ I landets ┆ 2023-06-2 ┆ … ┆ null      ┆ null      ┆ 0.7053    ┆ Neutral  │\n",
      "│           ┆ flytter   ┆ tyndest   ┆ 9         ┆   ┆           ┆           ┆           ┆          │\n",
      "│           ┆ på landet ┆ befolkede ┆ 06:20:43  ┆   ┆           ┆           ┆           ┆          │\n",
      "│           ┆           ┆ om…       ┆           ┆   ┆           ┆           ┆           ┆          │\n",
      "│ 3032577   ┆ Cybersex: ┆ En        ┆ 2023-06-2 ┆ … ┆ null      ┆ null      ┆ 0.9307    ┆ Neutral  │\n",
      "│           ┆ Hvornår   ┆ flirtende ┆ 9         ┆   ┆           ┆           ┆           ┆          │\n",
      "│           ┆ er man    ┆ sms til   ┆ 06:20:46  ┆   ┆           ┆           ┆           ┆          │\n",
      "│           ┆ utro?     ┆ den       ┆           ┆   ┆           ┆           ┆           ┆          │\n",
      "│           ┆           ┆ flott…    ┆           ┆   ┆           ┆           ┆           ┆          │\n",
      "│ …         ┆ …         ┆ …         ┆ …         ┆ … ┆ …         ┆ …         ┆ …         ┆ …        │\n",
      "│ 9803492   ┆ Vilde     ┆ Der er    ┆ 2023-06-2 ┆ … ┆ 100120    ┆ 4.112624e ┆ 0.6095    ┆ Neutral  │\n",
      "│           ┆ billeder: ┆ gang i    ┆ 9         ┆   ┆           ┆ 6         ┆           ┆          │\n",
      "│           ┆ Vulkan i  ┆ vulkanen  ┆ 06:49:26  ┆   ┆           ┆           ┆           ┆          │\n",
      "│           ┆ udbru…    ┆ på Hawa…  ┆           ┆   ┆           ┆           ┆           ┆          │\n",
      "│ 9803505   ┆ Flyvende  ┆ Verdens   ┆ 2023-06-2 ┆ … ┆ 959       ┆ 55691.0   ┆ 0.8884    ┆ Positive │\n",
      "│           ┆ Antonsen  ┆ nummer    ┆ 9         ┆   ┆           ┆           ┆           ┆          │\n",
      "│           ┆ knuser    ┆ syv, Chou ┆ 06:49:26  ┆   ┆           ┆           ┆           ┆          │\n",
      "│           ┆ topsp…    ┆ Tien-…    ┆           ┆   ┆           ┆           ┆           ┆          │\n",
      "│ 9803525   ┆ Dansk sku ┆ Julie R.  ┆ 2023-06-2 ┆ … ┆ 50361     ┆ 2.550671e ┆ 0.7737    ┆ Negative │\n",
      "│           ┆ espiller: ┆ Ølgaard   ┆ 9         ┆   ┆           ┆ 6         ┆           ┆          │\n",
      "│           ┆ - Jeg     ┆ fik akut  ┆ 06:49:26  ┆   ┆           ┆           ┆           ┆          │\n",
      "│           ┆ nægte…    ┆ kejs…     ┆           ┆   ┆           ┆           ┆           ┆          │\n",
      "│ 9803560   ┆ Så slemt  ┆ Tusindvis ┆ 2023-06-2 ┆ … ┆ 1237      ┆ 67514.0   ┆ 0.9927    ┆ Negative │\n",
      "│           ┆ er det:   ┆ af huse   ┆ 9         ┆   ┆           ┆           ┆           ┆          │\n",
      "│           ┆ 14.000    ┆ står      ┆ 06:49:26  ┆   ┆           ┆           ┆           ┆          │\n",
      "│           ┆ huse e…   ┆ under v…  ┆           ┆   ┆           ┆           ┆           ┆          │\n",
      "│ 9803607   ┆ Aktion    ┆ Flere     ┆ 2023-06-2 ┆ … ┆ 79590     ┆ 3.69476e6 ┆ 0.9948    ┆ Negative │\n",
      "│           ┆ mod svind ┆ kvinder   ┆ 9         ┆   ┆           ┆           ┆           ┆          │\n",
      "│           ┆ lere:     ┆ er ifølge ┆ 06:49:26  ┆   ┆           ┆           ┆           ┆          │\n",
      "│           ┆ Seks per… ┆ politi…   ┆           ┆   ┆           ┆           ┆           ┆          │\n",
      "└───────────┴───────────┴───────────┴───────────┴───┴───────────┴───────────┴───────────┴──────────┘\n"
     ]
    }
   ],
   "source": [
    "TRANSFORMER_MODEL_NAME = \"FacebookAI/xlm-roberta-base\"\n",
    "TEXT_COLUMNS_TO_USE = [DEFAULT_SUBTITLE_COL, DEFAULT_TITLE_COL]\n",
    "MAX_TITLE_LENGTH = 30\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(device)\n",
    "\n",
    "# LOAD HUGGINGFACE and move to device immediately:\n",
    "transformer_model = AutoModel.from_pretrained(TRANSFORMER_MODEL_NAME).to(device)\n",
    "transformer_tokenizer = AutoTokenizer.from_pretrained(TRANSFORMER_MODEL_NAME)\n",
    "\n",
    "# # We'll init the word embeddings using the\n",
    "# word2vec_embedding = get_transformers_word_embeddings(transformer_model)\n",
    "\n",
    "# # Concatenate text columns\n",
    "# df_articles, cat_cal = concat_str_columns(df_articles, columns=TEXT_COLUMNS_TO_USE)\n",
    "\n",
    "# # Get tokenized version\n",
    "# df_articles, token_col_title = convert_text2encoding_with_transformers(\n",
    "#     df_articles, transformer_tokenizer, cat_cal, max_length=MAX_TITLE_LENGTH\n",
    "# )\n",
    "\n",
    "print(\"DataFrame after tokenization:\")\n",
    "print(df_articles)\n",
    "\n",
    "# print(df_articles[token_col_title][0].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embedding tokenized article title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Encoding: 100%|██████████| 649/649 [01:38<00:00,  6.60text/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "from utils._python import batch_items_generator\n",
    "\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "n_batches = int(np.ceil(df_articles.height / BATCH_SIZE))\n",
    "\n",
    "chunked_text_list = batch_items_generator(df_articles[DEFAULT_TITLE_COL].to_list(), BATCH_SIZE)\n",
    "embeddings = (\n",
    "    generate_embeddings_with_transformers(\n",
    "        model=transformer_model,\n",
    "        tokenizer=transformer_tokenizer,\n",
    "        text_list=text_list,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        disable_tqdm=True,\n",
    "    )\n",
    "    for text_list in tqdm(\n",
    "        chunked_text_list, desc=\"Encoding\", total=n_batches, unit=\"text\"\n",
    "    )\n",
    ")\n",
    "embeddings = torch.vstack(list(embeddings))\n",
    "# print(embeddings.shape)\n",
    "# embedded_title = f\"{DEFAULT_TITLE_COL}_embedded\"\n",
    "\n",
    "# df_articles = df_articles.with_columns(pl.Series(embedded_title, embeddings.to(\"cpu\").numpy()))\n",
    "\n",
    "# article_mapping = create_article_id_to_value_mapping(\n",
    "#     df=df_articles, value_col=embedded_title\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reduce dimensionality of embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Dimensionality Reduction to 24 dimensions:\n",
      "Explained variance ratio (PCA): 89.16%\n",
      "Shape after reduction: (20738, 24)\n",
      "\n",
      "Dimensionality Reduction to 32 dimensions:\n",
      "Explained variance ratio (PCA): 90.54%\n",
      "Shape after reduction: (20738, 32)\n",
      "\n",
      "Dimensionality Reduction to 64 dimensions:\n",
      "Explained variance ratio (PCA): 94.02%\n",
      "Shape after reduction: (20738, 64)\n",
      "\n",
      "Dimensionality Reduction to 128 dimensions:\n",
      "Explained variance ratio (PCA): 97.20%\n",
      "Shape after reduction: (20738, 128)\n",
      "\n",
      "Dimensionality Reduction to 256 dimensions:\n",
      "Explained variance ratio (PCA): 99.17%\n",
      "Shape after reduction: (20738, 256)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.metrics import explained_variance_score\n",
    "import numpy as np\n",
    "\n",
    "def reduce_and_analyze_dimensionality(embeddings_array, target_dims=[24, 32, 64, 128, 256]):\n",
    "    \"\"\"\n",
    "    Reduce dimensionality using different methods and analyze information retention\n",
    "    \"\"\"\n",
    "    results = {}\n",
    "    \n",
    "    # PCA Analysis for different dimensions\n",
    "    for dim in target_dims:\n",
    "        # PCA\n",
    "        pca = PCA(n_components=dim)\n",
    "        reduced_data_pca = pca.fit_transform(embeddings_array)\n",
    "        \n",
    "        # Calculate explained variance ratio\n",
    "        explained_var = np.sum(pca.explained_variance_ratio_) * 100\n",
    "        \n",
    "        results[dim] = {\n",
    "            'method': 'PCA',\n",
    "            'explained_variance_ratio': explained_var,\n",
    "            'reduced_data': reduced_data_pca\n",
    "        }\n",
    "        \n",
    "        print(f\"\\nDimensionality Reduction to {dim} dimensions:\")\n",
    "        print(f\"Explained variance ratio (PCA): {explained_var:.2f}%\")\n",
    "        print(f\"Shape after reduction: {reduced_data_pca.shape}\")\n",
    "\n",
    "    return results\n",
    "\n",
    "# Convert embeddings to numpy array if it's not already\n",
    "embeddings_numpy = embeddings.cpu().numpy()\n",
    "\n",
    "# Analyze different dimensionality reductions\n",
    "reduction_results = reduce_and_analyze_dimensionality(embeddings_numpy)\n",
    "\n",
    "# Choose the dimension that provides good balance \n",
    "# between compression and information retention\n",
    "chosen_dim = hparams_nrms.__dict__['title_size']  # Adjust based on analysis results\n",
    "pca = PCA(n_components=chosen_dim)\n",
    "reduced_embeddings = pca.fit_transform(embeddings_numpy)\n",
    "\n",
    "# Update the dataframe with reduced embeddings\n",
    "embedded_title = f\"{DEFAULT_TITLE_COL}_embedded_reduced\"\n",
    "df_articles = df_articles.with_columns(pl.Series(embedded_title, reduced_embeddings))\n",
    "\n",
    "# Create new article mapping with reduced embeddings\n",
    "article_mapping = create_article_id_to_value_mapping(\n",
    "    df=df_articles, value_col=embedded_title\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Init dataloaders\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting preprocessing...\n",
      "Preprocessing data...\n",
      "(46855, 6)\n",
      "Data preprocessing completed in 5.12 seconds.\n",
      "Starting preprocessing...\n",
      "Preprocessing data...\n",
      "(48660, 6)\n",
      "Data preprocessing completed in 5.88 seconds.\n"
     ]
    }
   ],
   "source": [
    "train_dataset = NRMSDataSet(\n",
    "    behaviors=df_train,\n",
    "    article_dict=article_mapping,\n",
    "    unknown_representation=\"zeros\",\n",
    "    history_column=DEFAULT_HISTORY_ARTICLE_ID_COL,\n",
    "    eval_mode=False,\n",
    ")\n",
    "val_dataset = NRMSDataSet(\n",
    "    behaviors=df_validation,\n",
    "    article_dict=article_mapping,\n",
    "    unknown_representation=\"zeros\",\n",
    "    history_column=DEFAULT_HISTORY_ARTICLE_ID_COL,\n",
    "    eval_mode=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for idx in range(5):\n",
    "#     sample = train_dataset[idx]\n",
    "#     print(f\"Sample {idx}:\")\n",
    "#     print(f\"his_input_title shape: {sample[0][0].shape}\")\n",
    "#     print(f\"pred_input_title shape: {sample[0][1].shape} {sample[0][1].sum()}\")\n",
    "#     print(f\"Targets shape: {sample[1].shape} , {sample[1].dtype} {sample[1].sum()}\")\n",
    "#     print(f\"impression id: {sample[2]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "\n",
    "def collate_fn_with_global_padding(batch, max_len_pred, apply_padding_to_targets : bool = True):\n",
    "    try:\n",
    "        his_input_titles = [item[0][0] for item in batch]  # History inputs\n",
    "        pred_input_titles = [item[0][1] for item in batch]  # Prediction inputs\n",
    "        batch_ys = [item[1] for item in batch]  # Targets\n",
    "        impression_id = torch.tensor([item[2] for item in batch], dtype=torch.int64)  # Impression ID\n",
    "        \n",
    "\n",
    "        # Pad sequences to the global maximum length\n",
    "        his_input_titles_padded = pad_sequence(his_input_titles, batch_first=True, padding_value=0)\n",
    "\n",
    "        # Pad prediction inputs and adjust to the global maximum length\n",
    "        pred_input_titles_padded = pad_sequence(pred_input_titles, batch_first=True, padding_value=0)\n",
    "        if pred_input_titles_padded.size(1) < max_len_pred:\n",
    "            # Add padding if sequence length is shorter than max_len_pred\n",
    "            pad_size = max_len_pred - pred_input_titles_padded.size(1)\n",
    "            pred_input_titles_padded = torch.nn.functional.pad(\n",
    "                pred_input_titles_padded, (0, 0, 0, pad_size), value=0\n",
    "            )\n",
    "        elif pred_input_titles_padded.size(1) > max_len_pred:\n",
    "            # Trim if sequence length exceeds max_len_pred\n",
    "            pred_input_titles_padded = pred_input_titles_padded[:, :max_len_pred, :]\n",
    "\n",
    "        # Pad targets to the global maximum length\n",
    "        if apply_padding_to_targets:\n",
    "            batch_ys_padded = pad_sequence(batch_ys, batch_first=True, padding_value=-1)\n",
    "            if batch_ys_padded.size(1) < max_len_pred:\n",
    "                pad_size = max_len_pred - batch_ys_padded.size(1)\n",
    "                batch_ys_padded = torch.nn.functional.pad(batch_ys_padded, (0, pad_size), value=-1)\n",
    "            elif batch_ys_padded.size(1) > max_len_pred:\n",
    "                batch_ys_padded = batch_ys_padded[:, :max_len_pred]\n",
    "\n",
    "            return (his_input_titles_padded, pred_input_titles_padded), batch_ys_padded, impression_id\n",
    "        else:\n",
    "            return (his_input_titles_padded, pred_input_titles_padded), batch_ys, impression_id\n",
    "    except Exception as e:\n",
    "        print(f\"Error in collate_fn: {e}\")\n",
    "        raise\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wrap the dataset with DataLoader\n",
    "train_dataloader_temp = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=64,    # Set your desired batch size\n",
    "    shuffle=True,\n",
    "    num_workers=0,\n",
    "    collate_fn=lambda batch: collate_fn_with_global_padding(batch, max_inview_length_validation)\n",
    ")\n",
    "\n",
    "val_dataloader_temp = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=128,    # Set your desired batch size\n",
    "    shuffle=False,\n",
    "    num_workers=0,\n",
    "    collate_fn=lambda batch: collate_fn_with_global_padding(batch, max_inview_length_validation)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([128, 91, 24])\n",
      "torch.Size([128, 91])\n",
      "tensor([476486371, 486250838, 548034706, 429985421, 270600342, 144912392,\n",
      "        319273101, 442415886, 331908826, 124975098, 495129029, 575675398,\n",
      "        155091158, 376952414, 128143292, 434392815, 397020722, 351091793,\n",
      "        394979437, 436060607, 332555337, 401473097,  70913094, 329037752,\n",
      "        447115726,  53124324, 274131390, 515455547, 226074360, 328520011,\n",
      "        321307410, 412371681,  98500333, 544891149, 397022344, 531607281,\n",
      "         26682679,  43174555, 203419507, 356069058, 517325908, 469162826,\n",
      "        188190283, 302024944, 273724541, 198613745, 503451087, 355969488,\n",
      "        244824875, 161808169, 213471439, 434566162, 267986090,  92451239,\n",
      "         30808655, 260170279, 423992874, 325180839, 458238378,  74248419,\n",
      "        175413720, 550742362, 544099488, 415832851,  92251298, 293945647,\n",
      "        576714998, 284629602, 571329168, 209419784,  74735327, 267616804,\n",
      "        185873450, 227138675, 297062630, 258869509, 302263064,  55966732,\n",
      "        420337081, 130668611,  72968906, 222965717, 123961757, 291297029,\n",
      "        208891341, 163218613,  10017410, 538881013,   4736464, 102583733,\n",
      "        220924233,  38685173, 111074532, 448735917, 228793379, 490637707,\n",
      "        432444264, 458410482, 483718479, 180911045,  98013556, 185122465,\n",
      "        406001896, 128372913, 566558385, 412272737, 506006141, 365868321,\n",
      "         57181830, 455427117,  77564079,  76761995, 516098463, 195739824,\n",
      "        323297051, 113024993, 181918352,  79915649, 161224753, 559267502,\n",
      "         96091112, 294997513, 188281297, 382517286,  40733848,  28223356,\n",
      "        321309202, 526022341])\n",
      "Batch loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "for batch in val_dataloader_temp:\n",
    "    (his_input_titles_padded, pred_input_titles_padded), batch_ys_padded, impression_id = batch\n",
    "    print(pred_input_titles_padded.shape)  # Look at one padded sequence\n",
    "    print(batch_ys_padded.shape)  # Look at one padded sequence\n",
    "    print(impression_id)\n",
    "\n",
    "    print(\"Batch loaded successfully!\")\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # THIS CODE SHOULD ONLY RUN WHEN GENERATING THE DATA FOR THE FIRST TIME\n",
    "# import torch\n",
    "# from tqdm import tqdm\n",
    "\n",
    "# # Function to preprocess and save data\n",
    "# def preprocess_and_save(dataloader, filepath, device=\"cuda\"):\n",
    "#     all_inputs_his = []\n",
    "#     all_inputs_pred = []\n",
    "#     all_targets = []\n",
    "#     all_impression_ids = []\n",
    "\n",
    "#     # Iterate over DataLoader and collect data\n",
    "#     for (his_inputs, pred_inputs), targets, impressionID in tqdm(dataloader, desc=\"Processing Data\"):\n",
    "#         all_inputs_his.append(his_inputs)\n",
    "#         all_inputs_pred.append(pred_inputs)\n",
    "#         all_targets.append(targets)\n",
    "#         all_impression_ids.append(impressionID)\n",
    "\n",
    "#     # Concatenate all batches into a single tensor\n",
    "#     all_inputs_his = torch.cat(all_inputs_his).to(device)\n",
    "#     all_inputs_pred = torch.cat(all_inputs_pred).to(device)\n",
    "#     all_targets = torch.cat(all_targets).to(device)\n",
    "#     all_impression_ids = torch.cat(all_impression_ids).to(device)\n",
    "\n",
    "#     # Save the preprocessed data as a tuple\n",
    "#     torch.save((all_inputs_his, all_inputs_pred, all_targets, all_impression_ids), filepath)\n",
    "#     print(f\"Data saved to {filepath}\")\n",
    "\n",
    "# # Save train and validation data\n",
    "# preprocess_and_save(val_dataloader_temp, \"val_data_small_dataset_with_impression_ids.pt\", device=\"cuda\")\n",
    "\n",
    "# preprocess_and_save(train_dataloader_temp, \"train_data_small_dataset_with_impression_ids.pt\", device=\"cuda\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def load_preprocessed_data(filepath, device=\"cuda\"):\n",
    "#     # Load the data from the .pt file\n",
    "#     data = torch.load(filepath)\n",
    "\n",
    "#     # Unpack the data\n",
    "#     his_inputs, pred_inputs, targets, impression_ids = data\n",
    "\n",
    "#     # Move the data to the specified device\n",
    "#     his_inputs = his_inputs.to(device, non_blocking=True)\n",
    "#     pred_inputs = pred_inputs.to(device, non_blocking=True)\n",
    "#     targets = targets.to(device, non_blocking=True)\n",
    "#     impression_ids = impression_ids.to(device, non_blocking=True)\n",
    "\n",
    "#     return his_inputs, pred_inputs, targets, impression_ids\n",
    "\n",
    "# # Example: Load train and validation data\n",
    "# device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "# train_his_inputs, train_pred_inputs, train_targets, impression_ids = load_preprocessed_data(\"train_data_small_dataset_with_impression_ids.pt\", device)\n",
    "# val_his_inputs, val_pred_inputs, val_targets, impression_ids = load_preprocessed_data(\"val_data_small_dataset_with_impression_ids.pt\", device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def create_batches(inputs, targets, impression_ids, batch_size):\n",
    "#     his_inputs, pred_inputs = inputs\n",
    "#     for i in range(0, his_inputs.size(0), batch_size):\n",
    "#         his_batch = his_inputs[i:i+batch_size]\n",
    "#         pred_batch = pred_inputs[i:i+batch_size]\n",
    "#         target_batch = targets[i:i+batch_size]\n",
    "#         impression_id_batch = impression_ids[i:i+batch_size]\n",
    "#         yield (his_batch, pred_batch), target_batch, impression_id_batch\n",
    "\n",
    "# # Set the batch size\n",
    "# batch_size = 64\n",
    "\n",
    "# # Example: Create batches for train and validation data\n",
    "# #train_batches = create_batches((train_his_inputs, train_pred_inputs), train_targets, batch_size)\n",
    "# #val_batches = create_batches((val_his_inputs, val_pred_inputs), val_targets, batch_size)\n",
    "\n",
    "# train_batches = list(create_batches((train_his_inputs, train_pred_inputs), train_targets, impression_ids, batch_size))\n",
    "# val_batches = list(create_batches((val_his_inputs, val_pred_inputs), val_targets, impression_ids, batch_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for (his_batch, pred_batch), target_batch, impression_ids_batch in train_batches:\n",
    "#     print(f\"his_batch device: {his_batch.device}\")\n",
    "#     print(f\"pred_batch device: {pred_batch.device}\")\n",
    "#     print(f\"target_batch device: {target_batch.device}\")\n",
    "#     print(f\"impression_ids_batch device: {impression_ids_batch.device}\")\n",
    "#     break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create hyper-parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'__module__': 'models_pytorch.model_config', '__annotations__': {'title_size': <class 'int'>, 'embedding_dim': <class 'int'>, 'word_emb_dim': <class 'int'>, 'vocab_size': <class 'int'>, 'head_num': <class 'int'>, 'head_dim': <class 'int'>, 'attention_hidden_dim': <class 'int'>, 'optimizer': <class 'str'>, 'loss': <class 'str'>, 'dropout': <class 'float'>, 'learning_rate': <class 'float'>, 'weight_decay': <class 'float'>, 'units_per_layer': list[int]}, 'title_size': 24, 'embedding_dim': 32, 'word_emb_dim': 8, 'vocab_size': 10000, 'head_num': 12, 'head_dim': 12, 'attention_hidden_dim': 128, 'hidden_dim': 4, 'optimizer': 'adam', 'loss': 'cross_entropy_loss', 'dropout': 0.2, 'learning_rate': 0.0001, 'weight_decay': 0.001, 'news_output_dim': 64, 'units_per_layer': [128, 128, 128], '__dict__': <attribute '__dict__' of 'hparams_nrms' objects>, '__weakref__': <attribute '__weakref__' of 'hparams_nrms' objects>, '__doc__': None}\n"
     ]
    }
   ],
   "source": [
    "# see the model parameters: \n",
    "print(hparams_nrms.__dict__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the NRMS model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n",
      "Model device: mps:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/gustavsiphone/Documents/GitHub/Deeplearning-RecSys-Challenge-2024/models_pytorch/nrms.py:142: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  self.scaler = amp.GradScaler()\n",
      "/Users/gustavsiphone/Documents/GitHub/Deeplearning-RecSys-Challenge-2024/.venv/lib/python3.12/site-packages/torch/amp/grad_scaler.py:132: UserWarning: torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "# Define paths\n",
    "MODEL_NAME = \"NRMS\"\n",
    "LOG_DIR = os.path.join(\"downloads\", \"runs\", MODEL_NAME)\n",
    "MODEL_WEIGHTS = os.path.join(\"downloads\", \"data\", \"state_dict\", MODEL_NAME, \"weights.pth\")\n",
    "\n",
    "# Create directories if they don't exist\n",
    "os.makedirs(LOG_DIR, exist_ok=True)\n",
    "os.makedirs(os.path.dirname(MODEL_WEIGHTS), exist_ok=True)\n",
    "\n",
    "# Define ModelCheckpoint class\n",
    "class ModelCheckpoint:\n",
    "    \"\"\"Saves the model after every epoch if it has the best performance so far.\"\"\"\n",
    "    def __init__(self, filepath, verbose=False, save_best_only=True):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            filepath (str): Path to save the model checkpoint.\n",
    "            verbose (bool): If True, prints a message when the model is saved.\n",
    "            save_best_only (bool): If True, saves only when the model is better than before.\n",
    "        \"\"\"\n",
    "        self.filepath = filepath\n",
    "        self.verbose = verbose\n",
    "        self.save_best_only = save_best_only\n",
    "        self.best_loss = None\n",
    "\n",
    "    def __call__(self, model, val_loss):\n",
    "        if self.best_loss is None:\n",
    "            self.best_loss = val_loss\n",
    "            self.save_checkpoint(model)\n",
    "        elif val_loss < self.best_loss:\n",
    "            self.best_loss = val_loss\n",
    "            self.save_checkpoint(model)\n",
    "\n",
    "    def save_checkpoint(self, model):\n",
    "        torch.save(model.state_dict(), self.filepath)\n",
    "        if self.verbose:\n",
    "            print(f\"Model saved to {self.filepath}\")\n",
    "\n",
    "# Define EarlyStopping class\n",
    "class EarlyStopping:\n",
    "    \"\"\"Early stops the training if validation loss doesn't improve by a given percentage over a patience period.\"\"\"\n",
    "    def __init__(self, patience=3, min_delta=0.05, verbose=False):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            patience (int): Number of epochs to wait after last time validation loss improved by min_delta.\n",
    "            min_delta (float): Minimum percentage improvement required to reset patience.\n",
    "            verbose (bool): If True, prints a message when early stopping is triggered.\n",
    "        \"\"\"\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta  # Minimum percentage improvement\n",
    "        self.verbose = verbose\n",
    "        self.counter = 0\n",
    "        self.best_loss = None\n",
    "        self.early_stop = False\n",
    "\n",
    "    def __call__(self, val_loss):\n",
    "        if self.best_loss is None:\n",
    "            # Initialize best_loss with the first validation loss\n",
    "            self.best_loss = val_loss\n",
    "            self.counter = 0\n",
    "        elif val_loss < self.best_loss * (1 - self.min_delta):\n",
    "            # Significant improvement found\n",
    "            self.best_loss = val_loss\n",
    "            self.counter = 0\n",
    "            if self.verbose:\n",
    "                print(f\"Validation loss improved by at least {self.min_delta*100:.1f}%\")\n",
    "        else:\n",
    "            # No significant improvement\n",
    "            self.counter += 1\n",
    "            if self.verbose:\n",
    "                print(f\"No significant improvement in validation loss. Counter: {self.counter}/{self.patience}\")\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "\n",
    "# Initialize TensorBoard SummaryWriter\n",
    "writer = SummaryWriter(log_dir=LOG_DIR)\n",
    "\n",
    "# Initialize callbacks\n",
    "model_checkpoint = ModelCheckpoint(filepath=MODEL_WEIGHTS, verbose=True, save_best_only=True)\n",
    "# Initialize EarlyStopping\n",
    "early_stopping = EarlyStopping(patience=3, min_delta=0.05, verbose=True)\n",
    "\n",
    "# Initialize your model\n",
    "# Ensure that NRMSModel is a PyTorch nn.Module\n",
    "\n",
    "# CUDA checks\n",
    "#print(f\"CUDA Available: {torch.cuda.is_available()}\")\n",
    "#print(f\"Current Device: {torch.cuda.current_device()}\")\n",
    "#print(f\"Device Name: {torch.cuda.get_device_name()}\")\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    \n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Initialize model\n",
    "# model = NRMSModel(\n",
    "#     hparams=hparams_nrms.__dict__,\n",
    "#     word2vec_embedding=word2vec_embedding,\n",
    "#     vocab_size=1000,\n",
    "#     word_emb_dim=8,\n",
    "#     seed=seed,\n",
    "#     device=device\n",
    "# )\n",
    "\n",
    "# model = NRMSDocVecModel(hparams=hparams_nrms.__dict__,\n",
    "#                         device=device)\n",
    "\n",
    "model = NRMSModel(hparams=hparams_nrms.__dict__,\n",
    "                        device=device)\n",
    "print(f\"Model device: {next(model.parameters()).device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def print_prediction_details(outputs, targets, k=5):\n",
    "#     \"\"\"Print detailed prediction information for the first k samples\"\"\"\n",
    "#     # Get predicted class (highest score)\n",
    "#     _, predicted = torch.max(outputs.data, 1)\n",
    "    \n",
    "#     # Calculate accuracy for this batch\n",
    "#     correct = (predicted == targets).sum().item()\n",
    "#     total = targets.size(0)\n",
    "#     accuracy = 100 * correct / total\n",
    "    \n",
    "#     # Print details for k samples\n",
    "#     for i in range(min(k, len(targets))):\n",
    "#         print(f\"\\nSample {i}:\")\n",
    "#         print(f\"Predicted probabilities: {torch.softmax(outputs[i], dim=0)}\")\n",
    "#         print(f\"Predicted class: {predicted[i]}, True class: {targets[i]}\")\n",
    "#         print(f\"Correct: {predicted[i] == targets[i]}\")\n",
    "    \n",
    "#     return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NRMSModel(\n",
      "  (newsencoder): NewsEncoder(\n",
      "    (self_attention): SelfAttention()\n",
      "    (attention_layer): AttLayer2(\n",
      "      (q): Linear(in_features=128, out_features=1, bias=False)\n",
      "    )\n",
      "    (fc1): Linear(in_features=128, out_features=128, bias=True)\n",
      "    (relu): ReLU()\n",
      "    (fc2): Linear(in_features=128, out_features=64, bias=True)\n",
      "  )\n",
      "  (userencoder): UserEncoder(\n",
      "    (titleencoder): NewsEncoder(\n",
      "      (self_attention): SelfAttention()\n",
      "      (attention_layer): AttLayer2(\n",
      "        (q): Linear(in_features=128, out_features=1, bias=False)\n",
      "      )\n",
      "      (fc1): Linear(in_features=128, out_features=128, bias=True)\n",
      "      (relu): ReLU()\n",
      "      (fc2): Linear(in_features=128, out_features=64, bias=True)\n",
      "    )\n",
      "    (self_attention): SelfAttention()\n",
      "    (attention_layer): AttLayer2(\n",
      "      (q): Linear(in_features=64, out_features=1, bias=False)\n",
      "    )\n",
      "    (user_projection): Linear(in_features=64, out_features=64, bias=True)\n",
      "  )\n",
      ")\n",
      "Layer: newsencoder.attention_layer.b | Size: torch.Size([128])\n",
      "Layer: newsencoder.attention_layer.q.weight | Size: torch.Size([1, 128])\n",
      "Layer: newsencoder.fc1.weight | Size: torch.Size([128, 128])\n",
      "Layer: newsencoder.fc1.bias | Size: torch.Size([128])\n",
      "Layer: newsencoder.fc2.weight | Size: torch.Size([64, 128])\n",
      "Layer: newsencoder.fc2.bias | Size: torch.Size([64])\n",
      "Layer: userencoder.attention_layer.b | Size: torch.Size([64])\n",
      "Layer: userencoder.attention_layer.q.weight | Size: torch.Size([1, 64])\n",
      "Layer: userencoder.user_projection.weight | Size: torch.Size([64, 64])\n",
      "Layer: userencoder.user_projection.bias | Size: torch.Size([64])\n",
      "\n",
      "Total parameters: 29,312\n"
     ]
    }
   ],
   "source": [
    "# 1. Print model architecture\n",
    "print(model)\n",
    "\n",
    "# 2. Print specific layer sizes\n",
    "for name, param in model.named_parameters():\n",
    "    print(f\"Layer: {name} | Size: {param.size()}\")\n",
    "\n",
    "# 3. Get total number of parameters\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"\\nTotal parameters: {total_params:,}\")\n",
    "\n",
    "# 4. Print layer by layer with shapes\n",
    "def print_model_structure(model):\n",
    "    print(\"\\nDetailed Model Structure:\")\n",
    "    for name, module in model.named_children():\n",
    "        print(f\"\\nLayer: {name}\")\n",
    "        print(f\"Type: {type(module).__name__}\")\n",
    "        if hasattr(module, 'weight'):\n",
    "            print(f\"Shape: {module.weight.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for name, param in model.named_parameters():\n",
    "#     print(f\"Layer: {name} | Size: {param.size()} | Parameters: {param.numel()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MAX_GRAD_NORM = np.sqrt(sum(p.numel() for p in model.parameters()))\n",
    "# print(f\"Max grad norm: {MAX_GRAD_NORM}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###\n",
    "Added gradient clipping to avoid exploiding gradients. The paramter MAX_GRAD_NORM is set to 5.0 as this is a common value used in transformers. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# # Variables to track counts\n",
    "# total_inputs = 0\n",
    "# total_targets = 0\n",
    "\n",
    "# # Iterate over the DataLoader\n",
    "# for batch_idx, (inputs, targets, impression_ids) in enumerate(train_dataloader_temp):\n",
    "#     # Move data to GPU\n",
    "#     inputs = [inp.to(device) for inp in inputs]\n",
    "#     targets = targets.to(device)\n",
    "#     impression_ids = impression_ids.to(device)\n",
    "    \n",
    "#     # Print information for the first few batches to avoid delays\n",
    "#     if batch_idx < 5:  # Adjust the number of batches to print as needed\n",
    "#         print(f\"Batch {batch_idx + 1} (on {device}):\")\n",
    "#         print(f\"  - Number of inputs: {len(inputs[0])}\")  # History input\n",
    "#         print(f\"  - Number of targets: {len(targets)}\")   # Target labels\n",
    "#         print(f\"  - Impression IDs: {len(impression_ids)}\")\n",
    "    \n",
    "#     # Update total counts\n",
    "#     total_inputs += len(inputs[0])\n",
    "#     total_targets += len(targets)\n",
    "\n",
    "# # Final counts after iteration\n",
    "# print(f\"\\nTotal number of inputs in train_dataloader_temp: {total_inputs}\")\n",
    "# print(f\"Total number of targets in train_dataloader_temp: {total_targets}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperoptimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NUM_EPOCHS = 30\n",
    "\n",
    "# def objective(trial):\n",
    "#     # Define hyperparameter search space\n",
    "#     hparams = {\n",
    "#         'title_size': 768,\n",
    "#         'history_size': trial.suggest_int('history_size', 5, 20),\n",
    "#         'head_num': trial.suggest_int('head_num', 2, 8),\n",
    "#         'head_dim': trial.suggest_int('head_dim', 4, 16),\n",
    "#         'attention_hidden_dim': trial.suggest_int('attention_hidden_dim', 32, 128),\n",
    "#         'dropout': trial.suggest_float('dropout', 0.1, 0.5),\n",
    "#         'learning_rate': trial.suggest_float('learning_rate', 1e-5, 1e-3, log=True),\n",
    "#         'news_output_dim': trial.suggest_int('news_output_dim', 32, 128),\n",
    "#         'units_per_layer': [trial.suggest_int(f'unit_layer_{i}', 32, 128) for i in range(3)]\n",
    "#     }\n",
    "\n",
    "#     # Initialize model and training components\n",
    "#     model = NRMSDocVecModel(hparams=hparams, device=device)\n",
    "#     criterion = model.get_loss().to(device)\n",
    "#     optimizer = optim.Adam(model.parameters(), \n",
    "#                           lr=hparams['learning_rate'], \n",
    "#                           weight_decay=WEIGHT_DECAY)\n",
    "    \n",
    "#     # Initialize EarlyStopping\n",
    "#     early_stopping = EarlyStopping(patience=3, min_delta=0.05, verbose=True)\n",
    "#     best_val_loss = float('inf')\n",
    "\n",
    "#     for epoch in range(NUM_EPOCHS):\n",
    "#         # Training phase\n",
    "#         model.train()\n",
    "#         train_loss = train_one_epoch(model, train_dataloader_temp, optimizer, criterion)\n",
    "        \n",
    "#         # Validation phase\n",
    "#         val_loss = validate(model, val_dataloader_temp, criterion)\n",
    "        \n",
    "#         # Update best validation loss\n",
    "#         if val_loss < best_val_loss:\n",
    "#             best_val_loss = val_loss\n",
    "\n",
    "#         # Early stopping check\n",
    "#         early_stopping(val_loss)\n",
    "#         if early_stopping.early_stop:\n",
    "#             print(f\"Early stopping triggered at epoch {epoch}\")\n",
    "#             break\n",
    "\n",
    "#         # Report to Optuna\n",
    "#         trial.report(val_loss, epoch)\n",
    "#         if trial.should_prune():\n",
    "#             raise optuna.TrialPruned()\n",
    "\n",
    "#     return best_val_loss\n",
    "\n",
    "# def train_one_epoch(model, dataloader, optimizer, criterion):\n",
    "#     running_loss = 0.0\n",
    "#     batch_count = 0\n",
    "    \n",
    "#     for inputs, targets, impression_ids in dataloader:\n",
    "#         inputs = [inp.to(device) for inp in inputs]\n",
    "#         targets = targets.to(device)\n",
    "#         positive_indices = (targets == 1).nonzero(as_tuple=False)\n",
    "#         targets = positive_indices[:, 1].long()\n",
    "        \n",
    "#         optimizer.zero_grad()\n",
    "#         outputs = model(*inputs)\n",
    "#         loss = criterion(outputs, targets)\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "        \n",
    "#         running_loss += loss.item()\n",
    "#         batch_count += 1\n",
    "    \n",
    "#     return running_loss / batch_count\n",
    "\n",
    "# def validate(model, dataloader, criterion):\n",
    "#     model.eval()\n",
    "#     val_loss = 0.0\n",
    "#     batch_count = 0\n",
    "    \n",
    "#     with torch.no_grad():\n",
    "#         for inputs, targets, impression_ids in dataloader:\n",
    "#             inputs = [inp.to(device) for inp in inputs]\n",
    "#             targets = targets.to(device)\n",
    "#             positive_indices = (targets == 1).nonzero(as_tuple=False)\n",
    "#             targets = positive_indices[:, 1].long()\n",
    "#             outputs = model(*inputs)\n",
    "#             loss = criterion(outputs, targets)\n",
    "#             val_loss += loss.item()\n",
    "#             batch_count += 1\n",
    "    \n",
    "#     return val_loss / batch_count\n",
    "\n",
    "# # Create study and optimize\n",
    "# study = optuna.create_study(direction='minimize')\n",
    "# study.optimize(objective, n_trials=3)  # Run 50 trials\n",
    "\n",
    "# # Print results\n",
    "# print(\"Best trial:\")\n",
    "# trial = study.best_trial\n",
    "\n",
    "# print(\"  Value: \", trial.value)\n",
    "# print(\"  Params: \")\n",
    "# for key, value in trial.params.items():\n",
    "#     print(f\"    {key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the NRMS model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:   0%|          | 0/30 [00:00<?, ?it/s]/Users/gustavsiphone/Documents/GitHub/Deeplearning-RecSys-Challenge-2024/models_pytorch/nrms.py:159: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast():\n",
      "/Users/gustavsiphone/Documents/GitHub/Deeplearning-RecSys-Challenge-2024/.venv/lib/python3.12/site-packages/torch/amp/autocast_mode.py:266: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
      "  warnings.warn(\n",
      "Training Progress:  33%|███▎      | 10/30 [03:33<07:11, 21.59s/it, train_loss=1.6668, val_loss=2.4269]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to downloads/data/state_dict/NRMS/weights.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  67%|██████▋   | 20/30 [07:04<03:29, 21.00s/it, train_loss=1.6289, val_loss=2.3910]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to downloads/data/state_dict/NRMS/weights.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  83%|████████▎ | 25/30 [09:10<01:50, 22.01s/it, train_loss=1.6174, val_loss=2.4108]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[47], line 52\u001b[0m\n\u001b[1;32m     50\u001b[0m positive_indices \u001b[38;5;241m=\u001b[39m (targets \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mnonzero(as_tuple\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m     51\u001b[0m targets \u001b[38;5;241m=\u001b[39m positive_indices[:, \u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mlong()\n\u001b[0;32m---> 52\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     53\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(outputs, targets)\n\u001b[1;32m     55\u001b[0m val_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n",
      "File \u001b[0;32m~/Documents/GitHub/Deeplearning-RecSys-Challenge-2024/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/GitHub/Deeplearning-RecSys-Challenge-2024/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/Documents/GitHub/Deeplearning-RecSys-Challenge-2024/models_pytorch/nrms.py:164\u001b[0m, in \u001b[0;36mNRMSModel.forward\u001b[0;34m(self, his_input_title, pred_input_title)\u001b[0m\n\u001b[1;32m    161\u001b[0m pred_input_title \u001b[38;5;241m=\u001b[39m pred_input_title\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m    163\u001b[0m user_present \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muserencoder(his_input_title)\n\u001b[0;32m--> 164\u001b[0m news_present \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_batch_encode_news\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpred_input_title\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    166\u001b[0m \u001b[38;5;66;03m# Compute scores using batch matrix multiplication\u001b[39;00m\n\u001b[1;32m    167\u001b[0m scores \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mbmm(news_present, user_present\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m))\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m~/Documents/GitHub/Deeplearning-RecSys-Challenge-2024/models_pytorch/nrms.py:155\u001b[0m, in \u001b[0;36mNRMSModel._batch_encode_news\u001b[0;34m(self, titles)\u001b[0m\n\u001b[1;32m    153\u001b[0m batch_size, num_titles, title_size \u001b[38;5;241m=\u001b[39m titles\u001b[38;5;241m.\u001b[39msize()\n\u001b[1;32m    154\u001b[0m titles_flat \u001b[38;5;241m=\u001b[39m titles\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, title_size)\n\u001b[0;32m--> 155\u001b[0m encoded_flat \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnewsencoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtitles_flat\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    156\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m encoded_flat\u001b[38;5;241m.\u001b[39mview(batch_size, num_titles, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m~/Documents/GitHub/Deeplearning-RecSys-Challenge-2024/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/GitHub/Deeplearning-RecSys-Challenge-2024/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/Documents/GitHub/Deeplearning-RecSys-Challenge-2024/models_pytorch/nrms.py:102\u001b[0m, in \u001b[0;36mNewsEncoder.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m     95\u001b[0m     \u001b[38;5;66;03m# x = x.to(self.device)\u001b[39;00m\n\u001b[1;32m     96\u001b[0m     \u001b[38;5;66;03m# x = x.long()\u001b[39;00m\n\u001b[1;32m     97\u001b[0m     \n\u001b[1;32m     98\u001b[0m     \u001b[38;5;66;03m# embedded = self.embedding(x)\u001b[39;00m\n\u001b[1;32m     99\u001b[0m     \u001b[38;5;66;03m# embedded = embedded.mean(dim=1)  # Aggregate over sequence length\u001b[39;00m\n\u001b[0;32m--> 102\u001b[0m     embedded \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mself_attention\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    103\u001b[0m     embedded \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mattention_layer(embedded)\n\u001b[1;32m    107\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc1(embedded)\n",
      "File \u001b[0;32m~/Documents/GitHub/Deeplearning-RecSys-Challenge-2024/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/GitHub/Deeplearning-RecSys-Challenge-2024/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/Documents/GitHub/Deeplearning-RecSys-Challenge-2024/models_pytorch/layers.py:102\u001b[0m, in \u001b[0;36mSelfAttention.forward\u001b[0;34m(self, QKVs)\u001b[0m\n\u001b[1;32m    100\u001b[0m Q \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mWQ(Q_seq)\n\u001b[1;32m    101\u001b[0m K \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mWK(K_seq)\n\u001b[0;32m--> 102\u001b[0m V \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mWV\u001b[49m\u001b[43m(\u001b[49m\u001b[43mV_seq\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    103\u001b[0m \u001b[38;5;66;03m#print(\"SelfAttention Forward - Q, K, V shapes after linear transformations:\", Q.shape, K.shape, V.shape)\u001b[39;00m\n\u001b[1;32m    105\u001b[0m batch_size \u001b[38;5;241m=\u001b[39m Q\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m)\n",
      "File \u001b[0;32m~/Documents/GitHub/Deeplearning-RecSys-Challenge-2024/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/GitHub/Deeplearning-RecSys-Challenge-2024/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/Documents/GitHub/Deeplearning-RecSys-Challenge-2024/.venv/lib/python3.12/site-packages/torch/nn/modules/linear.py:125\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    124\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 125\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "NUM_EPOCHS = 30\n",
    "\n",
    "# Define loss function and optimizer\n",
    "criterion = model.get_loss().to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=hparams_nrms.__dict__['learning_rate'], weight_decay=hparams_nrms.__dict__['weight_decay'])\n",
    "\n",
    "# Training parameters\n",
    "train_losses, val_losses = [], []\n",
    "\n",
    "# Epoch progress bar\n",
    "epoch_pbar = tqdm(range(1, NUM_EPOCHS + 1), desc=\"Training Progress\", dynamic_ncols=True)\n",
    "for epoch in epoch_pbar:\n",
    "    # Training phase\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    train_batch_count = 0\n",
    "\n",
    "    for batch_idx, (inputs, targets, impression_ids) in enumerate(train_dataloader_temp):\n",
    "        # Prepare data\n",
    "        inputs = [inp.to(device) for inp in inputs]\n",
    "        targets = targets.to(device)\n",
    "\n",
    "        # Get positive labels\n",
    "        positive_indices = (targets == 1).nonzero(as_tuple=False)\n",
    "        targets = positive_indices[:, 1].long()\n",
    "\n",
    "        # Forward and backward passes\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(*inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Update running statistics\n",
    "        running_loss += loss.item()\n",
    "        train_batch_count += 1\n",
    "\n",
    "    # Compute average training loss\n",
    "    avg_train_loss = running_loss / train_batch_count if train_batch_count > 0 else float('inf')\n",
    "    train_losses.append(avg_train_loss)\n",
    "\n",
    "    # Validation phase\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    val_batch_count = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, targets, impression_ids in val_dataloader_temp:\n",
    "            inputs = [inp.to(device) for inp in inputs]\n",
    "            targets = targets.to(device)\n",
    "            positive_indices = (targets == 1).nonzero(as_tuple=False)\n",
    "            targets = positive_indices[:, 1].long()\n",
    "            outputs = model(*inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "\n",
    "            val_loss += loss.item()\n",
    "            val_batch_count += 1\n",
    "\n",
    "    avg_val_loss = val_loss / val_batch_count if val_batch_count > 0 else float('inf')\n",
    "    val_losses.append(avg_val_loss)\n",
    "\n",
    "    # Update tensorboard\n",
    "    writer.add_scalar('Loss/Train', avg_train_loss, epoch)\n",
    "    writer.add_scalar('Loss/Validation', avg_val_loss, epoch)\n",
    "\n",
    "    # Update epoch progress bar with metrics\n",
    "    epoch_pbar.set_postfix({\n",
    "        'train_loss': f'{avg_train_loss:.4f}',\n",
    "        'val_loss': f'{avg_val_loss:.4f}',\n",
    "    })\n",
    "\n",
    "    # Save checkpoint\n",
    "    if epoch % 10 == 0:\n",
    "        model_checkpoint(model, avg_val_loss)\n",
    "\n",
    "    # Check early stopping condition\n",
    "    # early_stopping(avg_val_loss)\n",
    "    # if early_stopping.early_stop:\n",
    "    #     print(\"Early stopping triggered. Stopping training.\")\n",
    "    #     break  # Exit the training loop\n",
    "\n",
    "writer.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Summary:\n",
      "Initial Training Loss: 3.0879\n",
      "Final Training Loss: 1.6172\n",
      "Best Training Loss: 1.6172\n",
      "\n",
      "Initial Validation Loss: 2.8068\n",
      "Final Validation Loss: 2.4108\n",
      "Best Validation Loss: 2.3910\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Plot the losses\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(range(1, len(train_losses) + 1), train_losses, label='Training Loss')\n",
    "plt.plot(range(1, len(val_losses) + 1), val_losses, label='Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training and Validation Losses')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Print final statistics\n",
    "print(\"\\nTraining Summary:\")\n",
    "print(f\"Initial Training Loss: {train_losses[0]:.4f}\")\n",
    "print(f\"Final Training Loss: {train_losses[-1]:.4f}\")\n",
    "print(f\"Best Training Loss: {min(train_losses):.4f}\")\n",
    "print(f\"\\nInitial Validation Loss: {val_losses[0]:.4f}\")\n",
    "print(f\"Final Validation Loss: {val_losses[-1]:.4f}\")\n",
    "print(f\"Best Validation Loss: {min(val_losses):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating DataLoader...\n",
      "Total samples in DataLoader: 48660\n",
      "\n",
      "First batch shapes:\n",
      "  - his_input_title: torch.Size([128, 40, 24])\n",
      "  - pred_input_title: torch.Size([128, 91, 24])\n",
      "  - targets: torch.Size([128, 91])\n",
      "\n",
      "Evaluation completed.\n",
      "Total predictions generated: 48660\n",
      "First few prediction lengths: [7, 13, 9, 36, 10, 14, 5, 34, 7, 28, 10, 14, 9, 54, 7]\n",
      "\n",
      "Validation against DataFrame:\n",
      "\n",
      "Metrics: {'auc': 0.512731142492542, 'mrr': 0.3194453485969988, 'ndcg@5': 0.35434363795527135, 'ndcg@10': 0.4350932290431804}\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "def evaluate_model(model, dataloader, device):\n",
    "    \"\"\"Evaluate the model and return predictions and labels for metric calculation.\"\"\"\n",
    "    model.eval()\n",
    "    all_predictions = []\n",
    "    all_labels = []\n",
    "\n",
    "    print(\"\\nEvaluating DataLoader...\")\n",
    "    total_samples = len(dataloader.dataset)\n",
    "    print(f\"Total samples in DataLoader: {total_samples}\")\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (inputs, targets, impression_ids) in enumerate(dataloader):\n",
    "            his_input_title, pred_input_title = inputs\n",
    "\n",
    "            if batch_idx == 0:  # Debug first batch shapes\n",
    "                print(\"\\nFirst batch shapes:\")\n",
    "                print(f\"  - his_input_title: {his_input_title.shape}\")\n",
    "                print(f\"  - pred_input_title: {pred_input_title.shape}\")\n",
    "                print(f\"  - targets: {targets.shape}\")\n",
    "\n",
    "            # Move data to device\n",
    "            his_input_title = his_input_title.to(device)\n",
    "            pred_input_title = pred_input_title.to(device)\n",
    "            targets = targets.to(device)\n",
    "\n",
    "            # Get predictions\n",
    "            predictions = model.predict(his_input_title, pred_input_title)\n",
    "            predictions = predictions.cpu().numpy()\n",
    "            targets = targets.cpu().numpy()\n",
    "\n",
    "            # Process each sample in the batch\n",
    "            batch_size = predictions.shape[0]\n",
    "            for sample_idx in range(batch_size):\n",
    "                pred = predictions[sample_idx]\n",
    "                label = targets[sample_idx]\n",
    "\n",
    "                # Create valid_mask where label is not equal to the padding value (-1)\n",
    "                valid_mask = (label != -1)\n",
    "                sample_preds = pred[valid_mask]\n",
    "                sample_labels = label[valid_mask]\n",
    "\n",
    "                if len(sample_labels) == 0:\n",
    "                    continue  # Skip empty samples\n",
    "\n",
    "                # Ensure that there is at least one positive and one negative label\n",
    "                if len(np.unique(sample_labels)) < 2:\n",
    "                    continue  # Skip samples with only one class\n",
    "\n",
    "                all_predictions.append(sample_preds.tolist())\n",
    "                all_labels.append(sample_labels.tolist())\n",
    "\n",
    "    print(\"\\nEvaluation completed.\")\n",
    "    print(f\"Total predictions generated: {len(all_predictions)}\")\n",
    "    print(f\"First few prediction lengths: {[len(x) for x in all_predictions[:15]]}\")\n",
    "    return all_labels, all_predictions\n",
    "\n",
    "\n",
    "# Evaluate the model\n",
    "labels_list, scores_list = evaluate_model(model, val_dataloader_temp, device)\n",
    "\n",
    "# Validate predictions against the DataFrame\n",
    "print(\"\\nValidation against DataFrame:\")\n",
    "if len(scores_list) != len(df_validation):\n",
    "    print(\"WARNING: Length mismatch!\")\n",
    "    print(f\"  - Number of predictions: {len(scores_list)}\")\n",
    "    print(f\"  - Number of rows in DataFrame: {len(df_validation)}\")\n",
    "\n",
    "# Compute metrics\n",
    "metrics = MetricEvaluator(\n",
    "    labels=labels_list,\n",
    "    predictions=scores_list,\n",
    "    metric_functions=[\n",
    "        AucScore(),\n",
    "        MrrScore(),\n",
    "        NdcgScore(k=5),\n",
    "        NdcgScore(k=10)\n",
    "    ],\n",
    ")\n",
    "results = metrics.evaluate()\n",
    "print(\"\\nMetrics:\", results.evaluations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FRACTION: 0.2, HISTORY_SIZE: 40\n",
      "Hyperparameters:\n",
      "title_size: 24\n",
      "embedding_dim: 32\n",
      "word_emb_dim: 8\n",
      "vocab_size: 10000\n",
      "head_num: 12\n",
      "head_dim: 12\n",
      "attention_hidden_dim: 128\n",
      "hidden_dim: 4\n",
      "optimizer: adam\n",
      "loss: cross_entropy_loss\n",
      "dropout: 0.2\n",
      "learning_rate: 0.0001\n",
      "weight_decay: 0.001\n",
      "news_output_dim: 64\n",
      "units_per_layer: [128, 128, 128]\n"
     ]
    }
   ],
   "source": [
    "print(f\"FRACTION: {FRACTION}, HISTORY_SIZE: {HISTORY_SIZE}\")\n",
    "\n",
    "# Filter out special Python attributes and print parameters\n",
    "params = {k: v for k, v in hparams_nrms.__dict__.items() if not k.startswith('__')}\n",
    "print(\"Hyperparameters:\")\n",
    "for key, value in params.items():\n",
    "    print(f\"{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_length_of_labels(df: pl.DataFrame, impression_id: int) -> int:\n",
    "    # Filter for matching impression_id\n",
    "    filtered = df.filter(pl.col('impression_id') == impression_id)\n",
    "    \n",
    "    if filtered.height == 0:\n",
    "        raise ValueError(f\"No row found for impression_id {impression_id}\")\n",
    "    \n",
    "    # Get labels from first row\n",
    "    labels = filtered.select('labels').row(0)[0]\n",
    "    \n",
    "    return len(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get_length_of_labels(df_validation, 349992000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label 0: [1, 0, 0, 0, 0, 0, 0]\n",
      "Label 1: [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]\n",
      "Label 2: [0, 0, 0, 0, 0, 0, 0, 1, 0]\n",
      "Label 3: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]\n",
      "Label 4: [0, 0, 0, 0, 0, 1, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "# Get first 5 labels\n",
    "first_5_labels = df_validation.select('labels').head(5)\n",
    "\n",
    "# Print each label with index\n",
    "for i, row in enumerate(first_5_labels.iter_rows()):\n",
    "    print(f\"Label {i}: {row[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipped 0 samples with only one class\n",
      "Remaining valid samples: 48660\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "# Initialize lists\n",
    "all_predictions = []\n",
    "all_labels = []\n",
    "skipped_samples = 0\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for (his_input_title, pred_input_title), targets, impression_ids in val_dataloader_temp:\n",
    "        # Move to device\n",
    "        his_input_title = his_input_title.to(device)\n",
    "        pred_input_title = pred_input_title.to(device)\n",
    "        \n",
    "        # Get predictions\n",
    "        predictions = model.predict(his_input_title, pred_input_title)\n",
    "        \n",
    "        \n",
    "        # Convert to probabilities if needed\n",
    "        if not torch.is_floating_point(predictions):\n",
    "            predictions = torch.sigmoid(predictions)\n",
    "        \n",
    "        # Convert to lists while preserving structure\n",
    "        batch_preds = predictions.cpu().numpy().tolist()\n",
    "        batch_labels = targets.cpu().numpy().tolist()\n",
    "        impression_ids = impression_ids.cpu().numpy().tolist()\n",
    "        \n",
    "        batch_preds_without_padding = []\n",
    "        batch_labels_without_padding = []\n",
    "        \n",
    "        for pred_sample, label_sample, impression_id_sample in zip(batch_preds, batch_labels, impression_ids):\n",
    "            # Remove padding\n",
    "            actual_length = get_length_of_labels(df_validation, impression_id_sample)\n",
    "            pred_sample = pred_sample[:actual_length]\n",
    "            \n",
    "            # Check if sample has both classes before adding\n",
    "            if 1 in label_sample[:actual_length] and 0 in label_sample[:actual_length]:\n",
    "                batch_preds_without_padding.append(pred_sample)\n",
    "                batch_labels_without_padding.append(label_sample[:actual_length])\n",
    "            else:\n",
    "                skipped_samples += 1\n",
    "        \n",
    "        # Add batch predictions and labels\n",
    "        all_predictions.extend(batch_preds_without_padding)\n",
    "        all_labels.extend(batch_labels_without_padding)\n",
    "print(f\"Skipped {skipped_samples} samples with only one class\")\n",
    "print(f\"Remaining valid samples: {len(all_predictions)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sample 0:\n",
      "Labels length:      7\n",
      "Predictions length: 7\n",
      "Num positives: 1.0\n",
      "Num negatives: 6.0\n",
      "Label distribution: [1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "\n",
      "Sample 1:\n",
      "Labels length:      13\n",
      "Predictions length: 13\n",
      "Num positives: 1.0\n",
      "Num negatives: 12.0\n",
      "Label distribution: [0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "\n",
      "Sample 2:\n",
      "Labels length:      9\n",
      "Predictions length: 9\n",
      "Num positives: 1.0\n",
      "Num negatives: 8.0\n",
      "Label distribution: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0]\n",
      "\n",
      "Sample 3:\n",
      "Labels length:      36\n",
      "Predictions length: 36\n",
      "Num positives: 1.0\n",
      "Num negatives: 35.0\n",
      "Label distribution: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "\n",
      "Sample 4:\n",
      "Labels length:      10\n",
      "Predictions length: 10\n",
      "Num positives: 1.0\n",
      "Num negatives: 9.0\n",
      "Label distribution: [0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0]\n"
     ]
    }
   ],
   "source": [
    "# Debug: Print label distribution for first 5 samples\n",
    "for i, (preds, labels) in enumerate(zip(all_predictions[:5], all_labels[:5])):\n",
    "    print(f\"\\nSample {i}:\")\n",
    "    print(f\"Labels length:      {len(labels)}\")\n",
    "    print(f\"Predictions length: {len(preds)}\")\n",
    "    print(f\"Num positives: {sum(labels)}\")\n",
    "    print(f\"Num negatives: {len(labels) - sum(labels)}\")\n",
    "    print(f\"Label distribution: {labels}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "<class 'list'>\n",
      "Number of predictions: 48660\n",
      "example prediction: [[0.9999992847442627, 0.9999996423721313, 0.9999991655349731, 0.9999986886978149, 0.9999986886978149, 0.9999990463256836, 0.999998927116394], [0.9999990463256836, 0.9999992847442627, 0.9999991655349731, 0.999998927116394, 0.9999988079071045, 0.9999994039535522, 0.9999986886978149, 0.9999985694885254, 0.9999984502792358, 0.9999990463256836, 0.9999674558639526, 0.9999979734420776, 0.999998927116394]]\n",
      "Number of labels: 48660\n",
      "example label: [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]]\n"
     ]
    }
   ],
   "source": [
    "print(type(all_predictions))\n",
    "print(type(all_labels))\n",
    "\n",
    "print(f\"Number of predictions: {len(all_predictions)}\")\n",
    "print(f\"example prediction: {all_predictions[0:2]}\")\n",
    "print(f\"Number of labels: {len(all_labels)}\")\n",
    "print(f\"example label: {all_labels[0:2]}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.1221\n"
     ]
    }
   ],
   "source": [
    "# Initialize counters\n",
    "correct_predictions = 0\n",
    "total_samples = len(all_predictions)\n",
    "\n",
    "# Iterate over samples\n",
    "for idx, _ in enumerate(all_predictions):\n",
    "    # print(f\"Sample {idx}: Prediction: {all_predictions[idx]}\")\n",
    "    # print(f\"Sample {idx}: Label:      {all_labels[idx]}\")\n",
    "    \n",
    "    # Extract index of maximum value in predictions and labels\n",
    "    pred_max_index = np.argmax(all_predictions[idx])\n",
    "    label_max_index = np.argmax(all_labels[idx])\n",
    "    \n",
    "    # Compare indices to determine if the prediction was correct\n",
    "    if pred_max_index == label_max_index:\n",
    "        # print(f\"Sample {idx}: Prediction was correct.\")\n",
    "        correct_predictions += 1\n",
    "  #  else:\n",
    "        # print(f\"Sample {idx}: Prediction was wrong.\")\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = correct_predictions / total_samples\n",
    "print(f\"Accuracy: {accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Mean AUC: 0.5217\n",
      "Number of valid AUC calculations: 48660\n"
     ]
    }
   ],
   "source": [
    "from evaluation import AucScore\n",
    "\n",
    "# auc_score = AucScore()\n",
    "\n",
    "# auc_score.calculate(all_predictions, all_labels)\n",
    "# print(f\"AUC: {auc_score.score}\")\n",
    "# # Calculate AUC per sample\n",
    "aucs = []\n",
    "for preds, labels in zip(all_predictions, all_labels):\n",
    "    try:\n",
    "        # Only calculate if we have both positive and negative samples\n",
    "        if sum(labels) > 0 and sum(labels) < len(labels):\n",
    "            auc = roc_auc_score(labels, preds)\n",
    "            aucs.append(auc)\n",
    "    except ValueError:\n",
    "        raise ValueError(\"Only one class present in labels. Cannot calculate AUC.\")\n",
    "\n",
    "print(f\"\\nMean AUC: {np.mean(aucs):.4f}\")\n",
    "print(f\"Number of valid AUC calculations: {len(aucs)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Output Prediction File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "https://github.com/recommenders-team/recommenders/blob/main/examples/00_quick_start/nrms_MIND.ipynb"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
